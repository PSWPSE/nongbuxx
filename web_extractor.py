import requests
from bs4 import BeautifulSoup, Tag
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent
from datetime import datetime
from typing import Dict, List, Optional, Any, Union, cast
import logging
import time
import os
import re # Added for regex operations

class WebExtractor:
    def __init__(self, use_selenium: bool = False, save_to_file: bool = True):
        """
        ì›¹ ì½˜í…ì¸  ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            use_selenium: Selenium ì‚¬ìš© ì—¬ë¶€
            save_to_file: ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
        """
        self.use_selenium = use_selenium
        self.save_to_file = save_to_file
        self.driver: Optional[webdriver.Chrome] = None
        self.session = requests.Session()
        self.ua = UserAgent()
        self.setup_logging()
        
        # ğŸš¨ í™ë³´ì„± ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ ì¶”ê°€
        self.promotional_patterns = {
            # ì œëª© ê¸°ë°˜ í™ë³´ì„± í‚¤ì›Œë“œ
            'title_keywords': [
                # í•œêµ­ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ê´‘ê³ ', 'í”„ë¡œëª¨ì…˜', 'í™ë³´', 'ì„ ì „', 'ì–´í•„', 'ì¶”ì²œ', 'ì†Œê°œ',
                'ë°”ë¡œê°€ê¸°', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°', 'êµ¬ë…', 'íŒ”ë¡œìš°', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…',
                'ëŒ“ê¸€', 'í›„ì›', 'ì œíœ´', 'í˜‘ì°¬', 'ìŠ¤í°ì„œ', 'ì§€ì›', 'ë„ì›€',
                'íŠ¹ê°€', 'í• ì¸', 'ì´ë²¤íŠ¸', 'í–‰ì‚¬', 'ëª¨ì§‘', 'ì±„ìš©', 'ê³µê³ ',
                'ì¶œì‹œ', 'ëŸ°ì¹­', 'ì˜¤í”ˆ', 'ì˜¤í”ˆì‹', 'ê¸°ë…', 'ì¶•í•˜', 'ê°ì‚¬',
                'ë‹¹ì²¨', 'ë‹¹ì²¨ì', 'ìˆ˜ìƒ', 'ìˆ˜ìƒì', 'ì‹œìƒ', 'ì‹œìƒì‹',
                'ë¬´ë£Œ', 'ì²´í—˜', 'ìƒ˜í”Œ', 'ì¦ì •', 'ê¸°í”„íŠ¸', 'ì„ ë¬¼',
                
                # ì˜ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ad', 'advertisement', 'sponsored', 'promotion', 'promotional',
                'sponsored content', 'paid', 'partnership', 'collaboration',
                'limited time', 'special offer', 'discount', 'sale', 'deal',
                'free trial', 'free sample', 'giveaway', 'contest', 'sweepstakes',
                'launch', 'release', 'announcement', 'press release',
                'event', 'celebration', 'ceremony', 'award', 'winner',
                'subscribe', 'follow', 'sign up', 'register', 'join',
                'click here', 'learn more', 'find out more', 'get started',
                'exclusive', 'premium', 'vip', 'membership', 'loyalty',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± í‚¤ì›Œë“œ ì¶”ê°€
                'etf ì†Œê°œ', 'etf ì¶”ì²œ', 'etf íˆ¬ì', 'etf ë¶„ì„', 'etf ì „ëµ',
                'íˆ¬ì ë ˆì´ë”', 'íˆ¬ì ê¸°íšŒ', 'íˆ¬ì ê°€ì¹˜', 'íˆ¬ì í¬ì¸íŠ¸',
                'íˆ¬ì ê³ ë ¤ì‚¬í•­', 'íˆ¬ì ê²€í† ', 'íˆ¬ì í‰ê°€', 'íˆ¬ì ì „ë§',
                'etf introduction', 'etf recommendation', 'etf investment',
                'investment radar', 'investment opportunity', 'investment value',
                'investment point', 'investment consideration', 'investment review',
                'investment evaluation', 'investment outlook',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ í‚¤ì›Œë“œ ì¶”ê°€
                'ì£¼ì‹ ì–´ë–„', 'ì£¼ì‹ ì¶”ì²œ', 'ì£¼ì‹ íˆ¬ì', 'ì£¼ì‹ ë§¤ìˆ˜', 'ì£¼ì‹ ë§¤ë„',
                'íˆ¬ì ì ê¸°', 'íˆ¬ì íƒ€ì´ë°', 'íˆ¬ì ì œì•ˆ', 'íˆ¬ì ì¶”ì²œ',
                'ë§¤ìˆ˜ ì‹œì ', 'ë§¤ë„ ì‹œì ', 'ë§¤ìˆ˜ íƒ€ì´ë°', 'ë§¤ë„ íƒ€ì´ë°',
                'ì£¼ê°€ ì „ë§', 'ì£¼ê°€ ì˜ˆì¸¡', 'ì£¼ê°€ ë¶„ì„', 'ì£¼ê°€ ì¶”ì²œ',
                'ì¢…ëª© ì¶”ì²œ', 'ì¢…ëª© ë¶„ì„', 'ì¢…ëª© ì „ë§', 'ì¢…ëª© íˆ¬ì',
                'stock recommendation', 'stock pick', 'stock analysis',
                'investment suggestion', 'investment advice', 'buy recommendation',
                'sell recommendation', 'timing', 'opportunity',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ í‚¤ì›Œë“œ (ì½˜í…ì¸  ìƒì„± ì‹œ ì œê±°ìš©)
                'zacks', 'zacks investment research', 'zacks rank', 'zacks industry rank',
                'zacks analyst', 'zacks estimate', 'zacks rating', 'zacks ranking',
                'automated insights', 'ai generated', 'machine learning',
                'ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨', 'ìë£Œ ì‚¬ìš©í•¨', 'ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨',
            ],
            
            # ì œëª© íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'title_patterns': [
                r'\[.*ê´‘ê³ .*\]', r'\[.*sponsored.*\]', r'\[.*ad.*\]',
                r'\(.*ê´‘ê³ .*\)', r'\(.*sponsored.*\)', r'\(.*ad.*\)',
                r'\[.*í”„ë¡œëª¨ì…˜.*\]', r'\[.*promotion.*\]',
                r'\[.*ì´ë²¤íŠ¸.*\]', r'\[.*event.*\]',
                r'\[.*íŠ¹ê°€.*\]', r'\[.*sale.*\]', r'\[.*í• ì¸.*\]',
                r'\[.*ë¬´ë£Œ.*\]', r'\[.*free.*\]',
                r'\[.*ì¶œì‹œ.*\]', r'\[.*launch.*\]',
                r'\[.*ëŸ°ì¹­.*\]', r'\[.*release.*\]',
                r'\[.*ê³µê°œ.*\]', r'\[.*announcement.*\]',
                r'\[.*ë‹¹ì²¨.*\]', r'\[.*winner.*\]',
                r'\[.*ìˆ˜ìƒ.*\]', r'\[.*award.*\]',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± íŒ¨í„´ ì¶”ê°€
                r'.*etf.*íˆ¬ì.*ë ˆì´ë”.*', r'.*etf.*íˆ¬ì.*ê¸°íšŒ.*',
                r'.*etf.*íˆ¬ì.*ê°€ì¹˜.*', r'.*etf.*íˆ¬ì.*í¬ì¸íŠ¸.*',
                r'.*etf.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*', r'.*etf.*íˆ¬ì.*ê²€í† .*',
                r'.*etf.*íˆ¬ì.*í‰ê°€.*', r'.*etf.*íˆ¬ì.*ì „ë§.*',
                r'.*etf.*ì†Œê°œ.*', r'.*etf.*ì¶”ì²œ.*', r'.*etf.*ë¶„ì„.*',
                r'.*etf.*ì „ëµ.*', r'.*íˆ¬ì.*ë ˆì´ë”.*', r'.*íˆ¬ì.*ê¸°íšŒ.*',
                r'.*íˆ¬ì.*ê°€ì¹˜.*', r'.*íˆ¬ì.*í¬ì¸íŠ¸.*', r'.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*',
                r'.*íˆ¬ì.*ê²€í† .*', r'.*íˆ¬ì.*í‰ê°€.*', r'.*íˆ¬ì.*ì „ë§.*',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ íŒ¨í„´ ì¶”ê°€
                r'.*ì£¼ì‹\s+ì–´ë–„\?.*', r'.*ì£¼ì‹\s+ì¶”ì²œ.*', r'.*ì£¼ì‹\s+íˆ¬ì.*',
                r'.*ì£¼ì‹\s+ë§¤ìˆ˜.*', r'.*ì£¼ì‹\s+ë§¤ë„.*', r'.*íˆ¬ì\s+ì ê¸°.*',
                r'.*íˆ¬ì\s+íƒ€ì´ë°.*', r'.*íˆ¬ì\s+ì œì•ˆ.*', r'.*íˆ¬ì\s+ì¶”ì²œ.*',
                r'.*ë§¤ìˆ˜\s+ì‹œì .*', r'.*ë§¤ë„\s+ì‹œì .*', r'.*ë§¤ìˆ˜\s+íƒ€ì´ë°.*',
                r'.*ë§¤ë„\s+íƒ€ì´ë°.*', r'.*ì£¼ê°€\s+ì „ë§.*', r'.*ì£¼ê°€\s+ì˜ˆì¸¡.*',
                r'.*ì£¼ê°€\s+ë¶„ì„.*', r'.*ì£¼ê°€\s+ì¶”ì²œ.*', r'.*ì¢…ëª©\s+ì¶”ì²œ.*',
                r'.*ì¢…ëª©\s+ë¶„ì„.*', r'.*ì¢…ëª©\s+ì „ë§.*', r'.*ì¢…ëª©\s+íˆ¬ì.*',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ íŒ¨í„´ ì¶”ê°€
                r'.*zacks.*', r'.*automated insights.*', r'.*ai generated.*',
                r'.*machine learning.*', r'.*ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨.*',
                r'.*ìë£Œ ì‚¬ìš©í•¨.*', r'.*ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨.*',
            ],
        }
        
        if use_selenium:
            self.setup_selenium()
    
    def setup_logging(self) -> None:
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_selenium(self) -> None:
        """Selenium ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument(f'user-agent={self.ua.random}')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
    
    def extract_data(self, url: str) -> Dict[str, Any]:
        """
        URLì—ì„œ ë°ì´í„° ì¶”ì¶œ
        
        Args:
            url: ì¶”ì¶œí•  ì›¹ í˜ì´ì§€ URL
            
        Returns:
            ì¶”ì¶œëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            self.logger.info(f"í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
            
            if self.use_selenium:
                data = self._extract_with_selenium(url)
            else:
                data = self._extract_with_requests(url)
            
            if self.save_to_file and data['success']:
                self._save_to_file(data)
            
            return data
            
        except TimeoutException:
            self.logger.error("í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
            return self._error_response(url, "í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
        except WebDriverException as e:
            self.logger.error(f"ì›¹ë“œë¼ì´ë²„ ì˜¤ë¥˜: {str(e)}")
            return self._error_response(url, f"ì›¹ë“œë¼ì´ë²„ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # 403 Forbidden ì—ëŸ¬ì— ëŒ€í•œ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ ì œê³µ
            error_str = str(e)
            if '403' in error_str and 'Forbidden' in error_str:
                return self._error_response(url, "ì´ ì‚¬ì´íŠ¸ëŠ” ìë™ ì½˜í…ì¸  ìˆ˜ì§‘ì„ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•´ ì£¼ì„¸ìš”.")
            elif '404' in error_str:
                return self._error_response(url, "í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            elif '500' in error_str:
                return self._error_response(url, "ì›¹ì‚¬ì´íŠ¸ ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            elif 'timeout' in error_str.lower():
                return self._error_response(url, "í˜ì´ì§€ ë¡œë”© ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
            elif 'connection' in error_str.lower():
                return self._error_response(url, "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            else:
                return self._error_response(url, error_str)
    
    def _extract_with_requests(self, url: str) -> Dict[str, Any]:
        """requestsë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì¶”ì¶œ"""
        headers = {'User-Agent': self.ua.random}
        response = requests.get(url, headers=headers, timeout=60)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        return self._parse_content(soup, url)
    
    def _extract_with_selenium(self, url: str) -> Dict[str, Any]:
        """Seleniumì„ ì‚¬ìš©í•œ ë°ì´í„° ì¶”ì¶œ"""
        if self.driver is None:
            raise RuntimeError("Selenium driver not initialized")
            
        self.driver.get(url)
        WebDriverWait(self.driver, 20).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        return self._parse_content(soup, url)
    
    def _parse_content(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """HTML ì½˜í…ì¸  íŒŒì‹±"""
        article = self._find_article(soup)
        if not article:
            return self._error_response(url, "ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ğŸš¨ í™ë³´ì„± ì½˜í…ì¸  í•„í„°ë§ ì ìš©
        title = self._get_title(soup)
        if self._is_promotional_content(title):
            return self._error_response(url, "í™ë³´ì„± ì½˜í…ì¸ ë¡œ íŒë‹¨ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ ë©”ì‹œì§€ ì œê±°
        content = self._get_content(article)
        # contentê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° text í•„ë“œ ì¶”ì¶œ
        if isinstance(content, dict):
            content_text = content.get('text', '')
            content_text = self._remove_zacks_automated_insights(content_text)
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ìœ ì§€
            content = {
                'text': content_text,
                'paragraphs': content.get('paragraphs', [])
            }
        else:
            # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì²˜ë¦¬
            content = self._remove_zacks_automated_insights(content)
        
        return {
            'success': True,
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'title': title,
            'metadata': self._get_metadata(soup),
            'content': content,
            'author': self._get_author(soup),
            'publish_date': self._get_publish_date(soup),
            'publisher': self._get_publisher(soup, url)
        }
    
    def _is_promotional_content(self, title: str) -> bool:
        """í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        if not title:
            return False
            
        title_lower = title.lower()
        
        # 1. ì œëª© í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.promotional_patterns['title_keywords']:
            if keyword.lower() in title_lower:
                self.logger.info(f"ğŸš« í™ë³´ì„± ì½˜í…ì¸  ì œì™¸ (í‚¤ì›Œë“œ): {title[:50]}...")
                return True
        
        # 2. ì œëª© íŒ¨í„´ ì²´í¬ (ëŒ€ê´„í˜¸, ì†Œê´„í˜¸ ì•ˆì˜ í™ë³´ì„± í‚¤ì›Œë“œ)
        for pattern in self.promotional_patterns['title_patterns']:
            if re.search(pattern, title, re.IGNORECASE):
                self.logger.info(f"ğŸš« í™ë³´ì„± ì½˜í…ì¸  ì œì™¸ (íŒ¨í„´): {title[:50]}...")
                return True
        
        return False
    
    def _remove_zacks_automated_insights(self, content: str) -> str:
        """Zacks/Automated Insights ê´€ë ¨ ë©”ì‹œì§€ ì œê±°"""
        if not content:
            return content
        
        # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ ë©”ì‹œì§€ ì œê±° íŒ¨í„´
        removal_patterns = [
            # Zacks ê´€ë ¨
            r'Zacks\s+ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨',
            r'Zacks\s+Investment\s+Researchì˜\s+ìë£Œ\s+ì‚¬ìš©í•¨',
            r'Zacks\s+Investment\s+Researchì˜\s+ìë£Œ\s+ì‚¬ìš©í•¨',
            r'Zacks\s+Rank\s+ì‹œìŠ¤í…œ',
            r'Zacks\s+Industry\s+Rank',
            r'Zacks\s+ì• ë„ë¦¬ìŠ¤íŠ¸',
            r'Zacks\s+í‰ê· \s+ì˜ˆìƒì¹˜',
            r'Zacks\s+ë“±ê¸‰',
            r'Zacks\s+í‰ê°€',
            r'Zacks\s+ìˆœìœ„',
            r'Zacks\s+ë¶„ì„',
            
            # Automated Insights ê´€ë ¨
            r'Automated\s+Insightsì˜\s+ë°ì´í„°\s+ê¸°ë°˜ìœ¼ë¡œ\s+ì‘ì„±ë¨',
            r'AI\s+generated',
            r'machine\s+learning',
            r'ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨',
            r'ìë£Œ\s+ì‚¬ìš©í•¨',
            r'ë°ì´í„°\s+ê¸°ë°˜ìœ¼ë¡œ\s+ì‘ì„±ë¨',
            
            # ì¼ë°˜ì ì¸ íŒ¨í„´
            r'ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨\s*Zacks\s+ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨',
            r'Zacks\s+ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨\s*ì›¹ì‚¬ì´íŠ¸ì—ì„œ\s+í™•ì¸\s+ê°€ëŠ¥í•¨',
        ]
        
        cleaned_content = content
        for pattern in removal_patterns:
            cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.IGNORECASE)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned_content = re.sub(r'\s+', ' ', cleaned_content)
        cleaned_content = cleaned_content.strip()
        
        if cleaned_content != content:
            self.logger.info("ğŸš« Zacks/Automated Insights ê´€ë ¨ ë©”ì‹œì§€ ì œê±°ë¨")
        
        return cleaned_content
    
    def _find_article(self, soup: BeautifulSoup) -> Optional[Tag]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì†Œ ì°¾ê¸°"""
        # Try different selectors for article content
        selectors = [
            'article',
            '.article',
            '.articlePage',
            '.story-body',
            '.content',
            '.post-content',
            '[class*="article"]',
            '[class*="content"]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element and isinstance(element, Tag):
                return element
        
        # Fallback: look for main content area
        main = soup.find('main') or soup.find('div', {'role': 'main'})
        if main and isinstance(main, Tag):
            return main
            
        return None
    
    def _get_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        
        # Yahoo Finance íŠ¹ìˆ˜ ì²˜ë¦¬ (ë¨¼ì € ì‹œë„)
        if 'finance.yahoo.com' in str(soup):
            yahoo_selectors = [
                '.cover-title',  # Yahoo Finance ì‹¤ì œ ê¸°ì‚¬ ì œëª©
                'title',         # Page title (ì •í™•í•¨)
            ]
            
            for selector in yahoo_selectors:
                title = soup.select_one(selector)
                if title and isinstance(title, Tag):
                    text = title.get_text().strip()
                    # Yahoo Finance ì‚¬ì´íŠ¸ ì´ë¦„ ì œê±°
                    if ' - Yahoo Finance' in text:
                        text = text.replace(' - Yahoo Finance', '')
                    elif ' | Yahoo Finance' in text:
                        text = text.replace(' | Yahoo Finance', '')
                    # ìœ íš¨í•œ ì œëª©ì¸ì§€ í™•ì¸
                    if text and 10 <= len(text) <= 200:
                        return text.strip()
        
        # ì¼ë°˜ì ì¸ ì œëª© ì„ íƒìë“¤
        title_selectors = [
            'h1',
            '#title_area h2',  # ë„¤ì´ë²„ ë‰´ìŠ¤
            '.media_end_head_headline h2',  # ë„¤ì´ë²„ ë‰´ìŠ¤
            '#articleTitle',  # êµ¬ ë„¤ì´ë²„ ë‰´ìŠ¤ 
            '.headline',
            '.title',
            '[class*="title"]',
            '[class*="headline"]',
        ]
        
        for selector in title_selectors:
            title = soup.select_one(selector)
            if title and isinstance(title, Tag):
                text = title.get_text().strip()
                if text and len(text) > 5:
                    return text
        
        # meta íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        meta_title = soup.find('meta', property='og:title')
        if meta_title and isinstance(meta_title, Tag):
            content = meta_title.get('content', '')
            if isinstance(content, str) and content.strip():
                return content.strip()
        
        # HTML title íƒœê·¸ (fallback)
        html_title = soup.find('title')
        if html_title and isinstance(html_title, Tag):
            text = html_title.get_text().strip()
            if text:
                return text
        
        return "ì œëª© ì—†ìŒ"
    
    def _get_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata: Dict[str, str] = {}
        meta_names = ['description', 'author', 'published_time', 'keywords']
        
        for meta in soup.find_all('meta'):
            if not isinstance(meta, Tag):
                continue
                
            name = meta.get('name', meta.get('property', ''))
            content = meta.get('content', '')
            
            if isinstance(name, str) and isinstance(content, str):
                name_lower = name.lower()
                if name_lower in meta_names and content:
                    metadata[name_lower] = content
        
        return metadata
    
    def _get_content(self, article: Tag) -> Dict[str, Any]:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        paragraphs = []
        
        # Find all text content elements
        content_elements = article.find_all(['p', 'h2', 'h3', 'h4', 'blockquote', 'div'])
        
        for element in content_elements:
            if not isinstance(element, Tag):
                continue
                
            text = element.get_text().strip()
            
            # Skip promotional content
            skip_keywords = [
                'recommended', 'related', 'subscribe', 'follow', 'download',
                'sign up', 'newsletter', 'advertisement', 'sponsored'
            ]
            
            if (text and 
                len(text) > 10 and 
                not any(keyword in text.lower() for keyword in skip_keywords)):
                paragraphs.append(text)
        
        return {
            'text': '\n\n'.join(paragraphs),
            'paragraphs': paragraphs
        }
    
    def _get_author(self, soup: BeautifulSoup) -> str:
        """ì €ì ì •ë³´ ì¶”ì¶œ"""
        # Try meta tag first
        author_meta = soup.find('meta', {'name': 'author'})
        if author_meta and isinstance(author_meta, Tag):
            content = author_meta.get('content', '')
            if isinstance(content, str) and content:
                return content
        
        # Try various author selectors
        author_selectors = [
            '[class*="author"]',
            '[class*="byline"]',
            '.author',
            '.byline'
        ]
        
        for selector in author_selectors:
            author = soup.select_one(selector)
            if author and isinstance(author, Tag):
                text = author.get_text().strip()
                if text and len(text) < 100:  # Reasonable author name length
                    return text
        
        return ''
    
    def _get_publish_date(self, soup: BeautifulSoup) -> str:
        """ë°œí–‰ì¼ ì¶”ì¶œ"""
        # Try meta tags first
        date_meta = (soup.find('meta', {'name': 'date'}) or 
                    soup.find('meta', {'property': 'article:published_time'}))
        if date_meta and isinstance(date_meta, Tag):
            content = date_meta.get('content', '')
            if isinstance(content, str) and content:
                return content
        
        # Try various date selectors
        date_selectors = [
            '[class*="date"]',
            '[class*="time"]',
            '.date',
            '.time',
            '.published'
        ]
        
        for selector in date_selectors:
            date = soup.select_one(selector)
            if date and isinstance(date, Tag):
                text = date.get_text().strip()
                if text and len(text) < 50:  # Reasonable date length
                    return text
        
        return ''
    
    def _get_publisher(self, soup: BeautifulSoup, url: str) -> str:
        """ì¶œì²˜/ë°œí–‰ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        # 1. Meta íƒœê·¸ì—ì„œ publisher ì •ë³´ ì°¾ê¸°
        publisher_meta = (
            soup.find('meta', {'property': 'og:site_name'}) or
            soup.find('meta', {'name': 'publisher'}) or
            soup.find('meta', {'property': 'article:publisher'})
        )
        if publisher_meta and isinstance(publisher_meta, Tag):
            content = publisher_meta.get('content', '')
            if isinstance(content, str) and content:
                return content.strip()
        
        # 2. ë„ë©”ì¸ë³„ í•˜ë“œì½”ë”©ëœ ì¶œì²˜ ë§¤í•‘
        domain_to_publisher = {
            'bloomberg.com': 'Bloomberg',
            'wsj.com': 'WSJ',
            'reuters.com': 'Reuters',
            'ft.com': 'Financial Times',
            'cnbc.com': 'CNBC',
            'finance.yahoo.com': 'Yahoo Finance',
            'naver.com': 'ë„¤ì´ë²„ë‰´ìŠ¤',
            'news.naver.com': 'ë„¤ì´ë²„ë‰´ìŠ¤',
            'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤',
            'bbc.com': 'BBC',
            'cnn.com': 'CNN',
            'forbes.com': 'Forbes',
            'economist.com': 'The Economist',
            'techcrunch.com': 'TechCrunch',
            'theverge.com': 'The Verge'
        }
        
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # www. ì œê±°
        if domain.startswith('www.'):
            domain = domain[4:]
        
        # ë„ë©”ì¸ ë§¤í•‘ì—ì„œ ì°¾ê¸°
        for key, value in domain_to_publisher.items():
            if key in domain:
                return value
        
        # 3. HTMLì—ì„œ publisher ì •ë³´ ì°¾ê¸°
        publisher_selectors = [
            '.publisher',
            '.source',
            '[class*="publisher"]',
            '[class*="source"]',
            '.media_end_head_top_logo img',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ë¡œê³ 
            '.press_logo img'  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ë¡œê³  (êµ¬ë²„ì „)
        ]
        
        for selector in publisher_selectors:
            publisher = soup.select_one(selector)
            if publisher and isinstance(publisher, Tag):
                # ì´ë¯¸ì§€ì¸ ê²½ìš° alt í…ìŠ¤íŠ¸ í™•ì¸
                if publisher.name == 'img':
                    alt_text = publisher.get('alt', '')
                    if isinstance(alt_text, str) and alt_text:
                        return alt_text.strip()
                else:
                    text = publisher.get_text().strip()
                    if text and len(text) < 50:
                        return text
        
        # 4. ë„ë©”ì¸ëª…ì„ ì¶œì²˜ë¡œ ì‚¬ìš© (fallback)
        return domain.split('.')[0].title()
    
    def _error_response(self, url: str, error_message: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            'success': False,
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'error': error_message
        }
    
    def _save_to_file(self, data: Dict[str, Any]) -> None:
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs('extracted_articles', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        txt_path = f'extracted_articles/article_{timestamp}.txt'
        
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"ì œëª©: {data['title']}\n")
            f.write("="*80 + "\n\n")
            
            if data['metadata']:
                f.write("ë©”íƒ€ ì •ë³´:\n")
                for key, value in data['metadata'].items():
                    f.write(f"{key}: {value}\n")
                f.write("-"*80 + "\n\n")
            
            f.write("ë³¸ë¬¸:\n")
            f.write(data['content']['text'])
        
        self.logger.info(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ë¨: {txt_path}")
    
    def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
        self.session.close() 