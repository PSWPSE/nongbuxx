import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random
from typing import List, Dict, Optional, Any
import logging
from fake_useragent import UserAgent
import re
from urllib.parse import urljoin, urlparse
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedNewsExtractor:
    """ë¹ ë¥´ê³  ê°€ë²¼ìš´ ë‰´ìŠ¤ ì¶”ì¶œê¸°"""
    
    def __init__(self, base_url: str, search_keywords: Optional[str] = None, max_news: int = 10):
        self.base_url = base_url
        self.search_keywords = search_keywords
        self.max_news = max_news
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”) - ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ë‹¨ì¶•
        self.timeout = 12
        
        # ì˜ëª»ëœ ë§í¬ íŒ¨í„´ (ì£¼ì‹ ticker ë“±)
        self.invalid_patterns = [
            r'/quote/[A-Z0-9]+-USD',  # ì•”í˜¸í™”í ticker
            r'/quote/[A-Z0-9]+\..*',  # ì£¼ì‹ ticker with exchange
            r'/quote/[A-Z]{1,5}$',    # ì£¼ì‹ ticker
            r'/symbol/[A-Z0-9]+',     # ì‹¬ë³¼ í˜ì´ì§€
            r'/lookup/',              # ê²€ìƒ‰ í˜ì´ì§€
            r'/calendar/',            # ìº˜ë¦°ë”
            r'/screener/',            # ìŠ¤í¬ë¦¬ë„ˆ
            r'/research/',            # ë¦¬ì„œì¹˜
            r'/premium/',             # í”„ë¦¬ë¯¸ì—„
            r'/video/',               # ë¹„ë””ì˜¤
            r'/author/',              # ì €ì í˜ì´ì§€
            r'/topic/[^/]+/$',        # í† í”½ ë©”ì¸ í˜ì´ì§€ (ë‰´ìŠ¤ ì•„ë‹˜)
        ]
        
        # ğŸš¨ ê°•í™”ëœ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
        self.promotional_patterns = {
            # ì œëª© ê¸°ë°˜ í™ë³´ì„± í‚¤ì›Œë“œ
            'title_keywords': [
                # í•œêµ­ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ê´‘ê³ ', 'í”„ë¡œëª¨ì…˜', 'í™ë³´', 'ì„ ì „', 'ì–´í•„', 'ì¶”ì²œ', 'ì†Œê°œ',
                'ë°”ë¡œê°€ê¸°', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°', 'êµ¬ë…', 'íŒ”ë¡œìš°', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…',
                'ëŒ“ê¸€', 'í›„ì›', 'ì œíœ´', 'í˜‘ì°¬', 'ìŠ¤í°ì„œ', 'ì§€ì›', 'ë„ì›€',
                'íŠ¹ê°€', 'í• ì¸', 'ì´ë²¤íŠ¸', 'í–‰ì‚¬', 'ëª¨ì§‘', 'ì±„ìš©', 'ê³µê³ ',
                'ì¶œì‹œ', 'ëŸ°ì¹­', 'ì˜¤í”ˆ', 'ì˜¤í”ˆì‹', 'ê¸°ë…', 'ì¶•í•˜', 'ê°ì‚¬',
                'ë‹¹ì²¨', 'ë‹¹ì²¨ì', 'ìˆ˜ìƒ', 'ìˆ˜ìƒì', 'ì‹œìƒ', 'ì‹œìƒì‹',
                'ë¬´ë£Œ', 'ì²´í—˜', 'ìƒ˜í”Œ', 'ì¦ì •', 'ê¸°í”„íŠ¸', 'ì„ ë¬¼',
                
                # ì˜ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ad', 'advertisement', 'sponsored', 'promotion', 'promotional',
                'sponsored content', 'paid', 'partnership', 'collaboration',
                'limited time', 'special offer', 'discount', 'sale', 'deal',
                'free trial', 'free sample', 'giveaway', 'contest', 'sweepstakes',
                'launch', 'release', 'announcement', 'press release',
                'event', 'celebration', 'ceremony', 'award', 'winner',
                'subscribe', 'follow', 'sign up', 'register', 'join',
                'click here', 'learn more', 'find out more', 'get started',
                'exclusive', 'premium', 'vip', 'membership', 'loyalty',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± í‚¤ì›Œë“œ ì¶”ê°€
                'etf ì†Œê°œ', 'etf ì¶”ì²œ', 'etf íˆ¬ì', 'etf ë¶„ì„', 'etf ì „ëµ',
                'íˆ¬ì ë ˆì´ë”', 'íˆ¬ì ê¸°íšŒ', 'íˆ¬ì ê°€ì¹˜', 'íˆ¬ì í¬ì¸íŠ¸',
                'íˆ¬ì ê³ ë ¤ì‚¬í•­', 'íˆ¬ì ê²€í† ', 'íˆ¬ì í‰ê°€', 'íˆ¬ì ì „ë§',
                'etf introduction', 'etf recommendation', 'etf investment',
                'investment radar', 'investment opportunity', 'investment value',
                'investment point', 'investment consideration', 'investment review',
                'investment evaluation', 'investment outlook',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ í‚¤ì›Œë“œ ì¶”ê°€
                'ì£¼ì‹ ì–´ë–„', 'ì£¼ì‹ ì¶”ì²œ', 'ì£¼ì‹ íˆ¬ì', 'ì£¼ì‹ ë§¤ìˆ˜', 'ì£¼ì‹ ë§¤ë„',
                'íˆ¬ì ì ê¸°', 'íˆ¬ì íƒ€ì´ë°', 'íˆ¬ì ì œì•ˆ', 'íˆ¬ì ì¶”ì²œ',
                'ë§¤ìˆ˜ ì‹œì ', 'ë§¤ë„ ì‹œì ', 'ë§¤ìˆ˜ íƒ€ì´ë°', 'ë§¤ë„ íƒ€ì´ë°',
                'ì£¼ê°€ ì „ë§', 'ì£¼ê°€ ì˜ˆì¸¡', 'ì£¼ê°€ ë¶„ì„', 'ì£¼ê°€ ì¶”ì²œ',
                'ì¢…ëª© ì¶”ì²œ', 'ì¢…ëª© ë¶„ì„', 'ì¢…ëª© ì „ë§', 'ì¢…ëª© íˆ¬ì',
                'stock recommendation', 'stock pick', 'stock analysis',
                'investment suggestion', 'investment advice', 'buy recommendation',
                'sell recommendation', 'timing', 'opportunity',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ í‚¤ì›Œë“œ (ì½˜í…ì¸  ìƒì„± ì‹œ ì œê±°ìš©)
                'zacks', 'zacks investment research', 'zacks rank', 'zacks industry rank',
                'zacks analyst', 'zacks estimate', 'zacks rating', 'zacks ranking',
                'automated insights', 'ai generated', 'machine learning',
                'ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨', 'ìë£Œ ì‚¬ìš©í•¨', 'ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨',
            ],
            
            # URL íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'url_patterns': [
                r'/ad/', r'/ads/', r'/advertisement/', r'/sponsored/',
                r'/promotion/', r'/promotional/', r'/event/', r'/events/',
                r'/contest/', r'/giveaway/', r'/sweepstakes/', r'/sale/',
                r'/deal/', r'/offer/', r'/special/', r'/limited/',
                r'/free/', r'/trial/', r'/sample/', r'/gift/',
                r'/subscribe/', r'/signup/', r'/register/', r'/join/',
                r'/membership/', r'/premium/', r'/vip/', r'/exclusive/',
                r'/press-release/', r'/announcement/', r'/launch/', r'/release/',
                r'/partnership/', r'/collaboration/', r'/sponsor/', r'/sponsored/',
                
                # ğŸš¨ ETF ë° íˆ¬ì ìƒí’ˆ ê´€ë ¨ URL íŒ¨í„´ ì¶”ê°€
                r'/etf/', r'/etfs/', r'/fund/', r'/funds/', r'/investment-product/',
                r'/product/', r'/products/', r'/investment/', r'/investing/',
                r'/portfolio/', r'/strategy/', r'/analysis/', r'/research/',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ ê´€ë ¨ URL íŒ¨í„´ ì¶”ê°€
                r'/recommendation/', r'/recommendations/', r'/pick/', r'/picks/',
                r'/advice/', r'/suggestion/', r'/timing/', r'/opportunity/',
                r'/buy/', r'/sell/', r'/trade/', r'/trading/',
                r'/stock-pick/', r'/stock-recommendation/', r'/investment-advice/',
                r'/market-timing/', r'/investment-timing/',
            ],
            
            # ì œëª© íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'title_patterns': [
                r'\[.*ê´‘ê³ .*\]', r'\[.*sponsored.*\]', r'\[.*ad.*\]',
                r'\(.*ê´‘ê³ .*\)', r'\(.*sponsored.*\)', r'\(.*ad.*\)',
                r'\[.*í”„ë¡œëª¨ì…˜.*\]', r'\[.*promotion.*\]',
                r'\[.*ì´ë²¤íŠ¸.*\]', r'\[.*event.*\]',
                r'\[.*íŠ¹ê°€.*\]', r'\[.*sale.*\]', r'\[.*í• ì¸.*\]',
                r'\[.*ë¬´ë£Œ.*\]', r'\[.*free.*\]',
                r'\[.*ì¶œì‹œ.*\]', r'\[.*launch.*\]',
                r'\[.*ëŸ°ì¹­.*\]', r'\[.*release.*\]',
                r'\[.*ê³µê°œ.*\]', r'\[.*announcement.*\]',
                r'\[.*ë‹¹ì²¨.*\]', r'\[.*winner.*\]',
                r'\[.*ìˆ˜ìƒ.*\]', r'\[.*award.*\]',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± íŒ¨í„´ ì¶”ê°€
                r'.*ETF\s+ì†Œê°œ.*', r'.*ETF\s+ì¶”ì²œ.*', r'.*ETF\s+íˆ¬ì.*',
                r'.*ETF\s+ë¶„ì„.*', r'.*ETF\s+ì „ëµ.*', r'.*íˆ¬ì\s+ë ˆì´ë”.*',
                r'.*íˆ¬ì\s+ê¸°íšŒ.*', r'.*íˆ¬ì\s+ê°€ì¹˜.*', r'.*íˆ¬ì\s+í¬ì¸íŠ¸.*',
                r'.*íˆ¬ì\s+ê³ ë ¤ì‚¬í•­.*', r'.*íˆ¬ì\s+ê²€í† .*', r'.*íˆ¬ì\s+í‰ê°€.*',
                r'.*íˆ¬ì\s+ì „ë§.*',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ íŒ¨í„´ ì¶”ê°€
                r'.*ì£¼ì‹\s+ì–´ë–„\?.*', r'.*ì£¼ì‹\s+ì¶”ì²œ.*', r'.*ì£¼ì‹\s+íˆ¬ì.*',
                r'.*ì£¼ì‹\s+ë§¤ìˆ˜.*', r'.*ì£¼ì‹\s+ë§¤ë„.*', r'.*íˆ¬ì\s+ì ê¸°.*',
                r'.*íˆ¬ì\s+íƒ€ì´ë°.*', r'.*íˆ¬ì\s+ì œì•ˆ.*', r'.*íˆ¬ì\s+ì¶”ì²œ.*',
                r'.*ë§¤ìˆ˜\s+ì‹œì .*', r'.*ë§¤ë„\s+ì‹œì .*', r'.*ë§¤ìˆ˜\s+íƒ€ì´ë°.*',
                r'.*ë§¤ë„\s+íƒ€ì´ë°.*', r'.*ì£¼ê°€\s+ì „ë§.*', r'.*ì£¼ê°€\s+ì˜ˆì¸¡.*',
                r'.*ì£¼ê°€\s+ë¶„ì„.*', r'.*ì£¼ê°€\s+ì¶”ì²œ.*', r'.*ì¢…ëª©\s+ì¶”ì²œ.*',
                r'.*ì¢…ëª©\s+ë¶„ì„.*', r'.*ì¢…ëª©\s+ì „ë§.*', r'.*ì¢…ëª©\s+íˆ¬ì.*',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ íŒ¨í„´ ì¶”ê°€
                r'.*zacks.*', r'.*automated insights.*', r'.*ai generated.*',
                r'.*machine learning.*', r'.*ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨.*',
                r'.*ìë£Œ ì‚¬ìš©í•¨.*', r'.*ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨.*',
            ],
            
            # ì§§ì€ ì œëª© í•„í„° (í™ë³´ì„± ì œëª©ì€ ë³´í†µ ì§§ìŒ)
            'min_title_length': 15,
            
            # ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì í•„í„° (í™ë³´ì„± ì œëª©ì— ìì£¼ ë‚˜íƒ€ë‚¨)
            'excessive_symbols': [
                '!', '!!', '!!!', '?', '??', '???', '~', '~~', '~~~',
                'â˜…', 'â˜†', 'â™¥', 'â™¡', 'â™ ', 'â™£', 'â™¦', 'â—', 'â—‹', 'â—†', 'â—‡',
                'â–¶', 'â—€', 'â–²', 'â–¼', 'â– ', 'â–¡', 'â–£', 'â–¤', 'â–¥', 'â–¦', 'â–§', 'â–¨', 'â–©',
            ]
        }
    
    def extract_news(self) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜"""
        try:
            print(f"ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {self.base_url}")
            
            # ë¹ ë¥¸ HTML ìš”ì²­
            html_content = self._fetch_html()
            if not html_content:
                return []
            
            # ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
            news_links = self._extract_news_links(html_content)
            
            # ğŸš¨ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì ìš©
            filtered_links = self._filter_promotional_content(news_links)
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
            news_items = self._create_news_items(filtered_links)
            
            print(f"HTML íŒŒì‹±ì—ì„œ {len(news_links)}ê°œ ë‰´ìŠ¤ ì¶”ì¶œ, í™ë³´ì„± í•„í„°ë§ í›„ {len(filtered_links)}ê°œ ìœ ì§€")
            return news_items[:self.max_news]
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _filter_promotional_content(self, links: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """í™ë³´ì„± ì½˜í…ì¸  í•„í„°ë§"""
        filtered_links = []
        
        for link in links:
            title = link['title']
            url = link['url']
            
            # í™ë³´ì„± ì½˜í…ì¸  ì²´í¬
            if self._is_promotional_content(title, url):
                print(f"ğŸš« í™ë³´ì„± ë‰´ìŠ¤ ì œì™¸: {title[:50]}...")
                continue
            
            # ğŸš¨ í´ë¦­ë² ì´íŠ¸/ì €í’ˆì§ˆ íˆ¬ì ì¶”ì²œ ê¸°ì‚¬ í•„í„°ë§
            if self._is_clickbait_investment_article(title):
                print(f"ğŸš« í´ë¦­ë² ì´íŠ¸ íˆ¬ì ê¸°ì‚¬ ì œì™¸: {title[:50]}...")
                continue
            
            filtered_links.append(link)
        
        return filtered_links
    
    def _is_promotional_content(self, title: str, url: str) -> bool:
        """í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        url_lower = url.lower()
        
        # 1. ì œëª© í‚¤ì›Œë“œ ì²´í¬ (ë” ì •êµí•œ íŒë‹¨)
        promotional_keywords = 0
        for keyword in self.promotional_patterns['title_keywords']:
            if keyword.lower() in title_lower:
                promotional_keywords += 1
        
        # í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ í™ë³´ì„±ìœ¼ë¡œ íŒë‹¨ (ë‹¨ì¼ í‚¤ì›Œë“œëŠ” í—ˆìš©)
        if promotional_keywords >= 2:
            return True
        
        # 2. URL íŒ¨í„´ ì²´í¬
        for pattern in self.promotional_patterns['url_patterns']:
            if re.search(pattern, url_lower):
                return True
        
        # 3. ì œëª© íŒ¨í„´ ì²´í¬ (ëŒ€ê´„í˜¸, ì†Œê´„í˜¸ ì•ˆì˜ í™ë³´ì„± í‚¤ì›Œë“œ)
        for pattern in self.promotional_patterns['title_patterns']:
            if re.search(pattern, title, re.IGNORECASE):
                return True
        
        # 4. ğŸš¨ ETF í™ë³´ì„± ì½˜í…ì¸  íŠ¹ë³„ ì²´í¬ (ë” ì •í™•í•œ íŒë‹¨)
        if self._is_etf_promotional_content(title):
            return True
        
        # 5. ì œëª© ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ í™ë³´ì„±ì¼ ê°€ëŠ¥ì„±)
        if len(title.strip()) < self.promotional_patterns['min_title_length']:
            return True
        
        # 6. ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì ì²´í¬
        symbol_count = sum(1 for symbol in self.promotional_patterns['excessive_symbols'] if symbol in title)
        if symbol_count >= 3:  # 3ê°œ ì´ìƒì˜ íŠ¹ìˆ˜ë¬¸ìê°€ ìˆìœ¼ë©´ í™ë³´ì„±
            return True
        
        # 7. ë°˜ë³µë˜ëŠ” ë¬¸ì ì²´í¬ (ì˜ˆ: "ëŒ€ë°•!!!", "ìµœê³ !!!")
        if re.search(r'([!?~â˜…â˜†â™¥â™¡])\1{2,}', title):
            return True
        
        # 8. ê³¼ë„í•œ ëŒ€ë¬¸ì ì²´í¬ (í™ë³´ì„± ì œëª©ì€ ëŒ€ë¬¸ìë¥¼ ë§ì´ ì‚¬ìš©)
        uppercase_ratio = sum(1 for char in title if char.isupper()) / len(title) if title else 0
        if uppercase_ratio > 0.7:  # 70% ì´ìƒì´ ëŒ€ë¬¸ìë©´ í™ë³´ì„±
            return True
        
        # 9. ğŸš¨ ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ ì²´í¬ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        if self._is_normal_news_content(title):
            return False
        
        return False
    
    def _is_normal_news_content(self, title: str) -> bool:
        """ì •ìƒì ì¸ ë‰´ìŠ¤ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        
        # ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ë“¤
        normal_news_patterns = [
            # ê¸°ì—… ê´€ë ¨
            r'.*ê¸°ì—….*ì‹¤ì .*',
            r'.*ê¸°ì—….*ì„±ê³¼.*',
            r'.*ê¸°ì—….*ì „ëµ.*',
            r'.*ê¸°ì—….*ë°œí‘œ.*',
            r'.*ê¸°ì—….*ì¶œì‹œ.*',
            r'.*ê¸°ì—….*ì§„ì¶œ.*',
            r'.*ê¸°ì—….*íˆ¬ì.*',
            r'.*ê¸°ì—….*ì¸ìˆ˜.*',
            r'.*ê¸°ì—….*í•©ë³‘.*',
            
            # ì‹œì¥ ê´€ë ¨
            r'.*ì‹œì¥.*ë™í–¥.*',
            r'.*ì‹œì¥.*ë¶„ì„.*',
            r'.*ì‹œì¥.*ì „ë§.*',
            r'.*ì‹œì¥.*ë³€í™”.*',
            r'.*ì‹œì¥.*ì„±ì¥.*',
            r'.*ì‹œì¥.*ê·œëª¨.*',
            
            # ê²½ì œ ê´€ë ¨
            r'.*ê²½ì œ.*ì •ì±….*',
            r'.*ê²½ì œ.*ì§€í‘œ.*',
            r'.*ê²½ì œ.*ì„±ì¥.*',
            r'.*ê²½ì œ.*ì „ë§.*',
            
            # ê¸°ìˆ  ê´€ë ¨
            r'.*ê¸°ìˆ .*ê°œë°œ.*',
            r'.*ê¸°ìˆ .*í˜ì‹ .*',
            r'.*ê¸°ìˆ .*íŠ¸ë Œë“œ.*',
            r'.*ê¸°ìˆ .*ë™í–¥.*',
            
            # ì£¼ì‹/íˆ¬ì ê´€ë ¨ (ì •ìƒì ì¸ ë‰´ìŠ¤)
            r'.*ì£¼ê°€.*ìƒìŠ¹.*',
            r'.*ì£¼ê°€.*í•˜ë½.*',
            r'.*ì£¼ê°€.*ë³€ë™.*',
            r'.*íˆ¬ì.*ë™í–¥.*',
            r'.*íˆ¬ì.*í™˜ê²½.*',
            r'.*íˆ¬ì.*ì‹œì¥.*',
        ]
        
        # ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ í™•ì¸
        for pattern in normal_news_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def _is_clickbait_investment_article(self, title: str) -> bool:
        """í´ë¦­ë² ì´íŠ¸ì„± íˆ¬ì ì¶”ì²œ ê¸°ì‚¬ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        
        # ğŸš¨ ì‚¬ìš©ìê°€ ì œì‹œí•œ íŒ¨í„´ë“¤ (ì ˆëŒ€ ì œì™¸í•´ì•¼ í•  ê¸°ì‚¬ë“¤)
        clickbait_patterns = [
            # 1. ê³¼ê±° íˆ¬ì ì‹œë®¬ë ˆì´ì…˜ íŒ¨í„´
            r"if you['']d invested \$?\d+",  # If You'd Invested $1000
            r"if you invested \$?\d+",  # If You Invested $100
            r"had you invested \$?\d+",  # Had You Invested
            r"\d+ years ago.*how much.*today",  # 5 Years Ago, Here's How Much You'd Have Today
            r"here['']s how much you['']d have",  # Here's How Much You'd Have
            r"here['']s what happened",  # Here's What Happened
            r"here['']s what you['']d have",  # Here's What You'd Have
            
            # 2. íŠ¹ì • ë‚ ì§œ ì¶”ì²œ íŒ¨í„´
            r"best .* to buy for [a-z]+ \d+",  # Best Income Stocks to Buy for August 25th
            r"top .* to buy for [a-z]+ \d+",  # Top Stocks to Buy for July 15th
            r"best .* to buy today",  # Best Stocks to Buy Today
            r"best .* to buy this week",  # Best Stocks to Buy This Week
            r"best .* to buy this month",  # Best Stocks to Buy This Month
            r"stocks to buy for [a-z]+ \d+",  # Stocks to Buy for August 25th
            
            # 3. ë¦¬ìŠ¤íŠ¸í˜• ì£¼ì‹ ì¶”ì²œ íŒ¨í„´
            r"^\d+ .*stocks",  # 2 Profitable Stocks, 3 Volatile Stocks
            r"^top \d+ stocks",  # Top 5 Stocks
            r"^best \d+ stocks",  # Best 10 Stocks
            r"\d+ stocks to",  # 5 Stocks to Watch
            r"\d+ stocks for",  # 3 Stocks for Long-Term
            r"\d+ stocks that",  # 2 Stocks That
            r"\d+ stocks with",  # 3 Stocks with Warning
            
            # 4. ì£¼ê´€ì  í‰ê°€ í¬í•¨ íŒ¨í„´
            r"we question",  # and 1 We Question
            r"we find risky",  # and 1 We Find Risky
            r"we think twice",  # We Think Twice About
            r"deserve.*love",  # Deserve Some Love
            r"should avoid",  # Should Avoid
            r"stay away from",  # Stay Away From
            r"warning sign",  # with Warning Sign
            r"red flag",  # Red Flag
            
            # 5. í´ë¦­ë² ì´íŠ¸ êµ¬ì¡° íŒ¨í„´
            r"and \d+ we",  # and 1 We Question/Find
            r"you won['']t believe",  # You Won't Believe
            r"shocking.*truth",  # Shocking Truth
            r"this one.*trick",  # This One Trick
            r"analysts.*hate",  # Analysts Hate
            r"wall street.*secret",  # Wall Street Secret
            
            # 6. ê°€ì •ë²•/ì¡°ê±´ë¶€ íŒ¨í„´
            r"what if you",  # What If You
            r"imagine if",  # Imagine If
            r"suppose you",  # Suppose You
            r"let['']s say you",  # Let's Say You
            
            # 7. ìˆ˜ìµë¥  ìë‘ íŒ¨í„´
            r"\d+% return",  # 500% Return
            r"\d+x your money",  # 10x Your Money
            r"doubled your money",  # Doubled Your Money
            r"tripled your investment",  # Tripled Your Investment
            r"millionaire.*\$\d+",  # Millionaire with $1000
            
            # 8. ê¸´ê¸‰ì„± ì¡°ì¥ íŒ¨í„´
            r"before it['']s too late",  # Before It's Too Late
            r"last chance",  # Last Chance
            r"don['']t miss",  # Don't Miss
            r"act now",  # Act Now
            r"limited time",  # Limited Time
            r"hurry",  # Hurry
            
            # 9. ì˜ˆì¸¡/ì¶”ì¸¡ íŒ¨í„´
            r"could be worth",  # Could Be Worth
            r"might reach",  # Might Reach
            r"expected to soar",  # Expected to Soar
            r"set to explode",  # Set to Explode
            r"ready to breakout",  # Ready to Breakout
            
            # 10. ë¦¬ìŠ¤íŠ¸ + ë¶€ì •ì  í‰ê°€ ì¡°í•©
            r"\d+.*and \d+.*risky",  # 2 Good and 1 Risky
            r"\d+.*and \d+.*avoid",  # 3 Buy and 2 Avoid
            r"\d+.*but \d+",  # 5 Winners but 2 Losers
            
            # 11. ê³¼ì¥ëœ í˜•ìš©ì‚¬ + íˆ¬ì ê¶Œìœ 
            r"\d+\s+(phenomenal|amazing|incredible|unbelievable|extraordinary|fantastic|spectacular|outstanding|remarkable|exceptional)",  # 3 Phenomenal Stocks
            r"(phenomenal|amazing|incredible|unbelievable|extraordinary|fantastic|spectacular|outstanding|remarkable|exceptional).*stocks.*buy",  # Phenomenal Stocks to Buy
            
            # 12. ì¦‰ì‹œ êµ¬ë§¤ ê¶Œìœ  íŒ¨í„´
            r"buy.*right now",  # Buy Right Now
            r"buy.*now",  # Buy Now
            r"buy.*immediately",  # Buy Immediately
            r"buy.*today",  # Buy Today
            r"buy.*asap",  # Buy ASAP
            r"must buy.*now",  # Must Buy Now
            r"should buy.*now",  # Should Buy Now
            
            # 13. ë¹„í˜„ì‹¤ì  ê°€ê²© ì˜ˆì¸¡ (ì˜ë¬¸í˜•)
            r"can .* hit \$?\d+",  # Can Bitcoin Hit $600,000?
            r"will .* reach \$?\d+",  # Will Tesla Reach $1000?
            r"could .* hit \$?\d+",  # Could Ethereum Hit $10,000?
            r"might .* reach \$?\d+",  # Might Apple Reach $500?
            r"can .* reach \$?\d+",  # Can Stock Reach $X?
            r"will .* hit \$?\d+",  # Will Stock Hit $X?
            
            # 14. ê·¹ë‹¨ì  ê°€ê²© ëª©í‘œ íŒ¨í„´
            r"\$?\d{3,},\d{3}",  # $600,000 ê°™ì€ í° ìˆ«ì
            r"to \$?\d{4,}",  # to $10000 ì´ìƒ
            r"hit.*\d{3,}%",  # hit 500% ê°™ì€ ê·¹ë‹¨ì  í¼ì„¼íŠ¸
            r"surge.*\d{3,}%",  # surge 1000%
            r"soar.*\d{3,}%",  # soar 500%
            
            # 15. í´ë¦­ë² ì´íŠ¸ ì§ˆë¬¸ íŒ¨í„´
            r"^can .*\?$",  # Can ... ? ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ì œëª©
            r"^will .*\?$",  # Will ... ? ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ì œëª©
            r"^should you .*\?$",  # Should You ... ?
            r"^is this .*\?$",  # Is This ... ?
            r"^are these .*\?$",  # Are These ... ?
        ]
        
        # íŒ¨í„´ ë§¤ì¹­ ê²€ì‚¬
        for pattern in clickbait_patterns:
            if re.search(pattern, title_lower, re.IGNORECASE):
                return True
        
        # ì¶”ê°€ ì¡°ê±´: ì œëª©ì— ë‹¬ëŸ¬ ê¸ˆì•¡ê³¼ "ago", "today"ê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš°
        if "$" in title and ("ago" in title_lower and "today" in title_lower):
            return True
        
        # ì¶”ê°€ ì¡°ê±´: "Best", "Top", "Must" + "Buy"/"Sell" ì¡°í•©
        if re.search(r"(best|top|must).*(buy|sell|own|avoid)", title_lower):
            return True
        
        # ì¶”ê°€ ì¡°ê±´: ìˆ«ìë¡œ ì‹œì‘í•˜ê³  "Stocks"ê°€ í¬í•¨ëœ ì œëª©
        if re.match(r"^\d+\s+\w+\s+stocks", title_lower):
            return True
        
        # ì¶”ê°€ ì¡°ê±´: ê³¼ì¥ëœ í˜•ìš©ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì‹ ì¶”ì²œ
        if re.search(r"(phenomenal|amazing|incredible|unbelievable|extraordinary|fantastic|spectacular).*stocks", title_lower):
            return True
        
        # ì¶”ê°€ ì¡°ê±´: ê·¹ë‹¨ì ì¸ ê°€ê²© ì˜ˆì¸¡ (íŠ¹íˆ ì›” ë‹¨ìœ„ ì˜ˆì¸¡)
        if re.search(r"by (january|february|march|april|may|june|july|august|september|october|november|december)", title_lower) and "$" in title:
            return True
        
        # ì¶”ê°€ ì¡°ê±´: "Right Now" ê¸´ê¸‰ì„± ì¡°ì¥
        if "right now" in title_lower:
            return True
        
        return False
    
    def _is_etf_promotional_content(self, title: str) -> bool:
        """ETF í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ ë” ì •í™•í•˜ê²Œ íŒë‹¨"""
        title_lower = title.lower()
        
        # ğŸš¨ ëª…í™•í•œ ETF í™ë³´ì„± íŒ¨í„´ë“¤ (ì˜ë¬¸í˜•, ì¶”ì²œí˜•, ì†Œê°œí˜•)
        etf_promotional_patterns = [
            # ì˜ë¬¸í˜• íŒ¨í„´ (íˆ¬ì ê²°ì •ì„ ìš”êµ¬í•˜ëŠ” í˜•íƒœ)
            r'.*etf.*íˆ¬ì.*ë ˆì´ë”.*ì˜¬ë ¤ì•¼.*í• ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ê°€ì¹˜.*ìˆì„ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ê¸°íšŒ.*í• ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ì¶”ì²œ.*í• ê¹Œ\?',
            
            # ì†Œê°œ/ì¶”ì²œí˜• íŒ¨í„´
            r'.*etf.*ì†Œê°œ.*',
            r'.*etf.*ì¶”ì²œ.*',
            r'.*etf.*íˆ¬ì.*ì „ëµ.*',
            
            # íˆ¬ì ë ˆì´ë” ê´€ë ¨ (ëª…í™•í•œ í™ë³´ì„±)
            r'.*íˆ¬ì.*ë ˆì´ë”.*',
            r'.*íˆ¬ì.*ê¸°íšŒ.*',
            r'.*íˆ¬ì.*ê°€ì¹˜.*',
            r'.*íˆ¬ì.*í¬ì¸íŠ¸.*',
            r'.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*',
            r'.*íˆ¬ì.*ê²€í† .*',
            r'.*íˆ¬ì.*í‰ê°€.*',
            r'.*íˆ¬ì.*ì „ë§.*',
        ]
        
        # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ íŒ¨í„´ë“¤ ì¶”ê°€
        stock_promotional_patterns = [
            # ì˜ë¬¸í˜• íŒ¨í„´ (íˆ¬ì ê²°ì •ì„ ìš”êµ¬í•˜ëŠ” í˜•íƒœ)
            r'.*ì£¼ì‹.*ì–´ë–„\?.*',
            r'.*ì£¼ì‹.*íˆ¬ì.*í• ê¹Œ\?.*',
            r'.*íˆ¬ì.*ì ê¸°.*',
            r'.*íˆ¬ì.*íƒ€ì´ë°.*',
            r'.*ë§¤ìˆ˜.*ì‹œì .*',
            r'.*ë§¤ë„.*ì‹œì .*',
            r'.*ë§¤ìˆ˜.*íƒ€ì´ë°.*',
            r'.*ë§¤ë„.*íƒ€ì´ë°.*',
            
            # ì¶”ì²œ/ì œì•ˆí˜• íŒ¨í„´
            r'.*ì£¼ì‹.*ì¶”ì²œ.*',
            r'.*ì£¼ì‹.*ë§¤ìˆ˜.*',
            r'.*ì£¼ì‹.*ë§¤ë„.*',
            r'.*íˆ¬ì.*ì œì•ˆ.*',
            r'.*íˆ¬ì.*ì¶”ì²œ.*',
            r'.*ì£¼ê°€.*ì „ë§.*',
            r'.*ì£¼ê°€.*ì˜ˆì¸¡.*',
            r'.*ì£¼ê°€.*ë¶„ì„.*',
            r'.*ì£¼ê°€.*ì¶”ì²œ.*',
            r'.*ì¢…ëª©.*ì¶”ì²œ.*',
            r'.*ì¢…ëª©.*ë¶„ì„.*',
            r'.*ì¢…ëª©.*ì „ë§.*',
            r'.*ì¢…ëª©.*íˆ¬ì.*',
        ]
        
        # ğŸš¨ ì •ìƒì ì¸ ETF ë‰´ìŠ¤ íŒ¨í„´ë“¤ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        etf_normal_patterns = [
            r'.*etf.*ì‹œì¥.*ë™í–¥.*',
            r'.*etf.*ì„±ê³¼.*ë¶„ì„.*',
            r'.*etf.*ìˆ˜ìµë¥ .*',
            r'.*etf.*ìì‚°.*ê·œëª¨.*',
            r'.*etf.*ìƒì¥.*',
            r'.*etf.*íì§€.*',
            r'.*etf.*ìš´ìš©ì‚¬.*',
            r'.*etf.*íˆ¬ìì.*',
        ]
        
        # ğŸš¨ ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ íŒ¨í„´ë“¤ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        stock_normal_patterns = [
            r'.*ì£¼ê°€.*ìƒìŠ¹.*',
            r'.*ì£¼ê°€.*í•˜ë½.*',
            r'.*ì£¼ê°€.*ë³€ë™.*',
            r'.*ì£¼ê°€.*ë™í–¥.*',
            r'.*ì£¼ê°€.*ì„±ê³¼.*',
            r'.*ì£¼ê°€.*ì‹¤ì .*',
            r'.*ì£¼ê°€.*ë°œí‘œ.*',
            r'.*ì£¼ê°€.*ì‹œì¥.*',
            r'.*ì¢…ëª©.*ì‹œì¥.*',
            r'.*ì¢…ëª©.*ë™í–¥.*',
            r'.*ì¢…ëª©.*ì„±ê³¼.*',
            r'.*ì¢…ëª©.*ì‹¤ì .*',
        ]
        
        # ì •ìƒì ì¸ ETF ë‰´ìŠ¤ì¸ì§€ ë¨¼ì € í™•ì¸
        for pattern in etf_normal_patterns:
            if re.search(pattern, title_lower):
                return False  # ì •ìƒì ì¸ ETF ë‰´ìŠ¤ëŠ” í™ë³´ì„± ì•„ë‹˜
        
        # ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ì¸ì§€ ë¨¼ì € í™•ì¸
        for pattern in stock_normal_patterns:
            if re.search(pattern, title_lower):
                return False  # ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ëŠ” í™ë³´ì„± ì•„ë‹˜
        
        # ETF í™ë³´ì„± íŒ¨í„´ í™•ì¸
        for pattern in etf_promotional_patterns:
            if re.search(pattern, title_lower):
                return True
        
        # ì£¼ì‹ í™ë³´ì„± íŒ¨í„´ í™•ì¸
        for pattern in stock_promotional_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def _extract_news_links(self, html: str) -> List[Dict[str, str]]:
        """HTMLì—ì„œ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)"""
        soup = BeautifulSoup(html, 'html.parser')
        news_links = []
        
        # Yahoo Finance ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
        if 'finance.yahoo.com' in self.base_url:
            # ëª¨ë“  ë§í¬ë¥¼ í™•ì¸í•˜ê³  ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë§Œ í•„í„°ë§
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨ (ì„±ëŠ¥ ìµœì í™”)
                if len(news_links) >= self.max_news * 2:  # ì—¬ìœ ë¶„ í¬í•¨
                    break
                    
                href = link.get('href')
                if not href:
                    continue
                
                text = link.get_text(strip=True)
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ íŒ¨í„´ í™•ì¸
                if (self._is_valid_news_link(str(href)) and 
                    len(text) > 20 and  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì œëª©
                    text != 'Ad' and  # ê´‘ê³  ì œì™¸
                    not text.startswith('See more') and  # ë”ë³´ê¸° ë§í¬ ì œì™¸
                    not text.startswith('View')):  # ë·° ë§í¬ ì œì™¸
                    
                    full_url = urljoin(self.base_url, str(href))
                    news_links.append({
                        'url': full_url,
                        'title': text
                    })
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_links = []
        for link in news_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
                
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨
                if len(unique_links) >= self.max_news:
                    break
        
        return unique_links
    
    def _fetch_html(self) -> Optional[str]:
        """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ë” ê°€ë²¼ìš´ ë²„ì „)"""
        try:
            # ë‹¨ìˆœí•œ ìºì‹œ ë°©ì§€ (ì„±ëŠ¥ ìµœì í™”)
            cache_buster = int(time.time() % 86400)  # í•˜ë£¨ ë‹¨ìœ„ë¡œ ìˆœí™˜
            url = f"{self.base_url}?_cb={cache_buster}"
            
            # ìµœì í™”ëœ í—¤ë” (í•„ìˆ˜ë§Œ)
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            response = self.session.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            print(f"[OPTIMIZED] ìš”ì²­ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"[OPTIMIZED] ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"[OPTIMIZED] ì‘ë‹µ ê¸¸ì´: {len(response.text):,} ë¬¸ì")
            
            return response.text
            
        except Exception as e:
            logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _is_valid_news_link(self, url: str) -> bool:
        """ìœ íš¨í•œ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        # ì˜ëª»ëœ íŒ¨í„´ í™•ì¸
        for pattern in self.invalid_patterns:
            if re.search(pattern, url):
                return False
        
        # ë‰´ìŠ¤ URL íŒ¨í„´ í™•ì¸
        news_patterns = [
            r'/news/[a-zA-Z0-9-]+\.html',  # í‘œì¤€ ë‰´ìŠ¤ URL
            r'/news/[a-zA-Z0-9-]+-\d+\.html',  # ë‰´ìŠ¤ ID í¬í•¨
        ]
        
        for pattern in news_patterns:
            if re.search(pattern, url):
                return True
        
        return False
    
    def _create_news_items(self, links: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±"""
        news_items = []
        
        for link in links:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            keywords = self._extract_keywords(link['title'])
            
            news_item = {
                'title': link['title'],
                'url': link['url'],
                'keywords': keywords,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            }
            
            news_items.append(news_item)
        
        return news_items
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
        keywords = []
        
        # ëŒ€ë¬¸ì ë‹¨ì–´ ì¶”ì¶œ (íšŒì‚¬ëª…, ë¸Œëœë“œëª… ë“±)
        uppercase_words = re.findall(r'\b[A-Z][A-Z0-9]+\b', title)
        keywords.extend(uppercase_words[:3])  # ìµœëŒ€ 3ê°œ
        
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b(Bitcoin|BTC|Ethereum|ETH|Tesla|TSLA|Apple|AAPL|Google|GOOGL|Amazon|AMZN|Microsoft|MSFT|Meta|META|Netflix|NFLX|Nvidia|NVDA)\b',
            r'\b(crypto|cryptocurrency|blockchain|AI|artificial intelligence|stock|shares|market|trading|investment)\b',
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, title, re.IGNORECASE)
            keywords.extend(matches[:2])  # ìµœëŒ€ 2ê°œì”©
        
        return list(set(keywords))[:5]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 5ê°œ


def extract_news_from_multiple_sources(sources: List[Dict[str, Any]], 
                                     keyword: str = "", 
                                     count: int = 10, 
                                     max_workers: int = 3) -> List[Dict[str, Any]]:
    """ì—¬ëŸ¬ ì¶œì²˜ì—ì„œ ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ì¶”ì¶œ (íŒŒì„œ íƒ€ì…ë³„ ë¶„ê¸° ì§€ì›)"""
    
    print(f"ë³‘ë ¬ ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {len(sources)}ê°œ ì¶œì²˜")
    start_time = time.time()
    
    all_news = []
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‰´ìŠ¤ ì¶”ì¶œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ì¶œì²˜ë³„ë¡œ ì¶”ì¶œ ì‘ì—… ì œì¶œ
        future_to_source = {}
        
        for source in sources:
            # full_urlì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ url ì‚¬ìš©
            base_url = source.get('full_url', source['url'])
            
            # ğŸ”§ íŒŒì„œ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬ (ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
            parser_type = source.get('parser_type', 'universal')
            
            if parser_type == 'naver_news':
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© íŒŒì„œ ì‚¬ìš©
                from naver_news_parser import NaverNewsExtractor
                extractor = NaverNewsExtractor(
                    base_url=base_url,
                    search_keywords=keyword if keyword else None,
                    max_news=count
                )
            else:
                # ê¸°ì¡´ ë²”ìš© íŒŒì„œ ì‚¬ìš© (Yahoo Finance ë“±)
                extractor = OptimizedNewsExtractor(
                    base_url=base_url,
                    search_keywords=keyword if keyword else None,
                    max_news=count
                )
            
            future = executor.submit(extractor.extract_news)
            future_to_source[future] = source
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_source):
            source = future_to_source[future]
            try:
                news_items = future.result()
                
                # ë””ë²„ê¹…: ì‹¤ì œ ì¶”ì¶œëœ ë‰´ìŠ¤ í™•ì¸
                print(f"[DEBUG] {source['name']} ì¶”ì¶œ ê²°ê³¼:")
                for i, item in enumerate(news_items):
                    print(f"  {i+1}. {item['title'][:50]}...")
                    print(f"     URL: {item['url']}")
                
                # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                for item in news_items:
                    item['source_id'] = source['id']
                    item['source_name'] = source['name']
                    item['source_url'] = source['url']
                
                all_news.extend(news_items)
                
                print(f"âœ… {source['name']}: {len(news_items)}ê°œ")
                
            except Exception as e:
                logger.error(f"âŒ {source['name']}: {e}")
                import traceback
                print(f"[DEBUG] Exception traceback:")
                print(traceback.format_exc())
    
    end_time = time.time()
    print(f"ê³ ì„±ëŠ¥ ë³‘ë ¬ ì¶”ì¶œ: {len(all_news)}ê°œ ë‰´ìŠ¤, {end_time - start_time:.2f}ì´ˆ")
    
    return all_news


def test_extractor():
    """ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸"""
    sources = [
        {
            'id': 'test_crypto',
            'name': 'Yahoo Finance Crypto Test',
            'url': 'https://finance.yahoo.com/topic/crypto/'
        }
    ]
    
    news = extract_news_from_multiple_sources(sources, count=3)
    
    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    for item in news:
        print(f"ì œëª©: {item['title']}")
        print(f"URL: {item['url']}")
        print(f"í‚¤ì›Œë“œ: {item['keywords']}")
        print("---")


if __name__ == "__main__":
    test_extractor()
