import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random
from typing import List, Dict, Optional, Any
import logging
from fake_useragent import UserAgent
import re
from urllib.parse import urljoin, urlparse
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimizedNewsExtractor:
    """ë¹ ë¥´ê³  ê°€ë²¼ìš´ ë‰´ìŠ¤ ì¶”ì¶œê¸°"""
    
    def __init__(self, base_url: str, search_keywords: Optional[str] = None, max_news: int = 10):
        self.base_url = base_url
        self.search_keywords = search_keywords
        self.max_news = max_news
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”) - ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•œ ë‹¨ì¶•
        self.timeout = 12
        
        # ì˜ëª»ëœ ë§í¬ íŒ¨í„´ (ì£¼ì‹ ticker ë“±)
        self.invalid_patterns = [
            r'/quote/[A-Z0-9]+-USD',  # ì•”í˜¸í™”í ticker
            r'/quote/[A-Z0-9]+\..*',  # ì£¼ì‹ ticker with exchange
            r'/quote/[A-Z]{1,5}$',    # ì£¼ì‹ ticker
            r'/symbol/[A-Z0-9]+',     # ì‹¬ë³¼ í˜ì´ì§€
            r'/lookup/',              # ê²€ìƒ‰ í˜ì´ì§€
            r'/calendar/',            # ìº˜ë¦°ë”
            r'/screener/',            # ìŠ¤í¬ë¦¬ë„ˆ
            r'/research/',            # ë¦¬ì„œì¹˜
            r'/premium/',             # í”„ë¦¬ë¯¸ì—„
            r'/video/',               # ë¹„ë””ì˜¤
            r'/author/',              # ì €ì í˜ì´ì§€
            r'/topic/[^/]+/$',        # í† í”½ ë©”ì¸ í˜ì´ì§€ (ë‰´ìŠ¤ ì•„ë‹˜)
        ]
    
    def extract_news(self) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜"""
        try:
            print(f"ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {self.base_url}")
            
            # ë¹ ë¥¸ HTML ìš”ì²­
            html_content = self._fetch_html()
            if not html_content:
                return []
            
            # ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
            news_links = self._extract_news_links(html_content)
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
            news_items = self._create_news_items(news_links)
            
            print(f"HTML íŒŒì‹±ì—ì„œ {len(news_items)}ê°œ ë‰´ìŠ¤ ì¶”ì¶œ ì™„ë£Œ")
            return news_items[:self.max_news]
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_news_links(self, html: str) -> List[Dict[str, str]]:
        """HTMLì—ì„œ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)"""
        soup = BeautifulSoup(html, 'html.parser')
        news_links = []
        
        # Yahoo Finance ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
        if 'finance.yahoo.com' in self.base_url:
            # ëª¨ë“  ë§í¬ë¥¼ í™•ì¸í•˜ê³  ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ë§Œ í•„í„°ë§
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨ (ì„±ëŠ¥ ìµœì í™”)
                if len(news_links) >= self.max_news * 2:  # ì—¬ìœ ë¶„ í¬í•¨
                    break
                    
                href = link.get('href')
                if not href:
                    continue
                
                text = link.get_text(strip=True)
                
                # ë‰´ìŠ¤ ê¸°ì‚¬ íŒ¨í„´ í™•ì¸
                if (self._is_valid_news_link(str(href)) and 
                    len(text) > 20 and  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì œëª©
                    text != 'Ad' and  # ê´‘ê³  ì œì™¸
                    not text.startswith('See more') and  # ë”ë³´ê¸° ë§í¬ ì œì™¸
                    not text.startswith('View')):  # ë·° ë§í¬ ì œì™¸
                    
                    full_url = urljoin(self.base_url, str(href))
                    news_links.append({
                        'url': full_url,
                        'title': text
                    })
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_links = []
        for link in news_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
                
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨
                if len(unique_links) >= self.max_news:
                    break
        
        return unique_links
    
    def _fetch_html(self) -> Optional[str]:
        """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ë” ê°€ë²¼ìš´ ë²„ì „)"""
        try:
            # ë‹¨ìˆœí•œ ìºì‹œ ë°©ì§€ (ì„±ëŠ¥ ìµœì í™”)
            cache_buster = int(time.time() % 86400)  # í•˜ë£¨ ë‹¨ìœ„ë¡œ ìˆœí™˜
            url = f"{self.base_url}?_cb={cache_buster}"
            
            # ìµœì í™”ëœ í—¤ë” (í•„ìˆ˜ë§Œ)
            headers = {
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            response = self.session.get(url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            print(f"[OPTIMIZED] ìš”ì²­ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"[OPTIMIZED] ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"[OPTIMIZED] ì‘ë‹µ ê¸¸ì´: {len(response.text):,} ë¬¸ì")
            
            return response.text
            
        except Exception as e:
            logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _is_valid_news_link(self, url: str) -> bool:
        """ìœ íš¨í•œ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        # ì˜ëª»ëœ íŒ¨í„´ í™•ì¸
        for pattern in self.invalid_patterns:
            if re.search(pattern, url):
                return False
        
        # ë‰´ìŠ¤ URL íŒ¨í„´ í™•ì¸
        news_patterns = [
            r'/news/[a-zA-Z0-9-]+\.html',  # í‘œì¤€ ë‰´ìŠ¤ URL
            r'/news/[a-zA-Z0-9-]+-\d+\.html',  # ë‰´ìŠ¤ ID í¬í•¨
        ]
        
        for pattern in news_patterns:
            if re.search(pattern, url):
                return True
        
        return False
    
    def _create_news_items(self, links: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±"""
        news_items = []
        
        for link in links:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            keywords = self._extract_keywords(link['title'])
            
            news_item = {
                'title': link['title'],
                'url': link['url'],
                'keywords': keywords,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            }
            
            news_items.append(news_item)
        
        return news_items
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
        keywords = []
        
        # ëŒ€ë¬¸ì ë‹¨ì–´ ì¶”ì¶œ (íšŒì‚¬ëª…, ë¸Œëœë“œëª… ë“±)
        uppercase_words = re.findall(r'\b[A-Z][A-Z0-9]+\b', title)
        keywords.extend(uppercase_words[:3])  # ìµœëŒ€ 3ê°œ
        
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b(Bitcoin|BTC|Ethereum|ETH|Tesla|TSLA|Apple|AAPL|Google|GOOGL|Amazon|AMZN|Microsoft|MSFT|Meta|META|Netflix|NFLX|Nvidia|NVDA)\b',
            r'\b(crypto|cryptocurrency|blockchain|AI|artificial intelligence|stock|shares|market|trading|investment)\b',
        ]
        
        for pattern in important_patterns:
            matches = re.findall(pattern, title, re.IGNORECASE)
            keywords.extend(matches[:2])  # ìµœëŒ€ 2ê°œì”©
        
        return list(set(keywords))[:5]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 5ê°œ


def extract_news_from_multiple_sources(sources: List[Dict[str, Any]], 
                                     keyword: str = "", 
                                     count: int = 10, 
                                     max_workers: int = 3) -> List[Dict[str, Any]]:
    """ì—¬ëŸ¬ ì¶œì²˜ì—ì„œ ë³‘ë ¬ë¡œ ë‰´ìŠ¤ ì¶”ì¶œ (íŒŒì„œ íƒ€ì…ë³„ ë¶„ê¸° ì§€ì›)"""
    
    print(f"ë³‘ë ¬ ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {len(sources)}ê°œ ì¶œì²˜")
    start_time = time.time()
    
    all_news = []
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‰´ìŠ¤ ì¶”ì¶œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ì¶œì²˜ë³„ë¡œ ì¶”ì¶œ ì‘ì—… ì œì¶œ
        future_to_source = {}
        
        for source in sources:
            # full_urlì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ url ì‚¬ìš©
            base_url = source.get('full_url', source['url'])
            
            # ğŸ”§ íŒŒì„œ íƒ€ì…ë³„ ë¶„ê¸° ì²˜ë¦¬ (ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
            parser_type = source.get('parser_type', 'universal')
            
            if parser_type == 'naver_news':
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© íŒŒì„œ ì‚¬ìš©
                from naver_news_parser import NaverNewsExtractor
                extractor = NaverNewsExtractor(
                    base_url=base_url,
                    search_keywords=keyword if keyword else None,
                    max_news=count
                )
            else:
                # ê¸°ì¡´ ë²”ìš© íŒŒì„œ ì‚¬ìš© (Yahoo Finance ë“±)
                extractor = OptimizedNewsExtractor(
                    base_url=base_url,
                    search_keywords=keyword if keyword else None,
                    max_news=count
                )
            
            future = executor.submit(extractor.extract_news)
            future_to_source[future] = source
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_source):
            source = future_to_source[future]
            try:
                news_items = future.result()
                
                # ë””ë²„ê¹…: ì‹¤ì œ ì¶”ì¶œëœ ë‰´ìŠ¤ í™•ì¸
                print(f"[DEBUG] {source['name']} ì¶”ì¶œ ê²°ê³¼:")
                for i, item in enumerate(news_items):
                    print(f"  {i+1}. {item['title'][:50]}...")
                    print(f"     URL: {item['url']}")
                
                # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                for item in news_items:
                    item['source_id'] = source['id']
                    item['source_name'] = source['name']
                    item['source_url'] = source['url']
                
                all_news.extend(news_items)
                
                print(f"âœ… {source['name']}: {len(news_items)}ê°œ")
                
            except Exception as e:
                logger.error(f"âŒ {source['name']}: {e}")
                import traceback
                print(f"[DEBUG] Exception traceback:")
                print(traceback.format_exc())
    
    end_time = time.time()
    print(f"ê³ ì„±ëŠ¥ ë³‘ë ¬ ì¶”ì¶œ: {len(all_news)}ê°œ ë‰´ìŠ¤, {end_time - start_time:.2f}ì´ˆ")
    
    return all_news


def test_extractor():
    """ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸"""
    sources = [
        {
            'id': 'test_crypto',
            'name': 'Yahoo Finance Crypto Test',
            'url': 'https://finance.yahoo.com/topic/crypto/'
        }
    ]
    
    news = extract_news_from_multiple_sources(sources, count=3)
    
    print(f"\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    for item in news:
        print(f"ì œëª©: {item['title']}")
        print(f"URL: {item['url']}")
        print(f"í‚¤ì›Œë“œ: {item['keywords']}")
        print("---")


if __name__ == "__main__":
    test_extractor()
