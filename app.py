#!/usr/bin/env python3

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import os
import traceback
import logging
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
import json
import uuid
from dotenv import load_dotenv

# Load environment variables from env.local file
load_dotenv('env.local')

from url_extractor import OptimizedNewsExtractor
from nongbuxx_generator import NongbuxxGenerator
from x_publisher import XPublisher
from scheduler_service import get_scheduler
from x_crawler import get_crawler
import asyncio

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Flask app (API only - no static file serving)
app = Flask(__name__)

# CORS configuration for production deployment
CORS(app, origins=[
    "http://localhost:3000",  # Local development
    "http://localhost:5000",  # Local development 
    "http://localhost:8080",  # Local development
    "https://nongbuxxfrontend.vercel.app",  # Main production frontend domain
    "https://nongbuxx.vercel.app",  # Alternative domain
    "https://*.vercel.app",  # All Vercel deployments
], allow_headers=["Content-Type", "Authorization", "Accept", "X-Requested-With"], 
methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"], 
supports_credentials=True)

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
app.config['UPLOAD_FOLDER'] = 'uploads'

# Create necessary directories
os.makedirs('uploads', exist_ok=True)
os.makedirs('generated_content', exist_ok=True)

# Store active jobs in memory (for production, use Redis or database)
active_jobs = {}

# ğŸš€ ì„±ëŠ¥ ìµœì í™”: ë©”ëª¨ë¦¬ ìºì‹± ì‹œìŠ¤í…œ
content_cache = {}  # URLë³„ ìƒì„±ëœ ì½˜í…ì¸  ìºì‹±
cache_expiry = {}   # ìºì‹œ ë§Œë£Œ ì‹œê°„ ê´€ë¦¬

# ìºì‹± í—¬í¼ í•¨ìˆ˜ë“¤
def normalize_url(url):
    """URL ì •ê·œí™” (ìºì‹± í‚¤ ìƒì„±ìš©)"""
    import re
    # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°, ìŠ¬ë˜ì‹œ ì •ê·œí™”
    url = re.sub(r'\?.*$', '', url)
    url = url.rstrip('/')
    return url.lower()

def get_cache_key(url, content_type):
    """ìºì‹œ í‚¤ ìƒì„±"""
    return f"{normalize_url(url)}:{content_type}"

def is_cache_valid(cache_key):
    """ìºì‹œ ìœ íš¨ì„± í™•ì¸ (30ë¶„ ìœ íš¨)"""
    if cache_key not in cache_expiry:
        return False
    return time.time() < cache_expiry[cache_key]

def get_cached_content(url, content_type):
    """ìºì‹œëœ ì½˜í…ì¸  ì¡°íšŒ"""
    cache_key = get_cache_key(url, content_type)
    if cache_key in content_cache and is_cache_valid(cache_key):
        logger.info(f"ğŸš€ ìºì‹œ íˆíŠ¸: {url} (íƒ€ì…: {content_type})")
        return content_cache[cache_key]
    return None

def set_cached_content(url, content_type, content):
    """ì½˜í…ì¸  ìºì‹±"""
    cache_key = get_cache_key(url, content_type)
    content_cache[cache_key] = content
    cache_expiry[cache_key] = time.time() + 1800  # 30ë¶„ í›„ ë§Œë£Œ
    logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {url} (íƒ€ì…: {content_type})")

def cleanup_expired_cache():
    """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
    current_time = time.time()
    expired_keys = [k for k, exp_time in cache_expiry.items() if current_time > exp_time]
    for key in expired_keys:
        content_cache.pop(key, None)
        cache_expiry.pop(key, None)
    if expired_keys:
        logger.info(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ ì •ë¦¬ ì™„ë£Œ")


# ê¸°ë³¸ ë¼ìš°íŠ¸ë“¤

# API ì„œë²„ë§Œ ì œê³µ - í”„ë¡ íŠ¸ì—”ë“œëŠ” Vercelì—ì„œ ì„œë¹™

@app.route('/favicon.ico')
def favicon():
    """favicon ìš”ì²­ ì²˜ë¦¬"""
    return '', 204  # No Content ì‘ë‹µìœ¼ë¡œ favicon ìš”ì²­ ë¬´ì‹œ

@app.route('/api/health', methods=['GET', 'OPTIONS'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

@app.route('/api/validate-key', methods=['POST', 'OPTIONS'])
def validate_api_key():
    """API í‚¤ ìœ íš¨ì„± ê²€ì¦ ì—”ë“œí¬ì¸íŠ¸"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        data = request.get_json()
        
        if not data or 'api_provider' not in data or 'api_key' not in data:
            return jsonify({
                'success': False,
                'error': 'API provider and API key are required',
                'code': 'MISSING_PARAMETERS'
            }), 400
        
        api_provider = data['api_provider']
        api_key = data['api_key']
        
        if api_provider not in ['anthropic', 'openai', 'perplexity']:
            return jsonify({
                'success': False,
                'error': 'API provider must be anthropic, openai, or perplexity',
                'code': 'INVALID_API_PROVIDER'
            }), 400
        
        if not api_key or len(api_key.strip()) < 10:
            return jsonify({
                'success': False,
                'error': 'Invalid API key format',
                'code': 'INVALID_API_KEY'
            }), 400
        
        # API í‚¤ í˜•ì‹ ê¸°ë³¸ ê²€ì¦
        if api_provider == 'anthropic':
            if not api_key.startswith('sk-ant-'):
                return jsonify({
                    'success': False,
                    'error': 'Invalid Anthropic API key format',
                    'code': 'INVALID_ANTHROPIC_KEY'
                }), 400
        elif api_provider == 'openai':
            if not api_key.startswith('sk-'):
                return jsonify({
                    'success': False,
                    'error': 'Invalid OpenAI API key format',
                    'code': 'INVALID_OPENAI_KEY'
                }), 400
        elif api_provider == 'perplexity':
            if not api_key.startswith('pplx-'):
                return jsonify({
                    'success': False,
                    'error': 'Invalid Perplexity API key format (should start with pplx-)',
                    'code': 'INVALID_PERPLEXITY_KEY'
                }), 400
        
        # ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ìœ íš¨ì„± ê²€ì¦
        try:
            if api_provider == 'anthropic':
                import anthropic
                client = anthropic.Anthropic(api_key=api_key)
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë¡œ ìœ íš¨ì„± í™•ì¸
                response = client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=10,
                    messages=[{"role": "user", "content": "Hello"}]
                )
                
            elif api_provider == 'openai':
                import openai
                client = openai.OpenAI(api_key=api_key)
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œë¡œ ìœ íš¨ì„± í™•ì¸
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    max_tokens=10,
                    messages=[{"role": "user", "content": "Hello"}]
                )
                
            elif api_provider == 'perplexity':
                import requests
                # Perplexity API í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
                headers = {
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }
                data = {
                    'model': 'llama-3.1-sonar-small-128k-online',
                    'messages': [{'role': 'user', 'content': 'Hello'}],
                    'max_tokens': 10
                }
                response = requests.post(
                    'https://api.perplexity.ai/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=30
                )
                if response.status_code != 200:
                    raise Exception(f"Perplexity API error: {response.status_code} - {response.text}")
            
            return jsonify({
                'success': True,
                'message': 'API key is valid',
                'provider': api_provider
            })
            
        except Exception as e:
            error_msg = str(e)
            if 'Incorrect API key' in error_msg or 'Invalid API key' in error_msg:
                return jsonify({
                    'success': False,
                    'error': 'Invalid API key',
                    'code': 'INVALID_API_KEY'
                }), 401
            elif 'exceeded' in error_msg.lower():
                return jsonify({
                    'success': False,
                    'error': 'API rate limit exceeded',
                    'code': 'RATE_LIMIT_EXCEEDED'
                }), 429
            else:
                return jsonify({
                    'success': False,
                    'error': f'API validation failed: {error_msg}',
                    'code': 'API_VALIDATION_FAILED'
                }), 400
                
    except Exception as e:
        logger.error(f"API key validation error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error',
            'code': 'INTERNAL_ERROR'
        }), 500

@app.route('/api/generate', methods=['POST', 'OPTIONS'])
def generate_content():
    """
    ì½˜í…ì¸  ìƒì„± API
    
    Request Body:
    {
        "url": "https://example.com/news",
        "api_provider": "anthropic",  // required: anthropic or openai
        "api_key": "sk-...",         // required: user's API key
        "filename": "custom_name",    // optional
        "save_intermediate": true,    // optional
        "content_type": "standard"    // optional: 'standard' or 'blog'
    }
    """
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = request.get_json()
        
        if not data or 'url' not in data:
            return jsonify({
                'success': False,
                'error': 'URL is required',
                'code': 'MISSING_URL'
            }), 400
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        required_fields = ['url', 'api_provider', 'api_key']
        for field in required_fields:
            if field not in data or not data[field]:
                return jsonify({
                    'success': False,
                    'error': f'{field} is required',
                    'code': f'MISSING_{field.upper()}'
                }), 400
        
        url = data['url']
        api_provider = data['api_provider']
        api_key = data['api_key']
        custom_filename = data.get('filename')
        save_intermediate = data.get('save_intermediate', False)
        content_type = data.get('content_type', 'standard')  # ê¸°ë³¸ê°’ì€ 'standard'
        
        # URL ê¸°ë³¸ ê²€ì¦
        if not url.startswith(('http://', 'https://')):
            return jsonify({
                'success': False,
                'error': 'Invalid URL format',
                'code': 'INVALID_URL'
            }), 400
        
        # API ì œê³µì ê²€ì¦
        if api_provider not in ['anthropic', 'openai', 'perplexity']:
            return jsonify({
                'success': False,
                'error': 'API provider must be anthropic, openai, or perplexity',
                'code': 'INVALID_API_PROVIDER'
            }), 400
        
        # ì½˜í…ì¸  íƒ€ì… ê²€ì¦
        if content_type not in ['standard', 'blog', 'enhanced_blog', 'threads', 'x']:
            return jsonify({
                'success': False,
                'error': 'Content type must be standard, blog, enhanced_blog, threads, or x',
                'code': 'INVALID_CONTENT_TYPE'
            }), 400
        
        # API í‚¤ ê¸°ë³¸ í˜•ì‹ ê²€ì¦
        if api_provider == 'anthropic' and not api_key.startswith('sk-ant-'):
            return jsonify({
                'success': False,
                'error': 'Invalid Anthropic API key format',
                'code': 'INVALID_ANTHROPIC_KEY'
            }), 400
        elif api_provider == 'openai' and not api_key.startswith('sk-'):
            return jsonify({
                'success': False,
                'error': 'Invalid OpenAI API key format',
                'code': 'INVALID_OPENAI_KEY'
            }), 400
        elif api_provider == 'perplexity' and not api_key.startswith('pplx-'):
            return jsonify({
                'success': False,
                'error': 'Invalid Perplexity API key format (should start with pplx-)',
                'code': 'INVALID_PERPLEXITY_KEY'
            }), 400
        
        # ì‘ì—… ID ìƒì„±
        job_id = str(uuid.uuid4())
        active_jobs[job_id] = {
            'status': 'processing',
            'url': url,
            'content_type': content_type,
            'started_at': datetime.now().isoformat(),
            'progress': 0
        }
        
        logger.info(f"Starting content generation for URL: {url} (Job ID: {job_id}, Type: {content_type})")
        
        # ğŸš€ ìºì‹œ í™•ì¸ (ì¦‰ì‹œ ì‘ë‹µ ê°€ëŠ¥)
        cached_result = get_cached_content(url, content_type)
        if cached_result:
            active_jobs[job_id].update({
                'status': 'completed',
                'progress': 100,
                'completed_at': datetime.now().isoformat(),
                'cached': True
            })
            return jsonify({
                'success': True,
                'job_id': job_id,
                'data': cached_result,
                'cached': True,
                'message': 'ìºì‹œëœ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.'
            })
        
        # NONGBUXX ìƒì„±ê¸° ì´ˆê¸°í™” (ì‚¬ìš©ì ì œê³µ API í‚¤ ì‚¬ìš©)
        generator = NongbuxxGenerator(
            api_provider=api_provider,
            api_key=api_key,
            save_intermediate=save_intermediate
        )
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        active_jobs[job_id]['progress'] = 25
        active_jobs[job_id]['status'] = 'extracting'
        
        # ì½˜í…ì¸  ìƒì„± (ì½˜í…ì¸  íƒ€ì… ì „ë‹¬)
        result = generator.generate_content(url, custom_filename, content_type)
        
        # ì •ë¦¬
        generator.cleanup()
        
        if result['success']:
            # ìƒì„±ëœ íŒŒì¼ ì½ê¸°
            with open(result['output_file'], 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
            active_jobs[job_id].update({
                'status': 'completed',
                'progress': 100,
                'completed_at': datetime.now().isoformat(),
                'output_file': str(result['output_file']),
                'title': result['title'],
                'content_type': result['content_type']
            })
            
            logger.info(f"Content generation completed for job {job_id} (Type: {content_type})")
            
            # ğŸš€ ê²°ê³¼ ìºì‹± ì €ì¥
            response_data = {
                'title': result['title'],
                'content': content,
                'output_file': str(result['output_file']),
                'timestamp': result['timestamp'],
                'url': url,
                'api_provider': api_provider,
                'content_type': result['content_type']
            }
            set_cached_content(url, content_type, response_data)
            
            return jsonify({
                'success': True,
                'job_id': job_id,
                'data': response_data,
                'cached': False
            })
        else:
            # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
            active_jobs[job_id].update({
                'status': 'failed',
                'error': result['error'],
                'completed_at': datetime.now().isoformat()
            })
            
            logger.error(f"Content generation failed for job {job_id}: {result['error']}")
            
            # ì—ëŸ¬ ë©”ì‹œì§€ì— ë”°ë¼ ì ì ˆí•œ HTTP ìƒíƒœ ì½”ë“œ ë°˜í™˜
            error_msg = result['error']
            if 'ì°¨ë‹¨' in error_msg or '403' in error_msg:
                # ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì ‘ê·¼ì„ ì°¨ë‹¨í•œ ê²½ìš°
                return jsonify({
                    'success': False,
                    'job_id': job_id,
                    'error': error_msg,
                    'code': 'ACCESS_BLOCKED'
                }), 403
            elif 'ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' in error_msg or '404' in error_msg:
                # í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
                return jsonify({
                    'success': False,
                    'job_id': job_id,
                    'error': error_msg,
                    'code': 'PAGE_NOT_FOUND'
                }), 404
            else:
                # ê¸°íƒ€ ì˜¤ë¥˜
                return jsonify({
                    'success': False,
                    'job_id': job_id,
                    'error': error_msg,
                    'code': 'GENERATION_FAILED'
            }), 500
            
    except Exception as e:
        logger.error(f"Content generation error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error',
            'code': 'INTERNAL_ERROR'
        }), 500

@app.route('/api/publish/x', methods=['POST', 'OPTIONS'])
def publish_to_x():
    """X(Twitter)ì— ì½˜í…ì¸  ê²Œì‹œ"""
    if request.method == 'OPTIONS':
        # CORS preflight ìš”ì²­ ì²˜ë¦¬
        return jsonify({'success': True}), 200
    
    try:
        data = request.json
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
        required_fields = ['content', 'consumer_key', 'consumer_secret', 
                          'access_token', 'access_token_secret']
        
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {field}',
                    'code': 'MISSING_FIELD'
                }), 400
        
        content = data['content']
        consumer_key = data['consumer_key']
        consumer_secret = data['consumer_secret']
        access_token = data['access_token']
        access_token_secret = data['access_token_secret']
        
        # ì˜µì…˜ íŒŒë¼ë¯¸í„°
        publish_as_thread = data.get('publish_as_thread', False)
        
        # X Publisher ì´ˆê¸°í™”
        publisher = XPublisher(
            consumer_key=consumer_key,
            consumer_secret=consumer_secret,
            access_token=access_token,
            access_token_secret=access_token_secret
        )
        
        # ì¸ì¦ í™•ì¸ ì œê±° - post_tweetì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
        # verify_credentials() í˜¸ì¶œ ì œê±°ë¡œ API 1íšŒ ì ˆì•½
        
        # ì½˜í…ì¸  í¬ë§·íŒ…
        formatted_content = publisher.format_content_for_x(content)
        
        # ê²Œì‹œ ì²˜ë¦¬
        if len(formatted_content) > 280 and publish_as_thread:
            # ìŠ¤ë ˆë“œë¡œ ê²Œì‹œ
            tweets = publisher.split_long_content(formatted_content)
            result = publisher.post_thread(tweets)
        else:
            # ë‹¨ì¼ íŠ¸ìœ— ê²Œì‹œ
            if len(formatted_content) > 280:
                # 280ì ì´ˆê³¼ ì‹œ ìë™ ì¶•ì•½
                formatted_content = formatted_content[:277] + '...'
            
            result = publisher.post_tweet(formatted_content)
        
        if result['success']:
            logger.info(f"X ê²Œì‹œ ì„±ê³µ: {result.get('tweet_url', 'URL ì—†ìŒ')}")
            return jsonify({
                'success': True,
                'data': result,
                'message': 'Xì— ì„±ê³µì ìœ¼ë¡œ ê²Œì‹œë˜ì—ˆìŠµë‹ˆë‹¤'
            }), 200
        else:
            logger.error(f"X ê²Œì‹œ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return jsonify({
                'success': False,
                'error': result.get('error', 'X ê²Œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤'),
                'code': 'PUBLISH_FAILED'
            }), 400
            
    except Exception as e:
        logger.error(f"X ê²Œì‹œ ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'INTERNAL_ERROR'
        }), 500

@app.route('/api/validate/x-credentials', methods=['POST', 'OPTIONS'])
def validate_x_credentials():
    """X API ì¸ì¦ ì •ë³´ ê²€ì¦"""
    if request.method == 'OPTIONS':
        return jsonify({'success': True}), 200
    
    try:
        data = request.json
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
        required_fields = ['consumer_key', 'consumer_secret', 
                          'access_token', 'access_token_secret']
        
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'error': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {field}',
                    'code': 'MISSING_FIELD'
                }), 400
        
        # X Publisher ì´ˆê¸°í™”
        publisher = XPublisher(
            consumer_key=data['consumer_key'],
            consumer_secret=data['consumer_secret'],
            access_token=data['access_token'],
            access_token_secret=data['access_token_secret']
        )
        
        # ì¸ì¦ í™•ì¸
        result = publisher.verify_credentials()
        
        if result['success']:
            return jsonify({
                'success': True,
                'user': result['user'],
                'message': f"ì¸ì¦ ì„±ê³µ! @{result['user']['username']}ë¡œ ë¡œê·¸ì¸ë˜ì—ˆìŠµë‹ˆë‹¤"
            }), 200
        else:
            error_code = result.get('code', 'AUTH_FAILED')
            status_code = 401  # ê¸°ë³¸ê°’
            
            # Rate limit ì—ëŸ¬ëŠ” 429 ë°˜í™˜
            if error_code == 'RATE_LIMIT_EXCEEDED':
                status_code = 429
            
            return jsonify({
                'success': False,
                'error': result.get('error', 'X API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤'),
                'code': error_code
            }), status_code
            
    except Exception as e:
        logger.error(f"X ì¸ì¦ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'code': 'INTERNAL_ERROR'
        }), 500

@app.route('/api/status/<job_id>', methods=['GET'])
def get_job_status(job_id):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    if job_id not in active_jobs:
        return jsonify({
            'success': False,
            'error': 'Job not found',
            'code': 'JOB_NOT_FOUND'
        }), 404
    
    job_info = active_jobs[job_id]
    return jsonify({
        'success': True,
        'job_id': job_id,
        'status': job_info['status'],
        'progress': job_info['progress'],
        'started_at': job_info['started_at'],
        'completed_at': job_info.get('completed_at'),
        'error': job_info.get('error')
    })

@app.route('/api/download/<job_id>', methods=['GET'])
def download_file(job_id):
    """ìƒì„±ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if job_id not in active_jobs:
        return jsonify({
            'success': False,
            'error': 'Job not found',
            'code': 'JOB_NOT_FOUND'
        }), 404
    
    job_info = active_jobs[job_id]
    
    if job_info['status'] != 'completed':
        return jsonify({
            'success': False,
            'error': 'Job not completed',
            'code': 'JOB_NOT_COMPLETED'
        }), 400
    
    file_path = Path(job_info['output_file'])
    if not file_path.exists():
        return jsonify({
            'success': False,
            'error': 'File not found',
            'code': 'FILE_NOT_FOUND'
        }), 404
    
    return send_from_directory(
        file_path.parent,
        file_path.name,
        as_attachment=True,
        mimetype='text/markdown'
    )

@app.route('/api/batch-generate', methods=['POST', 'OPTIONS'])
def batch_generate():
    """
    ë°°ì¹˜ ì½˜í…ì¸  ìƒì„± API
    
    Request Body:
    {
        "urls": ["https://example1.com", "https://example2.com"],
        "api_provider": "anthropic",  // required: anthropic or openai
        "api_key": "sk-...",         // required: user's API key
        "save_intermediate": false,   // optional
        "content_type": "standard"    // optional: 'standard' or 'blog'
    }
    """
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        data = request.get_json()
        
        if not data or 'urls' not in data:
            logger.error(f"[BATCH-GENERATE] URLs ëˆ„ë½")
            return jsonify({
                'success': False,
                'error': 'URLs are required',
                'code': 'MISSING_URLS'
            }), 400
        
        # API í‚¤ ê²€ì¦
        if 'api_provider' not in data or 'api_key' not in data:
            logger.error(f"[BATCH-GENERATE] API ìê²© ì¦ëª… ëˆ„ë½")
            return jsonify({
                'success': False,
                'error': 'API provider and API key are required',
                'code': 'MISSING_API_CREDENTIALS'
            }), 400
        
        urls = data['urls']
        api_provider = data['api_provider']
        api_key = data['api_key']
        save_intermediate = data.get('save_intermediate', False)  # ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ê°’ False
        content_type = data.get('content_type', 'standard')  # ê¸°ë³¸ê°’ì€ 'standard'
        selected_formats = data.get('selected_formats', None)  # ì™„ì„±í˜• ë¸”ë¡œê·¸ í˜•ì‹
        wordpress_type = data.get('wordpress_type', 'text')  # ì›Œë“œí”„ë ˆìŠ¤ í˜•ì‹ (text/html)
        
        if not isinstance(urls, list) or len(urls) == 0:
            return jsonify({
                'success': False,
                'error': 'URLs must be a non-empty list',
                'code': 'INVALID_URLS'
            }), 400
        
        # URL ê°œìˆ˜ ì œí•œìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
        if len(urls) > 50:
            return jsonify({
                'success': False,
                'error': 'Maximum 50 URLs allowed per batch to prevent timeout',
                'code': 'TOO_MANY_URLS'
            }), 400
        
        if api_provider not in ['anthropic', 'openai', 'perplexity']:
            return jsonify({
                'success': False,
                'error': 'API provider must be anthropic, openai, or perplexity',
                'code': 'INVALID_API_PROVIDER'
            }), 400
        
        # ì½˜í…ì¸  íƒ€ì… ê²€ì¦
        if content_type not in ['standard', 'blog', 'enhanced_blog', 'threads', 'x']:
            return jsonify({
                'success': False,
                'error': 'Content type must be standard, blog, enhanced_blog, threads, or x',
                'code': 'INVALID_CONTENT_TYPE'
            }), 400
        
        # ë°°ì¹˜ ì‘ì—… ID ìƒì„±
        batch_job_id = str(uuid.uuid4())
        active_jobs[batch_job_id] = {
            'status': 'processing',
            'type': 'batch',
            'urls': urls,
            'content_type': content_type,
            'started_at': datetime.now().isoformat(),
            'progress': 0,
            'results': []
        }
        
        logger.info(f"Starting batch generation for {len(urls)} URLs (Job ID: {batch_job_id}, Type: {content_type})")
        
        # NONGBUXX ìƒì„±ê¸° ì´ˆê¸°í™” (ì‚¬ìš©ì API í‚¤ ì‚¬ìš©)
        generator = NongbuxxGenerator(
            api_provider=api_provider,
            api_key=api_key,  # ì‚¬ìš©ì API í‚¤ ì „ë‹¬
            save_intermediate=save_intermediate
        )
        
        # ğŸš€ í–¥ìƒëœ ë°°ì¹˜ ì²˜ë¦¬ (ì½˜í…ì¸  íƒ€ì… ì „ë‹¬)
        try:
            # ì‘ì—… ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            
            # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì‚¬ìš©ì ì•Œë¦¼ìš©)
            estimated_time_per_url = 30  # ê¸°ë³¸ê°’
            if content_type == 'blog':
                estimated_time_per_url = 45
            elif content_type == 'enhanced_blog':
                estimated_time_per_url = 60
            
            total_estimated_time = len(urls) * estimated_time_per_url
            
            # ì‘ì—… ì •ë³´ ì—…ë°ì´íŠ¸
            active_jobs[batch_job_id].update({
                'estimated_time_seconds': total_estimated_time,
                'progress': 10
            })
            
            logger.info(f"Starting batch generation: {len(urls)} URLs, estimated time: {total_estimated_time}s")
            
            # ğŸ§¹ ìºì‹œ ì •ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ ì „)
            cleanup_expired_cache()
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ (ì›Œë“œí”„ë ˆìŠ¤ íƒ€ì… í¬í•¨)
            results = generator.batch_generate(
                urls, 
                content_type=content_type,
                selected_formats=selected_formats,
                wordpress_type=wordpress_type
            )
            
            # ğŸ¯ ë³‘ë ¬ì²˜ë¦¬ í†µê³„ ìˆ˜ì§‘
            parallel_stats = generator.get_parallel_stats()
            
            # ì •ë¦¬
            generator.cleanup()
            
            # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            actual_time = time.time() - start_time
            
            # ê²°ê³¼ ì²˜ë¦¬ - ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
            processed_results = []
            for result in results:
                if result['success']:
                    # íŒŒì¼ì—ì„œ ì½˜í…ì¸  ì½ê¸°
                    with open(result['output_file'], 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # íŒŒì¼ ì˜êµ¬ ë³´ê´€ (ê¸°ì¡´ ë°©ì‹ ë³µì›)
                    logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {result['output_file']}")
                    
                    processed_results.append({
                        'success': True,
                        'url': result['url'],
                        'title': result['title'],
                        'content': content,
                        'filename': result['output_file'].name,
                        'timestamp': result['timestamp'],
                        'content_type': result.get('content_type', content_type),  # ë°±ì—”ë“œì—ì„œ content_type ë³´ì¥
                        'output_file': str(result['output_file'])
                    })
                else:
                    processed_results.append({
                        'success': False,
                        'url': result['url'],
                        'error': result['error']
                    })
            
            # ì„±ê³µ í†µê³„
            success_count = sum(1 for r in processed_results if r['success'])
            
            # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
            active_jobs[batch_job_id].update({
                'status': 'completed',
                'progress': 100,
                'completed_at': datetime.now().isoformat(),
                'results': processed_results,
                'success_count': success_count,
                'total_count': len(urls),
                'actual_time_seconds': actual_time,
                'estimated_time_seconds': total_estimated_time
            })
            
            logger.info(f"Batch generation completed for job {batch_job_id}: {success_count}/{len(urls)} successful (Type: {content_type})")
            logger.info(f"Processing time: {actual_time:.2f}s (estimated: {total_estimated_time}s)")
            
            return jsonify({
                'success': True,
                'job_id': batch_job_id,
                'data': {
                    'results': processed_results,
                    'success_count': success_count,
                    'total_count': len(urls),
                    'api_provider': api_provider,
                    'content_type': content_type,
                    'processing_time_seconds': actual_time,
                    'parallel_stats': {
                        'max_workers': 3,
                        'completed_threads': parallel_stats.get('completed_tasks', 0),
                        'failed_threads': parallel_stats.get('failed_tasks', 0),
                        'total_threads': parallel_stats.get('completed_tasks', 0) + parallel_stats.get('failed_tasks', 0),
                        'parallel_efficiency': f"{((actual_time / len(urls) / 3) * 100):.1f}%" if len(urls) > 0 else "100%",
                        'speedup_factor': f"{3:.1f}x" if len(urls) > 1 else "1x"
                    }
                }
            })
            
        except Exception as processing_error:
            # ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
            error_msg = str(processing_error)
            logger.error(f"Batch processing error: {error_msg}")
            
            # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            if 'timeout' in error_msg.lower():
                user_error = "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ íƒí•œ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
            elif 'memory' in error_msg.lower():
                user_error = "ì„œë²„ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì„ íƒí•œ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
            elif 'api' in error_msg.lower() and 'key' in error_msg.lower():
                user_error = "API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            elif 'rate' in error_msg.lower() and 'limit' in error_msg.lower():
                user_error = "API ì‚¬ìš©ëŸ‰ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                user_error = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
            
            # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
            active_jobs[batch_job_id].update({
                'status': 'failed',
                'error': user_error,
                'completed_at': datetime.now().isoformat()
            })
            
            return jsonify({
                'success': False,
                'error': user_error,
                'code': 'PROCESSING_ERROR'
            }), 500
        
    except Exception as e:
        logger.error(f"Batch generation error: {str(e)}")
        return jsonify({
            'success': False,
            'error': 'Internal server error',
            'code': 'INTERNAL_ERROR'
        }), 500

@app.route('/api/extract-news-links', methods=['POST', 'OPTIONS'])
def extract_news_links():
    """
    ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ API (ë‹¤ì¤‘ ì¶œì²˜ ì§€ì›)
    
    Request Body:
    {
        "keyword": "Tesla AI",           // optional: ê²€ìƒ‰ í‚¤ì›Œë“œ
        "count": 10,                    // optional: ì¶”ì¶œí•  ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)
        "sources": ["yahoo_finance"]     // optional: ì¶œì²˜ ID ë°°ì—´ (ê¸°ë³¸ê°’: í™œì„±í™”ëœ ëª¨ë“  ì¶œì²˜)
    }
    """
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        data = request.get_json() or {}
        
        keyword = data.get('keyword', '').strip()
        count = data.get('count', 10)
        requested_sources = data.get('sources', [])
        
        # ì…ë ¥ê°’ ê²€ì¦
        if count < 1 or count > 50:
            return jsonify({
                'success': False,
                'error': 'ë‰´ìŠ¤ ê°œìˆ˜ëŠ” 1~50ê°œ ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.',
                'code': 'INVALID_COUNT'
            }), 400
        
        # ì‚¬ìš©í•  ì¶œì²˜ ê²°ì • (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)
        available_sources = get_all_extractable_sources()  # ì„œë¸Œì¹´í…Œê³ ë¦¬ í¬í•¨
        
        if requested_sources:
            # ìš”ì²­ëœ ì¶œì²˜ë§Œ ì‚¬ìš©
            selected_sources = [s for s in available_sources if s['id'] in requested_sources and s.get('active', True)]
            if not selected_sources:
                return jsonify({
                    'success': False,
                    'error': 'ìš”ì²­ëœ ì¶œì²˜ ì¤‘ í™œì„±í™”ëœ ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.',
                    'code': 'NO_ACTIVE_SOURCES'
                }), 400
        else:
            # í™œì„±í™”ëœ ëª¨ë“  ì¶œì²˜ ì‚¬ìš©
            selected_sources = [s for s in available_sources if s.get('active', True)]
            if not selected_sources:
                return jsonify({
                    'success': False,
                    'error': 'í™œì„±í™”ëœ ì¶œì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.',
                    'code': 'NO_ACTIVE_SOURCES'
                }), 400
        
        logger.info(f"Starting multi-source news extraction - keyword: '{keyword}', count: {count}, sources: {[s['id'] for s in selected_sources]}")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë‰´ìŠ¤ ì¶”ì¶œ (ì„±ëŠ¥ ìµœì í™”)
        from url_extractor import extract_news_from_multiple_sources
        
        logger.info(f"ë³‘ë ¬ ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {len(selected_sources)}ê°œ ì¶œì²˜")
        start_time = time.time()
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ì¶œì²˜ì—ì„œ ë™ì‹œì— ë‰´ìŠ¤ ì¶”ì¶œ
        all_news_items = extract_news_from_multiple_sources(
            sources=selected_sources,
            keyword=keyword,
            count=count,
            max_workers=min(len(selected_sources), 8)  # ìµœëŒ€ 8ê°œ ë™ì‹œ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
        )
        
        end_time = time.time()
        logger.info(f"ë³‘ë ¬ ì¶”ì¶œ ì™„ë£Œ: {len(all_news_items)}ê°œ ë‰´ìŠ¤, ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        # ì¶œì²˜ë³„ ê²°ê³¼ í†µê³„
        source_results = []
        source_counts = {}
        
        for item in all_news_items:
            source_id = item.get('source_id')
            if source_id:
                source_counts[source_id] = source_counts.get(source_id, 0) + 1
        
        for source in selected_sources:
            count_extracted = source_counts.get(source['id'], 0)
            source_results.append({
                'source_id': source['id'],
                'source_name': source['name'],
                'count': count_extracted,
                'success': count_extracted > 0
            })
            
            # ì„±ê³µí•œ ì†ŒìŠ¤ë§Œ ë¡œê¹… (ì„±ëŠ¥ ìµœì í™”)
            if count_extracted > 0:
                logger.info(f"âœ… {source['name']}: {count_extracted}ê°œ ë‰´ìŠ¤ ì¶”ì¶œ")
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€) - ê°™ì€ URLì˜ ë‰´ìŠ¤ê°€ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‚˜ì˜¬ ë•Œ ì†ŒìŠ¤ ì •ë³´ ë³‘í•©
        unique_news = []
        seen_urls = {}
        
        for item in all_news_items:
            url = item['url']
            if url not in seen_urls:
                # ìƒˆë¡œìš´ URL - ê·¸ëŒ€ë¡œ ì¶”ê°€
                unique_news.append(item)
                seen_urls[url] = len(unique_news) - 1  # ì¸ë±ìŠ¤ ì €ì¥
            else:
                # ì¤‘ë³µ URL - ê¸°ì¡´ ì•„ì´í…œì— ì†ŒìŠ¤ ì •ë³´ ë³‘í•©
                existing_index = seen_urls[url]
                existing_item = unique_news[existing_index]
                
                # ì†ŒìŠ¤ ì •ë³´ ë³‘í•© (ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ê°™ì€ ë‰´ìŠ¤ê°€ ë‚˜ì˜¬ ë•Œ)
                if existing_item['source_name'] != item['source_name']:
                    existing_item['source_name'] = f"{existing_item['source_name']}, {item['source_name']}"
                    
                # í‚¤ì›Œë“œ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                if item.get('keywords'):
                    existing_keywords = set(existing_item.get('keywords', []))
                    new_keywords = set(item['keywords'])
                    merged_keywords = list(existing_keywords.union(new_keywords))
                    existing_item['keywords'] = merged_keywords
        
        # ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ê²°ê³¼ í™•ì¸
        if not unique_news:
            return jsonify({
                'success': False,
                'error': f"'{keyword}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." if keyword and keyword.strip() else 'ì„ íƒí•œ ì¶œì²˜ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶œì²˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.',
                'code': 'NO_NEWS_FOUND',
                'source_results': source_results
            }), 404
        
        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°)
        if keyword:
            keyword_lower = keyword.lower()
            unique_news.sort(key=lambda x: (
                keyword_lower in x['title'].lower(),
                sum(1 for kw in x.get('keywords', []) if keyword_lower in kw.lower())
            ), reverse=True)
        
        # ğŸš¨ í™ë³´ì„± í•„í„°ë§ í†µê³„ ê³„ì‚°
        total_extracted = len(all_news_items)
        filtered_count = len(unique_news)
        promotional_filtered = total_extracted - filtered_count
        
        logger.info(f"Multi-source news extraction completed: {len(unique_news)} unique articles from {len(selected_sources)} sources (í™ë³´ì„± ë‰´ìŠ¤ {promotional_filtered}ê°œ ì œì™¸)")
        
        return jsonify({
            'success': True,
            'data': {
                'keyword': keyword,
                'count': len(unique_news),
                'total_extracted': total_extracted,
                'filtered_count': filtered_count,
                'promotional_filtered': promotional_filtered,
                'unique_count': len(unique_news),
                'news_items': unique_news,
                'source_results': source_results
            }
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Multi-source news extraction error: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': f'ë‰´ìŠ¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'EXTRACTION_ERROR'
        }), 500

def extract_news_from_source(source, keyword, count):
    """íŠ¹ì • ì¶œì²˜ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)"""
    try:
        # ëª¨ë“  ì¶œì²˜ì— ëŒ€í•´ ìµœì í™”ëœ ì¶”ì¶œê¸° ì‚¬ìš©
        from url_extractor import OptimizedNewsExtractor
        
        # ì‹¤ì œ ì¶”ì¶œ URL ê²°ì • (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)
        extraction_url = source.get('full_url', source['url'])
        
        logger.info(f"Extracting news from {source['name']}")
        logger.info(f"Extraction URL: {extraction_url}")
        
        extractor = OptimizedNewsExtractor(
            base_url=extraction_url,
            search_keywords=keyword if keyword else None,
            max_news=count
        )
        
        return extractor.extract_news()
            
    except Exception as e:
        logger.error(f"Error extracting news from {source['name']}: {str(e)}")
        raise e

# ============================================================================
# ìƒì„±ëœ ì½˜í…ì¸  ê´€ë¦¬ API
# ============================================================================

@app.route('/api/generated-content', methods=['GET'])
def get_generated_content():
    """ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        generated_dir = Path('generated_content')
        if not generated_dir.exists():
            return jsonify({
                'success': True,
                'data': {
                    'files': [],
                    'total_count': 0
                }
            })
        
        # ëª¨ë“  .md íŒŒì¼ ì°¾ê¸°
        md_files = list(generated_dir.glob('*.md'))
        
        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        file_info = []
        for file_path in md_files:
            try:
                # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì½ê¸°
                stat_info = file_path.stat()
                created_time = datetime.fromtimestamp(stat_info.st_ctime)
                modified_time = datetime.fromtimestamp(stat_info.st_mtime)
                
                # íŒŒì¼ ë‚´ìš©ì—ì„œ ì œëª© ì¶”ì¶œ
                title = "ì œëª© ì—†ìŒ"
                content_type = "standard"
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                                    # ì œëª© ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì²« ë²ˆì§¸ # í—¤ë” ë˜ëŠ” ì´ëª¨ì§€ ì œëª©)
                lines = content.split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('# '):
                        title = line[2:].strip()
                        break
                    elif line and not line.startswith('##') and len(line) > 10:
                        # ì²« ë²ˆì§¸ ì¤„ì´ ì œëª© í˜•íƒœì¸ ê²½ìš° (ì´ëª¨ì§€ í¬í•¨)
                        if any(ord(char) > 127 for char in line[:5]):  # ì´ëª¨ì§€ ë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì í¬í•¨
                            title = line[:60] + ('...' if len(line) > 60 else '')
                            break
                    
                    # ì½˜í…ì¸  íƒ€ì… íŒë³„
                    if 'blog_' in file_path.name:
                        content_type = "blog"
                
                file_info.append({
                    'filename': file_path.name,
                    'title': title,
                    'content_type': content_type,
                    'size': stat_info.st_size,
                    'created_at': created_time.isoformat(),
                    'modified_at': modified_time.isoformat(),
                    'url': f"/api/generated-content/{file_path.name}"
                })
                
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {file_path.name} - {e}")
                continue
        
        # ìˆ˜ì • ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        file_info.sort(key=lambda x: x['modified_at'], reverse=True)
        
        return jsonify({
            'success': True,
            'data': {
                'files': file_info,
                'total_count': len(file_info)
            }
        })
        
    except Exception as e:
        logger.error(f"ìƒì„±ëœ ì½˜í…ì¸  ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return jsonify({
            'success': False,
            'error': 'ìƒì„±ëœ ì½˜í…ì¸  ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'code': 'GENERATED_CONTENT_LIST_ERROR'
        }), 500

@app.route('/api/generated-content/<filename>', methods=['GET'])
def get_generated_content_file(filename):
    """íŠ¹ì • ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ì¡°íšŒ"""
    try:
        # íŒŒì¼ëª… ê²€ì¦ (ë³´ì•ˆ)
        if not filename.endswith('.md') or '..' in filename or '/' in filename:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ëª…ì…ë‹ˆë‹¤.',
                'code': 'INVALID_FILENAME'
            }), 400
        
        file_path = Path('generated_content') / filename
        if not file_path.exists():
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'code': 'FILE_NOT_FOUND'
            }), 404
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # íŒŒì¼ ì •ë³´
        stat_info = file_path.stat()
        
        # ì œëª© ì¶”ì¶œ
        title = "ì œëª© ì—†ìŒ"
        lines = content.split('\n')
        for line in lines:
            if line.startswith('# '):
                title = line[2:].strip()
                break
        
        # ì½˜í…ì¸  íƒ€ì… íŒë³„
        content_type = "blog" if 'blog_' in filename else "standard"
        
        return jsonify({
            'success': True,
            'data': {
                'filename': filename,
                'title': title,
                'content': content,
                'content_type': content_type,
                'size': stat_info.st_size,
                'created_at': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),
                'modified_at': datetime.fromtimestamp(stat_info.st_mtime).isoformat()
            }
        })
        
    except Exception as e:
        logger.error(f"ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {filename} - {e}")
        return jsonify({
            'success': False,
            'error': 'íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'code': 'FILE_READ_ERROR'
        }), 500

@app.route('/api/generated-content/<filename>', methods=['DELETE'])
def delete_generated_content_file(filename):
    """íŠ¹ì • ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ì‚­ì œ"""
    try:
        # íŒŒì¼ëª… ê²€ì¦ (ë³´ì•ˆ)
        if not filename.endswith('.md') or '..' in filename or '/' in filename:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ëª…ì…ë‹ˆë‹¤.',
                'code': 'INVALID_FILENAME'
            }), 400
        
        file_path = Path('generated_content') / filename
        if not file_path.exists():
            return jsonify({
                'success': False,
                'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'code': 'FILE_NOT_FOUND'
            }), 404
        
        # íŒŒì¼ ì‚­ì œ
        file_path.unlink()
        
        logger.info(f"ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {filename}")
        
        return jsonify({
            'success': True,
            'message': 'íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'filename': filename
        })
        
    except Exception as e:
        logger.error(f"ìƒì„±ëœ ì½˜í…ì¸  íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {filename} - {e}")
        return jsonify({
            'success': False,
            'error': 'íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
            'code': 'FILE_DELETE_ERROR'
        }), 500



# ============================================================================
# ì¶œì²˜ ê´€ë¦¬ API
# ============================================================================

def load_sources():
    """ì¶œì²˜ ì •ë³´ë¥¼ JSON íŒŒì¼ì—ì„œ ë¡œë“œ (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)"""
    sources_file = Path('data/sources.json')
    try:
        if sources_file.exists():
            with open(sources_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('sources', [])
        else:
            # ê¸°ë³¸ ì¶œì²˜ ë°ì´í„°ë¡œ íŒŒì¼ ìƒì„±
            default_sources = [{
                "id": "yahoo_finance",
                "name": "Yahoo Finance",
                "url": "https://finance.yahoo.com/topic/latest-news/",
                "parser_type": "yahoo_finance",
                "active": True,
                "description": "ê¸€ë¡œë²Œ ê¸ˆìœµ ë‰´ìŠ¤ ë° ì‹œì¥ ì •ë³´",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }]
            save_sources(default_sources)
            return default_sources
    except Exception as e:
        logger.error(f"Failed to load sources: {e}")
        return []

def save_sources(sources):
    """ì¶œì²˜ ì •ë³´ë¥¼ JSON íŒŒì¼ì— ì €ì¥"""
    sources_file = Path('data/sources.json')
    try:
        # data ë””ë ‰í† ë¦¬ ìƒì„±
        sources_file.parent.mkdir(exist_ok=True)
        
        data = {
            "sources": sources,
            "updated_at": datetime.now().isoformat()
        }
        
        with open(sources_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        logger.error(f"Failed to save sources: {e}")
        return False

def find_source_by_id(source_id):
    """IDë¡œ ì¶œì²˜ ì°¾ê¸° (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)"""
    sources = load_sources()
    
    # ë¶€ëª¨ ì¶œì²˜ì—ì„œ ì°¾ê¸°
    for source in sources:
        if source['id'] == source_id:
            return source
            
        # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¾ê¸°
        if 'subcategories' in source:
            for subcategory in source['subcategories']:
                if subcategory['id'] == source_id:
                    # ë¶€ëª¨ URLê³¼ ì„œë¸Œ ì¹´í…Œê³ ë¦¬ URL ê²°í•©
                    subcategory_copy = subcategory.copy()
                    subcategory_copy['full_url'] = source['url'].rstrip('/') + subcategory['url']
                    subcategory_copy['parent_id'] = source['id']
                    subcategory_copy['parent_name'] = source['name']
                    return subcategory_copy
    
    return None

def get_all_extractable_sources():
    """ì¶”ì¶œ ê°€ëŠ¥í•œ ëª¨ë“  ì¶œì²˜ ë°˜í™˜ (ì„œë¸Œ ì¹´í…Œê³ ë¦¬ í¬í•¨)"""
    sources = load_sources()
    extractable_sources = []
    
    for source in sources:
        if source.get('is_parent', False):
            # ë¶€ëª¨ ì¶œì²˜ì¸ ê²½ìš° ì„œë¸Œ ì¹´í…Œê³ ë¦¬ë“¤ ì¶”ê°€
            if 'subcategories' in source:
                for subcategory in source['subcategories']:
                    if subcategory.get('active', True):
                        subcategory_copy = subcategory.copy()
                        subcategory_copy['full_url'] = source['url'].rstrip('/') + subcategory['url']
                        subcategory_copy['parent_id'] = source['id']
                        subcategory_copy['parent_name'] = source['name']
                        subcategory_copy['parent_url'] = source['url']
                        extractable_sources.append(subcategory_copy)
        else:
            # ë‹¨ë… ì¶œì²˜ì¸ ê²½ìš°
            if source.get('active', True):
                extractable_sources.append(source)
    
    return extractable_sources

def get_all_sources_with_structure():
    """ê³„ì¸µì  êµ¬ì¡°ë¥¼ í¬í•¨í•œ ëª¨ë“  ì¶œì²˜ ë°˜í™˜"""
    sources = load_sources()
    result = {
        'parent_sources': [],
        'standalone_sources': [],
        'extractable_sources': []
    }
    
    for source in sources:
        if source.get('is_parent', False):
            # ë¶€ëª¨ ì¶œì²˜ ì¶”ê°€
            result['parent_sources'].append(source)
            
            # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ë“¤ì„ extractable_sourcesì— ì¶”ê°€
            if 'subcategories' in source:
                for subcategory in source['subcategories']:
                    if subcategory.get('active', True):
                        subcategory_copy = subcategory.copy()
                        subcategory_copy['full_url'] = source['url'].rstrip('/') + subcategory['url']
                        subcategory_copy['parent_id'] = source['id']
                        subcategory_copy['parent_name'] = source['name']
                        subcategory_copy['parent_url'] = source['url']
                        result['extractable_sources'].append(subcategory_copy)
        else:
            # ë‹¨ë… ì¶œì²˜ì¸ ê²½ìš°
            if source.get('active', True):
                result['standalone_sources'].append(source)
                result['extractable_sources'].append(source)
    
    return result

def validate_source_data(data):
    """ì¶œì²˜ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
    required_fields = ['name', 'url']
    for field in required_fields:
        if not data.get(field):
            return False, f'{field} is required'
    
    # URL ìœ íš¨ì„± ê²€ì¦
    url = data.get('url', '')
    if not url.startswith(('http://', 'https://')):
        return False, 'Invalid URL format'
    
    return True, None

@app.route('/api/sources/extractable', methods=['GET'])
def get_extractable_sources():
    """ì¶”ì¶œ ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡ ì¡°íšŒ (ì„œë¸Œ ì¹´í…Œê³ ë¦¬ í¬í•¨)"""
    try:
        extractable_sources = get_all_extractable_sources()
        
        return jsonify({
            'success': True,
            'data': {
                'sources': extractable_sources,
                'total_count': len(extractable_sources)
            }
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to get extractable sources: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': f'ì¶”ì¶œ ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'EXTRACTABLE_SOURCES_ERROR'
        }), 500

@app.route('/api/sources/structured', methods=['GET'])
def get_structured_sources():
    """ê³„ì¸µì  êµ¬ì¡°ë¥¼ í¬í•¨í•œ ì¶œì²˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        structured_sources = get_all_sources_with_structure()
        
        return jsonify({
            'success': True,
            'data': structured_sources
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to get structured sources: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': f'ê³„ì¸µì  ì¶œì²˜ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'STRUCTURED_SOURCES_ERROR'
        }), 500

@app.route('/api/sources', methods=['GET', 'OPTIONS'])
def get_sources():
    """ëª¨ë“  ì¶œì²˜ ì¡°íšŒ"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        sources = load_sources()
        
        return jsonify({
            'success': True,
            'data': {
                'sources': sources,
                'total_count': len(sources)
            }
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to get sources: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': f'ì¶œì²˜ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'SOURCES_LOAD_ERROR'
        }), 500

@app.route('/api/sources', methods=['POST'])
def create_source():
    """ìƒˆ ì¶œì²˜ ë“±ë¡ (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Request data is required',
                'code': 'MISSING_DATA'
            }), 400
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        is_valid, error_msg = validate_source_data(data)
        if not is_valid:
            return jsonify({
                'success': False,
                'error': error_msg,
                'code': 'INVALID_DATA'
            }), 400
        
        # ê¸°ì¡´ ì¶œì²˜ ë¡œë“œ
        sources = load_sources()
        
        # ê³ ìœ  ID ìƒì„±
        source_id = data.get('id') or f"source_{uuid.uuid4().hex[:8]}"
        
        # ID ì¤‘ë³µ í™•ì¸
        if any(source['id'] == source_id for source in sources):
            return jsonify({
                'success': False,
                'error': 'Source ID already exists',
                'code': 'DUPLICATE_ID'
            }), 400
        
        # ìƒˆ ì¶œì²˜ ê°ì²´ ìƒì„±
        new_source = {
            'id': source_id,
            'name': data['name'],
            'url': data['url'],
            'is_parent': data.get('is_parent', False),
            'active': data.get('active', True),
            'description': data.get('description', ''),
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # ë‹¨ë… ì¶œì²˜ì¸ ê²½ìš°ì—ë§Œ parser_type ì¶”ê°€
        if not new_source['is_parent']:
            new_source['parser_type'] = data.get('parser_type', 'generic')
        
        # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ (ë¶€ëª¨ ì¶œì²˜ì¸ ê²½ìš°)
        if new_source['is_parent'] and 'subcategories' in data:
            new_source['subcategories'] = []
            for subcategory in data['subcategories']:
                subcategory_obj = {
                    'id': subcategory.get('id') or f"sub_{uuid.uuid4().hex[:8]}",
                    'name': subcategory['name'],
                    'url': subcategory['url'],
                    'parser_type': subcategory.get('parser_type', 'universal'),
                    'active': subcategory.get('active', True),
                    'description': subcategory.get('description', ''),
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
                new_source['subcategories'].append(subcategory_obj)
        
        # ì¶œì²˜ ëª©ë¡ì— ì¶”ê°€
        sources.append(new_source)
        
        # ì €ì¥
        if save_sources(sources):
            logger.info(f"New source created: {source_id} - {new_source['name']}")
            
            return jsonify({
                'success': True,
                'data': new_source,
                'message': 'ì¶œì²˜ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.'
            }), 201
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to save source',
                'code': 'SAVE_ERROR'
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to create source: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': f'ì¶œì²˜ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'CREATE_SOURCE_ERROR'
        }), 500

@app.route('/api/sources/<source_id>', methods=['GET', 'OPTIONS'])
def get_source(source_id):
    """íŠ¹ì • ì¶œì²˜ ì¡°íšŒ"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'GET, PUT, DELETE, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, Accept'
        return response
    
    try:
        source = find_source_by_id(source_id)
        
        if not source:
            return jsonify({
                'success': False,
                'error': 'Source not found',
                'code': 'SOURCE_NOT_FOUND'
            }), 404
        
        return jsonify({
            'success': True,
            'data': source
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to get source {source_id}: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': f'ì¶œì²˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'GET_SOURCE_ERROR'
        }), 500

@app.route('/api/sources/<source_id>', methods=['PUT'])
def update_source(source_id):
    """ì¶œì²˜ ìˆ˜ì •"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Request data is required',
                'code': 'MISSING_DATA'
            }), 400
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        is_valid, error_msg = validate_source_data(data)
        if not is_valid:
            return jsonify({
                'success': False,
                'error': error_msg,
                'code': 'INVALID_DATA'
            }), 400
        
        # ê¸°ì¡´ ì¶œì²˜ ë¡œë“œ
        sources = load_sources()
        
        # ì¶œì²˜ ì°¾ê¸°
        source_index = next((i for i, source in enumerate(sources) if source['id'] == source_id), None)
        
        if source_index is None:
            return jsonify({
                'success': False,
                'error': 'Source not found',
                'code': 'SOURCE_NOT_FOUND'
            }), 404
        
        # ì¶œì²˜ ì •ë³´ ì—…ë°ì´íŠ¸
        existing_source = sources[source_index]
        updated_source = {
            **existing_source,
            'name': data['name'],
            'url': data['url'],
            'is_parent': data.get('is_parent', existing_source.get('is_parent', False)),
            'active': data.get('active', existing_source.get('active', True)),
            'description': data.get('description', existing_source.get('description', '')),
            'updated_at': datetime.now().isoformat()
        }
        
        # ë¶€ëª¨ ì¶œì²˜ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ parser_type ì¶”ê°€
        if not updated_source['is_parent']:
            updated_source['parser_type'] = data.get('parser_type', existing_source.get('parser_type', 'generic'))
        
        # ì„œë¸Œì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸ (ë¶€ëª¨ ì¶œì²˜ì¸ ê²½ìš°)
        if updated_source['is_parent'] and 'subcategories' in data:
            updated_source['subcategories'] = []
            for subcategory in data['subcategories']:
                subcategory_obj = {
                    'id': subcategory.get('id') or f"sub_{uuid.uuid4().hex[:8]}",
                    'name': subcategory['name'],
                    'url': subcategory['url'],
                    'parser_type': subcategory.get('parser_type', 'universal'),
                    'active': subcategory.get('active', True),
                    'description': subcategory.get('description', ''),
                    'created_at': subcategory.get('created_at', datetime.now().isoformat()),
                    'updated_at': datetime.now().isoformat()
                }
                updated_source['subcategories'].append(subcategory_obj)
        
        sources[source_index] = updated_source
        
        # ì €ì¥
        if save_sources(sources):
            logger.info(f"Source updated: {source_id} - {updated_source['name']}")
            
            return jsonify({
                'success': True,
                'data': updated_source,
                'message': 'ì¶œì²˜ê°€ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to save source',
                'code': 'SAVE_ERROR'
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to update source {source_id}: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': f'ì¶œì²˜ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'UPDATE_SOURCE_ERROR'
        }), 500

@app.route('/api/sources/<source_id>', methods=['DELETE'])
def delete_source(source_id):
    """ì¶œì²˜ ì‚­ì œ (ê³„ì¸µì  êµ¬ì¡° ì§€ì›)"""
    try:
        # ê¸°ì¡´ ì¶œì²˜ ë¡œë“œ
        sources = load_sources()
        
        # ì¶œì²˜ ì°¾ê¸° (ë¶€ëª¨ ì¶œì²˜ ë° ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ê²€ìƒ‰)
        source_index = None
        parent_index = None
        subcategory_index = None
        
        for i, source in enumerate(sources):
            if source['id'] == source_id:
                source_index = i
                break
            
            # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ì—ì„œ ì°¾ê¸°
            if 'subcategories' in source:
                for j, subcategory in enumerate(source['subcategories']):
                    if subcategory['id'] == source_id:
                        parent_index = i
                        subcategory_index = j
                        break
        
        if source_index is not None:
            # ë¶€ëª¨ ì¶œì²˜ ì‚­ì œ
            if len(sources) <= 1:
                return jsonify({
                    'success': False,
                    'error': 'ìµœì†Œ í•˜ë‚˜ì˜ ì¶œì²˜ëŠ” ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.',
                    'code': 'MINIMUM_SOURCES_REQUIRED'
                }), 400
            
            deleted_source = sources.pop(source_index)
        elif parent_index is not None and subcategory_index is not None:
            # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì‚­ì œ
            deleted_source = sources[parent_index]['subcategories'].pop(subcategory_index)
        else:
            return jsonify({
                'success': False,
                'error': 'Source not found',
                'code': 'SOURCE_NOT_FOUND'
            }), 404
        
        # ì €ì¥
        if save_sources(sources):
            logger.info(f"Source deleted: {source_id} - {deleted_source['name']}")
            
            return jsonify({
                'success': True,
                'data': deleted_source,
                'message': 'ì¶œì²˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to save sources',
                'code': 'SAVE_ERROR'
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to delete source {source_id}: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': f'ì¶œì²˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'DELETE_SOURCE_ERROR'
        }), 500

@app.route('/api/sources/<parent_id>/subcategories', methods=['POST'])
def add_subcategory(parent_id):
    """ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Request data is required',
                'code': 'MISSING_DATA'
            }), 400
        
        # ë¶€ëª¨ ì¶œì²˜ ì°¾ê¸°
        sources = load_sources()
        parent_source = None
        parent_index = None
        
        for i, source in enumerate(sources):
            if source['id'] == parent_id:
                parent_source = source
                parent_index = i
                break
        
        if not parent_source:
            return jsonify({
                'success': False,
                'error': 'Parent source not found',
                'code': 'PARENT_NOT_FOUND'
            }), 404
        
        if not parent_source.get('is_parent', False):
            return jsonify({
                'success': False,
                'error': 'Source is not a parent source',
                'code': 'NOT_PARENT_SOURCE'
            }), 400
        
        # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ê°ì²´ ìƒì„±
        subcategory_id = data.get('id') or f"sub_{uuid.uuid4().hex[:8]}"
        new_subcategory = {
            'id': subcategory_id,
            'name': data['name'],
            'url': data['url'],
            'parser_type': data.get('parser_type', 'universal'),
            'active': data.get('active', True),
            'description': data.get('description', ''),
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
        
        # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì´ ì—†ìœ¼ë©´ ìƒì„±
        if 'subcategories' not in parent_source:
            parent_source['subcategories'] = []
        
        # ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        parent_source['subcategories'].append(new_subcategory)
        parent_source['updated_at'] = datetime.now().isoformat()
        
        # ì €ì¥
        if save_sources(sources):
            logger.info(f"Subcategory added: {subcategory_id} - {new_subcategory['name']}")
            
            return jsonify({
                'success': True,
                'data': new_subcategory,
                'message': 'ì„œë¸Œ ì¹´í…Œê³ ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.'
            }), 201
        else:
            return jsonify({
                'success': False,
                'error': 'Failed to save sources',
                'code': 'SAVE_ERROR'
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Failed to add subcategory: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            'success': False,
            'error': f'ì„œë¸Œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}',
            'code': 'ADD_SUBCATEGORY_ERROR'
        }), 500

@app.route('/api/cache-stats', methods=['GET'])
def get_cache_stats():
    """ìºì‹œ ì‚¬ìš© í†µê³„ ì¡°íšŒ"""
    try:
        cleanup_expired_cache()  # ì •ë¦¬ í›„ í†µê³„ ì¡°íšŒ
        
        total_cache_size = len(content_cache)
        valid_cache_count = sum(1 for k in content_cache.keys() if is_cache_valid(k))
        
        return jsonify({
            'success': True,
            'cache_stats': {
                'total_cached_items': total_cache_size,
                'valid_cached_items': valid_cache_count,
                'expired_items_cleaned': total_cache_size - valid_cache_count,
                'cache_hit_potential': f"{(valid_cache_count / max(total_cache_size, 1)) * 100:.1f}%"
            },
            'performance_impact': {
                'potential_speedup': "2-3ë°° ë¹ ë¥¸ ì‘ë‹µ (ìºì‹œëœ í•­ëª©)",
                'cache_duration': "30ë¶„",
                'memory_efficiency': "ìë™ ë§Œë£Œ ê´€ë¦¬"
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ============================================================================
# ê¸°ì¡´ ì—ëŸ¬ í•¸ë“¤ëŸ¬ë“¤
# ============================================================================

@app.errorhandler(413)
def request_entity_too_large(error):
    return jsonify({
        'success': False,
        'error': 'Request entity too large',
        'code': 'REQUEST_TOO_LARGE'
    }), 413

@app.errorhandler(404)
def not_found(error):
    """404 ì—ëŸ¬ ì²˜ë¦¬"""
    # API ìš”ì²­ì¸ ê²½ìš° JSON ì‘ë‹µ
    if request.path.startswith('/api/'):
        return jsonify({
            'success': False,
            'error': 'API endpoint not found',
            'code': 'NOT_FOUND',
            'path': request.path
        }), 404
    
    # favicon ìš”ì²­ì€ ë¹ˆ ì‘ë‹µ
    if request.path.endswith('.ico'):
        return '', 204
    
    # ê¸°íƒ€ ìš”ì²­ì€ ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    return send_from_directory('frontend', 'index.html')

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        'success': False,
        'error': 'Internal server error',
        'code': 'INTERNAL_ERROR'
    }), 500

# ==================== X í¬ë¡¤ëŸ¬ API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.route('/api/x-crawler/schedule', methods=['GET', 'POST'])
def handle_x_crawler_schedule():
    """ìŠ¤ì¼€ì¤„ ê´€ë¦¬"""
    scheduler = get_scheduler()
    
    if request.method == 'GET':
        # í˜„ì¬ ìŠ¤ì¼€ì¤„ ìƒíƒœ ì¡°íšŒ
        status = scheduler.get_status()
        return jsonify({
            'success': True,
            'data': status
        })
    
    else:  # POST
        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        config = request.json
        scheduler.setup_schedules(config)
        scheduler.start()
        
        return jsonify({
            'success': True,
            'message': 'ìŠ¤ì¼€ì¤„ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤'
        })

# ì¸í”Œë£¨ì–¸ì„œ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜, ì‹¤ì œë¡œëŠ” DB ì‚¬ìš© ê¶Œì¥)
influencers_storage = []

@app.route('/api/x-crawler/influencers', methods=['GET', 'POST', 'DELETE'])
def handle_influencers():
    """ì¸í”Œë£¨ì–¸ì„œ ê´€ë¦¬"""
    global influencers_storage
    
    if request.method == 'GET':
        # ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡ ì¡°íšŒ
        return jsonify({
            'success': True,
            'data': influencers_storage
        })
    
    elif request.method == 'POST':
        # ì¸í”Œë£¨ì–¸ì„œ ì¶”ê°€
        data = request.json
        
        # ì¤‘ë³µ ì²´í¬
        username = data.get('username', '').replace('@', '')
        if any(inf['username'] == username for inf in influencers_storage):
            return jsonify({
                'success': False,
                'error': 'ì´ë¯¸ ë“±ë¡ëœ ì¸í”Œë£¨ì–¸ì„œì…ë‹ˆë‹¤'
            }), 400
        
        # ìƒˆ ì¸í”Œë£¨ì–¸ì„œ ì¶”ê°€
        new_influencer = {
            'id': f'inf_{int(time.time()*1000)}',
            'username': username,
            'name': data.get('name', username),
            'profileImage': data.get('profileImage', 'https://via.placeholder.com/60'),
            'isActive': data.get('isActive', True),
            'addedAt': datetime.now().isoformat(),
            'lastFetched': None,
            'stats': {
                'postsCollected': 0,
                'lastPostDate': None
            }
        }
        
        influencers_storage.append(new_influencer)
        logger.info(f"âœ… ì¸í”Œë£¨ì–¸ì„œ ì¶”ê°€: @{username}")
        
        return jsonify({
            'success': True,
            'message': f'@{username}ì´(ê°€) ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤',
            'data': new_influencer
        })
    
    else:  # DELETE
        # ì¸í”Œë£¨ì–¸ì„œ ì‚­ì œ
        influencer_id = request.args.get('id')
        
        if not influencer_id:
            return jsonify({
                'success': False,
                'error': 'IDê°€ í•„ìš”í•©ë‹ˆë‹¤'
            }), 400
        
        # ì¸í”Œë£¨ì–¸ì„œ ì°¾ì•„ì„œ ì‚­ì œ
        initial_count = len(influencers_storage)
        influencers_storage = [inf for inf in influencers_storage if inf['id'] != influencer_id]
        
        if len(influencers_storage) < initial_count:
            logger.info(f"ğŸ—‘ï¸ ì¸í”Œë£¨ì–¸ì„œ ì‚­ì œ: ID {influencer_id}")
            return jsonify({
                'success': True,
                'message': 'ì¸í”Œë£¨ì–¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì¸í”Œë£¨ì–¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }), 404

@app.route('/api/x-crawler/collect', methods=['POST', 'OPTIONS'])
def collect_posts():
    """ìˆ˜ë™ ìˆ˜ì§‘ ì‹¤í–‰"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, X-Credentials'
        return response
    
    try:
        global influencers_storage
        
        # X API ìê²©ì¦ëª… í™•ì¸
        x_credentials = request.headers.get('X-Credentials')
        if not x_credentials:
            return jsonify({
                'success': False,
                'error': 'X API ìê²©ì¦ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤'
            }), 401
        
        # ìê²©ì¦ëª… ë””ì½”ë“œ
        try:
            import base64
            credentials = json.loads(base64.b64decode(x_credentials))
        except:
            return jsonify({
                'success': False,
                'error': 'ì˜ëª»ëœ ìê²©ì¦ëª… í˜•ì‹'
            }), 400
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = get_crawler()
        if not crawler.setup_x_api(credentials):
            return jsonify({
                'success': False,
                'error': 'X API ì¸ì¦ ì‹¤íŒ¨'
            }), 401
        
        # AI API ì„¤ì • (ì„ íƒì )
        ai_provider = request.json.get('ai_provider')
        ai_key = request.json.get('ai_key')
        if ai_provider and ai_key:
            crawler.setup_ai_api({
                'provider': ai_provider,
                'api_key': ai_key
            })
            logger.info(f"âœ… AI API ì„¤ì •ë¨: {ai_provider}")
        
        # í™œì„± ì¸í”Œë£¨ì–¸ì„œ ëª©ë¡
        active_influencers = [inf for inf in influencers_storage if inf.get('isActive', True)]
        
        if not active_influencers:
            return jsonify({
                'success': False,
                'error': 'í™œì„± ì¸í”Œë£¨ì–¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # ìˆ˜ì§‘ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        all_posts = []
        collection_results = []
        
        for influencer in active_influencers:
            username = influencer['username']
            posts = loop.run_until_complete(
                crawler.fetch_influencer_posts(username, count=5, since_hours=48)
            )
            
            # ì¸í”Œë£¨ì–¸ì„œ í†µê³„ ì—…ë°ì´íŠ¸
            influencer['stats']['postsCollected'] = influencer['stats'].get('postsCollected', 0) + len(posts)
            influencer['lastFetched'] = datetime.now().isoformat()
            
            collection_results.append({
                'username': username,
                'posts_collected': len(posts)
            })
            
            all_posts.extend(posts)
            
            logger.info(f"âœ… @{username}: {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘")
        
        # AI ìš”ì•½ ìƒì„±
        summary_result = {}
        if all_posts:
            summary_result = loop.run_until_complete(
                crawler.generate_summary(all_posts)
            )
            logger.info(f"ğŸ“ AI ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary_result.get('summary', '')[:50]}...")
        else:
            summary_result = {
                'summary': "ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤",
                'hashtags': []
            }
        
        return jsonify({
            'success': True,
            'message': f'{len(active_influencers)}ëª…ì˜ ì¸í”Œë£¨ì–¸ì„œë¡œë¶€í„° {len(all_posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘',
            'data': {
                'total_posts': len(all_posts),
                'influencers': collection_results,
                'posts': all_posts[:10],  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
                'summary': summary_result.get('summary', ''),
                'hashtags': summary_result.get('hashtags', []),
                'analyzed_count': summary_result.get('analyzed_count', 0)
            }
        })
        
    except Exception as e:
        logger.error(f"ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/x-crawler/publish', methods=['POST', 'OPTIONS'])
def publish_to_x_crawler():
    """ìˆ˜ë™ ê²Œì‹œ"""
    if request.method == 'OPTIONS':
        response = jsonify({})
        response.headers['Access-Control-Allow-Origin'] = request.headers.get('Origin', '*')
        response.headers['Access-Control-Allow-Methods'] = 'POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, X-Credentials'
        return response
    
    try:
        # X API ìê²©ì¦ëª… í™•ì¸
        x_credentials = request.headers.get('X-Credentials')
        if not x_credentials:
            return jsonify({
                'success': False,
                'error': 'X API ìê²©ì¦ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤'
            }), 401
        
        # ìê²©ì¦ëª… ë””ì½”ë“œ
        try:
            import base64
            credentials = json.loads(base64.b64decode(x_credentials))
        except:
            return jsonify({
                'success': False,
                'error': 'ì˜ëª»ëœ ìê²©ì¦ëª… í˜•ì‹'
            }), 400
        
        # ìš”ì²­ ë°ì´í„°
        data = request.get_json() or {}
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = get_crawler()
        if not crawler.setup_x_api(credentials):
            return jsonify({
                'success': False,
                'error': 'X API ì¸ì¦ ì‹¤íŒ¨'
            }), 401
        
        # ê²Œì‹œí•  ì½˜í…ì¸ 
        content = data.get('content', {})
        
        # ìš”ì•½ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
        if not content.get('summary') and not content.get('text'):
            return jsonify({
                'success': False,
                'error': 'ê²Œì‹œí•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # Xì— ê²Œì‹œ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(
            crawler.post_to_x(content)
        )
        
        if result['success']:
            logger.info(f"âœ… X ê²Œì‹œ ì„±ê³µ: {result['tweet_id']}")
            return jsonify({
                'success': True,
                'message': 'Xì— ì„±ê³µì ìœ¼ë¡œ ê²Œì‹œë˜ì—ˆìŠµë‹ˆë‹¤',
                'data': result
            })
        else:
            logger.error(f"âŒ X ê²Œì‹œ ì‹¤íŒ¨: {result.get('error')}")
            return jsonify({
                'success': False,
                'error': result.get('error', 'ê²Œì‹œ ì‹¤íŒ¨')
            }), 400
            
    except Exception as e:
        logger.error(f"ê²Œì‹œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/x-crawler/queue', methods=['GET'])
def get_publish_queue():
    """ê²Œì‹œ í ì¡°íšŒ"""
    scheduler = get_scheduler()
    return jsonify({
        'success': True,
        'data': scheduler.publish_queue
    })

@app.route('/api/x-crawler/history', methods=['GET'])
def get_collection_history():
    """ìˆ˜ì§‘ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        crawler = get_crawler()
        
        # íˆìŠ¤í† ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        history = {
            'collections': crawler.collection_history[-20:],  # ìµœê·¼ 20ê°œ
            'publishes': crawler.publish_history[-20:],  # ìµœê·¼ 20ê°œ
            'total_collections': len(crawler.collection_history),
            'total_publishes': len(crawler.publish_history)
        }
        
        return jsonify({
            'success': True,
            'data': history
        })
        
    except Exception as e:
        logger.error(f"íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/x-crawler/stats', methods=['GET'])
def get_x_crawler_stats():
    """í†µê³„ ì¡°íšŒ (API í˜¸ì¶œ ìƒì„¸ í¬í•¨)"""
    crawler = get_crawler()
    scheduler = get_scheduler()
    
    stats = {
        'crawler': crawler.get_stats(),
        'scheduler': scheduler.get_status()
    }
    
    return jsonify({
        'success': True,
        'data': stats
    })

@app.route('/api/x-api-stats', methods=['GET'])
def get_x_api_stats():
    """X API í˜¸ì¶œ í†µê³„ ì¡°íšŒ"""
    try:
        # ìµœê·¼ ì‚¬ìš©í•œ publisher ì¸ìŠ¤í„´ìŠ¤ì˜ í†µê³„ (ìˆëŠ” ê²½ìš°)
        # ì‹¤ì œë¡œëŠ” ì „ì—­ ë˜ëŠ” ì„¸ì…˜ ê¸°ë°˜ìœ¼ë¡œ ê´€ë¦¬ í•„ìš”
        stats = {
            'message': 'X API í†µê³„ëŠ” X í¬ë¡¤ëŸ¬ í†µê³„ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤',
            'crawler_stats_url': '/api/x-crawler/stats'
        }
        
        return jsonify({
            'success': True,
            'data': stats
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/x-crawler/test', methods=['POST'])
def test_x_crawler():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        scheduler = get_scheduler()
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        collection_result = loop.run_until_complete(scheduler.test_collection())
        
        # ê²Œì‹œ í…ŒìŠ¤íŠ¸
        publish_result = loop.run_until_complete(scheduler.test_publishing())
        
        return jsonify({
            'success': True,
            'collection': collection_result,
            'publishing': publish_result
        })
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# ============================================================================
# Manual Summary Generation API
# ============================================================================

@app.route('/api/manual-summary/generate', methods=['POST', 'OPTIONS'])
def generate_manual_summary():
    """ìˆ˜ë™ ì…ë ¥ëœ í¬ìŠ¤íŒ…ìœ¼ë¡œ AI ìš”ì•½ ìƒì„±"""
    if request.method == 'OPTIONS':
        return '', 200
    
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['influencer_name', 'posts', 'ai_provider', 'ai_api_key']
        missing_fields = [field for field in required_fields if not data.get(field)]
        
        if missing_fields:
            return jsonify({
                'success': False,
                'error': f'í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {", ".join(missing_fields)}'
            }), 400
        
        influencer_name = data.get('influencer_name', '').strip()
        posts = data.get('posts', [])
        ai_provider = data.get('ai_provider')
        ai_api_key = data.get('ai_api_key')
        
        # í¬ìŠ¤íŒ… ë°ì´í„° ê²€ì¦
        if not posts or len(posts) == 0:
            return jsonify({
                'success': False,
                'error': 'ìµœì†Œ 1ê°œ ì´ìƒì˜ í¬ìŠ¤íŒ…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”'
            }), 400
        
        # ë¹ˆ í¬ìŠ¤íŒ… ì œê±°
        valid_posts = []
        for post in posts:
            if post.get('content', '').strip():
                valid_posts.append({
                    'id': f'manual_{int(time.time() * 1000)}_{len(valid_posts)}',
                    'author': influencer_name.replace('@', ''),
                    'text': post.get('content', '').strip(),
                    'created_at': post.get('datetime', datetime.now().isoformat()),
                    'likes': 0,  # ìˆ˜ë™ ì…ë ¥ì—ì„œëŠ” ì¢‹ì•„ìš” ìˆ˜ ë¶ˆí•„ìš”
                    'retweets': 0,  # ìˆ˜ë™ ì…ë ¥ì—ì„œëŠ” ë¦¬íŠ¸ìœ— ìˆ˜ ë¶ˆí•„ìš”
                    'engagement': 0,  # ìˆ˜ë™ ì…ë ¥ì—ì„œëŠ” ì¸ê²Œì´ì§€ë¨¼íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”
                    'url': f'https://twitter.com/{influencer_name.replace("@", "")}/status/manual_{len(valid_posts)}'
                })
        
        if not valid_posts:
            return jsonify({
                'success': False,
                'error': 'ìœ íš¨í•œ í¬ìŠ¤íŒ… ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤'
            }), 400
        
        logger.info(f"ìˆ˜ë™ ìš”ì•½ ìƒì„± ìš”ì²­: {influencer_name}, {len(valid_posts)}ê°œ í¬ìŠ¤íŒ…")
        
        # X í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° AI ì„¤ì •
        crawler = get_crawler()
        
        # AI API ì„¤ì •
        ai_setup_success = crawler.setup_ai_api({
            'provider': ai_provider,
            'api_key': ai_api_key
        })
        
        if not ai_setup_success:
            return jsonify({
                'success': False,
                'error': 'AI API ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
            }), 400
        
        # AI ìš”ì•½ ìƒì„± (ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            summary_result = loop.run_until_complete(crawler.generate_summary(valid_posts))
            
            if 'error' in summary_result:
                return jsonify({
                    'success': False,
                    'error': f'AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {summary_result["error"]}'
                }), 500
            
            logger.info(f"ìˆ˜ë™ ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summary_result.get('summary', ''))}ì")
            
            return jsonify({
                'success': True,
                'data': {
                    'influencer_name': influencer_name,
                    'posts_count': len(valid_posts),
                    'summary': summary_result.get('summary', ''),
                    'hashtags': summary_result.get('hashtags', []),
                    'analyzed_count': summary_result.get('analyzed_count', len(valid_posts)),
                    'generation_time': datetime.now().isoformat()
                }
            })
            
        finally:
            loop.close()
        
    except Exception as e:
        logger.error(f"ìˆ˜ë™ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        }), 500

if __name__ == '__main__':
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    port = int(os.getenv('PORT', 8080))  # Railway ê¸°ë³¸ í¬íŠ¸ 8080
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    logger.info(f"ğŸš€ Starting NONGBUXX API server on port {port}")
    logger.info(f"ğŸ”§ Debug mode: {debug}")
    logger.info(f"ğŸŒ Environment: {os.getenv('FLASK_ENV', 'development')}")
    logger.info(f"ğŸ”‘ API Keys - OpenAI: {'SET' if os.getenv('OPENAI_API_KEY') else 'NOT_SET'}, Anthropic: {'SET' if os.getenv('ANTHROPIC_API_KEY') else 'NOT_SET'}")
    
    # X í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” Flask ì•± ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œë§Œ ì‹œì‘
    # scheduler = get_scheduler()
    # scheduler.start()
    logger.info("ğŸ“… X í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„ë¨ (ìˆ˜ë™ ì‹œì‘ í•„ìš”)")
    

    
    try:
        # Railway í™˜ê²½ì—ì„œëŠ” gunicorn ì‚¬ìš©
        if os.getenv('RAILWAY_ENVIRONMENT_NAME'):
            import subprocess
            cmd = [
                'gunicorn',
                '--bind', f'0.0.0.0:{port}',
                '--workers', '2',
                '--timeout', '600',
                '--keep-alive', '10',
                '--max-requests', '1000',
                '--max-requests-jitter', '50',
                'app:app'
            ]
            logger.info(f"ğŸš‚ Railway environment detected, using gunicorn: {' '.join(cmd)}")
            subprocess.run(cmd)
        else:
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” Flask ê°œë°œ ì„œë²„ ì‚¬ìš©
            app.run(host='0.0.0.0', port=port, debug=debug)
    except Exception as e:
        logger.error(f"âŒ Failed to start server: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise # ê°•ì œ ì¬ë°°í¬ íŠ¸ë¦¬ê±° - Sat Aug  2 17:12:50 KST 2025
