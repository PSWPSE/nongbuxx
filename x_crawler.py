"""
X í¬ë¡¤ëŸ¬ ë°±ì—”ë“œ ì„œë¹„ìŠ¤
ì¸í”Œë£¨ì–¸ì„œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° AI ìš”ì•½ ìƒì„±
"""

import json
import logging
import os
import asyncio
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pytz
import tweepy
from openai import OpenAI
import anthropic

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('x_crawler')

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')


class XCrawler:
    """X í¬ë¡¤ëŸ¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.x_client = None
        self.ai_client = None
        self.ai_provider = None
        self.influencers = []
        self.collected_posts = []
        self.collection_history = []  # ìˆ˜ì§‘ ê¸°ë¡
        self.publish_history = []  # ê²Œì‹œ ê¸°ë¡
        self.api_usage = {  # API ì‚¬ìš©ëŸ‰
            'x_api': {'calls': 0, 'last_reset': datetime.now(KST)},
            'ai_api': {'tokens': 0, 'calls': 0}
        }
        
        # ìºì‹± ì¶”ê°€
        self.user_cache = {}  # ì‚¬ìš©ì ì •ë³´ ìºì‹œ
        self.cache_ttl = 21600  # 6ì‹œê°„ ìºì‹œ (ìˆ˜ì§‘ ì£¼ê¸°ì™€ ë™ì¼)
        self.last_collection_time = 0  # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„
        self.min_collection_interval = 21600  # ìµœì†Œ 6ì‹œê°„ ê°„ê²© (6 * 60 * 60)
        
        logger.info("âœ… X í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”")
    
    def setup_x_api(self, credentials: Dict[str, str]) -> bool:
        """X API ì„¤ì • - Free Tier ìµœì í™”"""
        try:
            # OAuth 1.0a ì„¤ì • (Free Tierì—ì„œ ì£¼ë¡œ ì‚¬ìš©)
            auth = tweepy.OAuthHandler(
                credentials.get('consumer_key'),
                credentials.get('consumer_secret')
            )
            auth.set_access_token(
                credentials.get('access_token'),
                credentials.get('access_token_secret')
            )
            
            # v1.1 API (Free Tierì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
            self.x_api_v1 = tweepy.API(auth, wait_on_rate_limit=True)
            
            # v2 Client ì„¤ì • (Free Tierì—ì„œ ì œí•œì )
            # OAuth 1.0a User Context ì‚¬ìš©
            self.x_client = tweepy.Client(
                consumer_key=credentials.get('consumer_key'),
                consumer_secret=credentials.get('consumer_secret'),
                access_token=credentials.get('access_token'),
                access_token_secret=credentials.get('access_token_secret'),
                wait_on_rate_limit=True
            )
            
            # ì¸ì¦ í…ŒìŠ¤íŠ¸ (Free Tierì—ì„œ ê°€ëŠ¥)
            user = self.x_api_v1.verify_credentials()
            logger.info(f"âœ… X API ì¸ì¦ ì„±ê³µ: @{user.screen_name}")
            logger.info("âš ï¸ Free Tier ì œí•œ: íŠ¸ìœ— ìˆ˜ì§‘ ë¶ˆê°€, ê²Œì‹œë§Œ ê°€ëŠ¥")
            return True
            
        except Exception as e:
            logger.error(f"âŒ X API ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False
    
    def setup_ai_api(self, config):
        """AI API ì„¤ì • - dict í˜•íƒœë¡œ ë°›ê¸°"""
        try:
            # dictë¡œ ë°›ì€ ê²½ìš°
            if isinstance(config, dict):
                provider = config.get('provider')
                api_key = config.get('api_key')
            # ì‹¤ìˆ˜ë¡œ ë‘ ê°œì˜ ì¸ìë¡œ ë°›ì€ ê²½ìš° (backward compatibility)
            else:
                provider = config
                api_key = None
                
            if not provider or not api_key:
                logger.warning("âš ï¸ AI API ì„¤ì • ì •ë³´ ë¶€ì¡±")
                return False
                
            if provider == 'openai':
                self.ai_client = OpenAI(api_key=api_key)
                self.ai_provider = 'openai'
            elif provider == 'anthropic':
                self.ai_client = anthropic.Anthropic(api_key=api_key)
                self.ai_provider = 'anthropic'
            elif provider == 'perplexity':
                # Perplexity APIëŠ” requestsë¡œ ì§ì ‘ í˜¸ì¶œ
                self.ai_client = None
                self.ai_provider = 'perplexity'
                self.perplexity_api_key = api_key
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì œê³µì: {provider}")
            
            logger.info(f"âœ… {provider} AI API ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ AI API ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def fetch_influencer_posts(
        self,
        username: str,
        count: int = 30,  # íŠ¸ìœ— 30ê°œë¡œ ì„¤ì •
        since_hours: int = 24
    ) -> List[Dict]:
        """ì¸í”Œë£¨ì–¸ì„œ ìµœì‹  í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ìµœì í™”ë¨)"""
        # ìˆ˜ì§‘ ê°„ê²© ì²´í¬
        current_time = time.time()
        if self.last_collection_time > 0:
            elapsed = current_time - self.last_collection_time
            if elapsed < self.min_collection_interval:
                remaining = int((self.min_collection_interval - elapsed) / 60)
                logger.warning(f"â³ ìˆ˜ì§‘ ê°„ê²© ì œí•œ: {remaining}ë¶„ í›„ ì¬ì‹œë„ ê°€ëŠ¥")
                return []
        
        try:
            if not self.x_client:
                logger.error("âŒ X API í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []
            
            # @ ì œê±°
            username = username.replace('@', '')
            
            logger.info(f"ğŸ“¥ {username} í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            
            # ì‹œê°„ í•„í„°
            since_time = datetime.now(KST) - timedelta(hours=since_hours)
            
            # ìºì‹œëœ ì‚¬ìš©ì ì •ë³´ í™•ì¸
            cache_key = f"user_{username}"
            user_id = None
            
            if cache_key in self.user_cache:
                cached = self.user_cache[cache_key]
                if current_time - cached['timestamp'] < self.cache_ttl:
                    user_id = cached['user_id']
                    logger.info(f"ğŸ“¦ ìºì‹œëœ ì‚¬ìš©ì ì •ë³´ ì‚¬ìš©: @{username}")
            
            # Free TierëŠ” ëŒ€ë¶€ë¶„ì˜ APIì— ì œí•œì´ ìˆìŒ
            # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ API í˜¸ì¶œ ëŒ€ì‹ )
            tweets = []
            
            # Free Tier ì œí•œ ì•ˆë‚´
            logger.warning(f"âš ï¸ X API Free TierëŠ” íƒ€ì„ë¼ì¸ ë° ê²€ìƒ‰ API ì ‘ê·¼ì´ ì œí•œë©ë‹ˆë‹¤.")
            logger.warning(f"âš ï¸ Basic Tier ($100/ì›”) ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            if username.lower() in ['nongbudaddy', 'elonmusk', 'test']:
                # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŠ¸ìœ— ìƒì„±
                sample_tweets = [
                    {
                        'id': f'sample_{datetime.now().timestamp():.0f}_1',
                        'author': username,
                        'text': f"[í…ŒìŠ¤íŠ¸] {username}ì˜ ìµœì‹  íŠ¸ìœ—ì…ë‹ˆë‹¤. Free Tierì—ì„œëŠ” ì‹¤ì œ íŠ¸ìœ—ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. #í…ŒìŠ¤íŠ¸ #XCrawler",
                        'created_at': datetime.now(KST).isoformat(),
                        'likes': 10,
                        'retweets': 5,
                        'url': f'https://twitter.com/{username}/status/sample1',
                        'engagement': 15
                    },
                    {
                        'id': f'sample_{datetime.now().timestamp():.0f}_2',
                        'author': username,
                        'text': f"[í…ŒìŠ¤íŠ¸] Basic Tierë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ë©´ ì‹¤ì œ íŠ¸ìœ— ìˆ˜ì§‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. #XAPI #ì—…ê·¸ë ˆì´ë“œ",
                        'created_at': (datetime.now(KST) - timedelta(hours=1)).isoformat(),
                        'likes': 20,
                        'retweets': 10,
                        'url': f'https://twitter.com/{username}/status/sample2',
                        'engagement': 30
                    }
                ]
                tweets = sample_tweets
                logger.info(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: @{username} - {len(tweets)}ê°œ")
                logger.info(f"âœ… {username}: {len(tweets)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ '{username}'ì€ í…ŒìŠ¤íŠ¸ ê³„ì •ì´ ì•„ë‹™ë‹ˆë‹¤. Free Tierì—ì„œëŠ” ì‹¤ì œ ìˆ˜ì§‘ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                tweets = []
                
        except tweepy.errors.TooManyRequests as e:
            logger.warning(f"âš ï¸ Rate limit ë„ë‹¬ - {username}: {str(e)}")
            self.collection_history.append({
                'timestamp': datetime.now(KST).isoformat(),
                'influencer': username,
                'posts_count': 0,
                'success': False,
                'error': 'Rate limit exceeded'
            })
            return []
        except Exception as e:
            logger.error(f"âŒ íŠ¸ìœ— ìˆ˜ì§‘ ì˜¤ë¥˜ - {username}: {str(e)}")
            return []
            
            # ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.last_collection_time = current_time
            
            # API ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ (íƒ€ì„ë¼ì¸ ì¡°íšŒ)
            self.api_usage['x_api']['calls'] += 1
            self.api_usage['x_api']['timeline_calls'] = self.api_usage['x_api'].get('timeline_calls', [])
            self.api_usage['x_api']['timeline_calls'].append({
                'timestamp': datetime.now(KST).isoformat(),
                'username': username,
                'count': len(tweets)
            })
            
            # ìˆ˜ì§‘ ê¸°ë¡ ì €ì¥
            if tweets:
                self.collection_history.append({
                    'timestamp': datetime.now(KST).isoformat(),
                    'influencer': username,
                    'posts_count': len(tweets),
                    'success': True
                })
            
            return tweets
            
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def collect_all_influencers(
        self, 
        influencers: List[str]
    ) -> List[Dict]:
        """ëª¨ë“  ì¸í”Œë£¨ì–¸ì„œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        all_posts = []
        
        for influencer in influencers:
            posts = await self.fetch_influencer_posts(influencer)
            all_posts.extend(posts)
            
            # Rate limit ê³ ë ¤í•œ ì§€ì—°
            await asyncio.sleep(2)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        all_posts.sort(key=lambda x: x['created_at'], reverse=True)
        
        logger.info(f"ğŸ“Š ì´ {len(all_posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_posts
    
    async def generate_summary(
        self, 
        posts: List[Dict],
        max_length: int = 280
    ) -> Dict:
        """AIë¥¼ ì´ìš©í•œ í¬ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            if not posts:
                return {
                    'summary': "",
                    'hashtags': [],
                    'error': "ìš”ì•½í•  í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"
                }
            
            if not self.ai_provider or (not self.ai_client and self.ai_provider != 'perplexity'):
                logger.warning("âš ï¸ AI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {
                    'summary': f"ğŸ“Š {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ (AI ìš”ì•½ ë¯¸ì„¤ì •)",
                    'hashtags': [],
                    'posts_count': len(posts)
                }
            
            # ì¸ê¸°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 10ê°œ ì„ íƒ
            posts_to_summarize = sorted(
                posts, 
                key=lambda x: x.get('engagement', x.get('likes', 0) + x.get('retweets', 0)), 
                reverse=True
            )[:10]
            
            # í¬ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ê²°í•© (ë¦¬í¬ìŠ¤íŒ… ì •ë³´ í¬í•¨)
            combined_text_parts = []
            for post in posts_to_summarize:
                post_info = f"@{post['author']} (â¤ï¸{post.get('likes', 0)} ğŸ”{post.get('retweets', 0)}):\n{post['text'][:200]}"
                
                # ë¦¬í¬ìŠ¤íŒ…ì¸ ê²½ìš° ì›ë¬¸ ì •ë³´ ì¶”ê°€
                if post.get('is_repost', False):
                    original_author = post.get('original_author', '')
                    original_content = post.get('original_content', '')
                    if original_author and original_content:
                        post_info += f"\n[ì°¸ì¡° ì›ë¬¸] @{original_author}: {original_content[:150]}"
                
                combined_text_parts.append(post_info)
            
            combined_text = "\n\n".join(combined_text_parts)
            
            # ìƒˆë¡œìš´ í¬ë§·íŒ… ìŠ¤íƒ€ì¼
            from datetime import datetime
            import pytz
            
            KST = pytz.timezone('Asia/Seoul')
            
            # ì¸í”Œë£¨ì–¸ì„œë³„ë¡œ ê·¸ë£¹í™”
            posts_by_author = {}
            for post in posts_to_summarize:
                author = post.get('author', 'unknown')
                if author not in posts_by_author:
                    posts_by_author[author] = []
                posts_by_author[author].append(post)
            
            # í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ ìƒì„± (ë¦¬í¬ìŠ¤íŒ… ì§€ì›)
            formatted_posts = []
            
            for author, author_posts in posts_by_author.items():
                formatted_posts.append(f"@{author}ì˜ í¬ìŠ¤íŠ¸:")
                for post in author_posts[:5]:  # ì‘ì„±ìë‹¹ ìµœëŒ€ 5ê°œ
                    # ë¦¬í¬ìŠ¤íŒ…ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                    if post.get('is_repost', False):
                        formatted_posts.append(f"ğŸ’¬ ì¸í”Œë£¨ì–¸ì„œ ì½”ë©˜íŠ¸: \"{post['text'][:150]}...\"")
                        created_at = post.get('created_at', '')
                        if created_at:
                            try:
                                dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                                dt_kst = dt.astimezone(KST)
                                formatted_posts.append(f"   -{dt_kst.strftime('%Y.%m.%d (%a) %H:%M')}")
                            except:
                                formatted_posts.append(f"   -ì‹œê°„ ì •ë³´ ì—†ìŒ")
                        
                        # ì›ë¬¸ ì •ë³´ ì¶”ê°€
                        original_author = post.get('original_author', '')
                        original_content = post.get('original_content', '')
                        original_datetime = post.get('original_datetime', '')
                        
                        if original_author and original_content:
                            formatted_posts.append(f"ğŸ“° ì°¸ì¡°í•œ í¬ìŠ¤íŒ…: @{original_author}ì˜ í¬ìŠ¤íŒ…")
                            formatted_posts.append(f"   \"{original_content[:150]}...\"")
                            if original_datetime:
                                try:
                                    dt = datetime.fromisoformat(original_datetime.replace('Z', '+00:00'))
                                    dt_kst = dt.astimezone(KST)
                                    formatted_posts.append(f"   -{dt_kst.strftime('%Y.%m.%d (%a) %H:%M')}")
                                except:
                                    formatted_posts.append(f"   -ì›ë¬¸ ì‹œê°„ ì •ë³´ ì—†ìŒ")
                    else:
                        # ì¼ë°˜ í¬ìŠ¤íŒ…
                        formatted_posts.append(f"â–¶ \"{post['text'][:150]}...\"")
                        created_at = post.get('created_at', '')
                        if created_at:
                            try:
                                dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                                dt_kst = dt.astimezone(KST)
                                formatted_posts.append(f"   -{dt_kst.strftime('%Y.%m.%d (%a) %H:%M')}")
                            except:
                                formatted_posts.append(f"   -ì‹œê°„ ì •ë³´ ì—†ìŒ")
                    
                    formatted_posts.append("------------------------------------")
                formatted_posts.append("")
            
            combined_text = "\n".join(formatted_posts)
            
            prompt = f"""ë‹¤ìŒ X(íŠ¸ìœ„í„°) í¬ìŠ¤íŠ¸ë“¤ì„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

[ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸]
{combined_text}

[ìš”êµ¬ í¬ë§·]
ğŸ“± ì˜¤ëŠ˜ì˜ @ì¸í”Œë£¨ì–¸ì„œëª… ì˜ ê²Œì‹œê¸€ ëª¨ìŒ  

ğŸ’¬ "ì¸í”Œë£¨ì–¸ì„œ í¬ìŠ¤íŒ… ë‚´ìš© ì¸ìš©ë¬¸"
-2025.09.02 (í™”) 10:30

ğŸ“° ì°¸ì¡°í•œ í¬ìŠ¤íŒ… : @ì°¸ì¡°ì›ë¬¸ì‘ì„±ìëª… ì˜ í¬ìŠ¤íŒ…
"ì°¸ì¡°í•œ í¬ìŠ¤íŒ…ì˜ ì›ë¬¸ ë‚´ìš© ì¸ìš©ë¬¸"
-2025.09.02 (í™”) 08:30

------------------------------------

ğŸ’¬ "ë‹¤ìŒ ì¸í”Œë£¨ì–¸ì„œ í¬ìŠ¤íŒ… ë‚´ìš©"
-2025.09.02 (í™”) 15:20

ğŸ“° ì°¸ì¡°í•œ í¬ìŠ¤íŒ… : @ì°¸ì¡°ì›ë¬¸ì‘ì„±ìëª… ì˜ í¬ìŠ¤íŒ…
"ì°¸ì¡°í•œ í¬ìŠ¤íŒ…ì˜ ì›ë¬¸ ë‚´ìš© ì¸ìš©ë¬¸"
-2025.09.02 (í™”) 14:00

------------------------------------

[ì‘ì„± ì›ì¹™]
1. ì œëª© ì•ì— ì ì ˆí•œ ì•Œë¦¼ ì´ëª¨ì§€ 1ê°œ ì¶”ê°€ (ğŸ“±, ğŸ“Š, ğŸ’°, ğŸ”¥, ğŸ“ˆ ë“± ë‚´ìš©ì— ë§ê²Œ)
2. ë¦¬í¬ìŠ¤íŒ…ì¸ ê²½ìš°: ğŸ’¬ ì¸í”Œë£¨ì–¸ì„œ ì½”ë©˜íŠ¸ â†’ ğŸ“° ì°¸ì¡° ì›ë¬¸ ìˆœì„œë¡œ ì‘ì„±
3. ì¼ë°˜ í¬ìŠ¤íŒ…ì¸ ê²½ìš°: ğŸ’¬ "í¬ìŠ¤íŒ… ë‚´ìš©" í˜•íƒœë¡œë§Œ ì‘ì„±
4. ê° í¬ìŠ¤íŒ… ì‚¬ì´ì— êµ¬ë¶„ì„ (------------------------------------) ì¶”ê°€
5. ì¸ìš©ë¬¸ì€ êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ ë²ˆì—­
6. ì‹œê°„ì€ í•œêµ­ ì‹œê°„(KST) ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ í‘œì‹œ
7. ë§í¬ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ
8. í•´ì‹œíƒœê·¸ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ"""
            
            if self.ai_provider == 'openai':
                response = self.ai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í¬ë§·ì— ë§ì¶° ì •í™•íˆ ì‘ì„±í•˜ê³ , êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=500,
                    temperature=0.7
                )
                ai_response = response.choices[0].message.content
                
            elif self.ai_provider == 'anthropic':
                response = self.ai_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=500,
                    temperature=0.7,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                ai_response = response.content[0].text
                
            elif self.ai_provider == 'perplexity':
                import requests
                headers = {
                    'Authorization': f'Bearer {self.perplexity_api_key}',
                    'Content-Type': 'application/json'
                }
                data = {
                    'model': 'llama-3.1-sonar-large-128k-chat',
                    'messages': [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í¬ë§·ì— ë§ì¶° ì •í™•íˆ ì‘ì„±í•˜ê³ , êµ¬ì–´ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    'max_tokens': 500,
                    'temperature': 0.7
                }
                response = requests.post(
                    'https://api.perplexity.ai/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=60
                )
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result['choices'][0]['message']['content']
                else:
                    logger.error(f"Perplexity API error: {response.status_code} - {response.text}")
                    raise Exception(f"Perplexity API error: {response.status_code}")
            
            else:
                return {
                    'summary': f"ğŸ“± ì˜¤ëŠ˜ì˜ í…Œí¬ ë¦¬ë” ì¸ì‚¬ì´íŠ¸\n\n{len(posts)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ë¨",
                    'hashtags': ['#XíŠ¸ë Œë“œ', '#AIìš”ì•½', '#ì¸í”Œë£¨ì–¸ì„œ'],
                    'posts_count': len(posts)
                }
            
            # ìƒˆë¡œìš´ í¬ë§· ì‘ë‹µ íŒŒì‹±
            lines = ai_response.strip().split('\n')
            summary = ai_response  # ì „ì²´ ì‘ë‹µì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
            
            # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì¶”ì¶œ
            positive_keywords = []
            negative_keywords = []
            hashtags = []
            
            for line in lines:
                if 'ê¸ì • í‚¤ì›Œë“œ:' in line:
                    keyword_text = line.split('ê¸ì • í‚¤ì›Œë“œ:')[1].strip()
                    positive_keywords = [tag.strip() for tag in keyword_text.split() if tag.startswith('#')]
                elif 'ë¶€ì • í‚¤ì›Œë“œ:' in line:
                    keyword_text = line.split('ë¶€ì • í‚¤ì›Œë“œ:')[1].strip()
                    negative_keywords = [tag.strip() for tag in keyword_text.split() if tag.startswith('#')]
            
            # ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•´ì‹œíƒœê·¸ë¡œ ê²°í•©
            hashtags = positive_keywords + negative_keywords
            
            # ê¸°ë³¸ê°’ ì²˜ë¦¬
            if not hashtags:
                hashtags = ['#XíŠ¸ë Œë“œ', '#AIìš”ì•½', '#ì¸í”Œë£¨ì–¸ì„œ', '#ì†Œì…œë¯¸ë””ì–´', '#íŠ¸ë Œë“œë¶„ì„']
            
            logger.info(f"âœ… AI ìš”ì•½ ìƒì„± ì™„ë£Œ ({len(summary)}ì, í•´ì‹œíƒœê·¸ {len(hashtags)}ê°œ)")
            
            return {
                'summary': summary,
                'hashtags': hashtags,
                'posts_count': len(posts),
                'analyzed_count': len(posts_to_summarize)
            }
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                'summary': f"ğŸ“± ì˜¤ëŠ˜ì˜ ì¸ì‚¬ì´íŠ¸: {len(posts)}ê°œ í¬ìŠ¤íŠ¸",
                'hashtags': [],
                'error': str(e)
            }
    
    async def create_summary_content(
        self, 
        posts: List[Dict]
    ) -> Dict:
        """ìš”ì•½ ì½˜í…ì¸  ìƒì„±"""
        # ì¸í”Œë£¨ì–¸ì„œë³„ ê·¸ë£¹í™”
        by_author = {}
        for post in posts:
            author = post['author']
            if author not in by_author:
                by_author[author] = []
            by_author[author].append(post)
        
        # ìš”ì•½ ìƒì„±
        summary_text = await self.generate_summary(posts)
        
        # í•´ì‹œíƒœê·¸ ìƒì„±
        hashtags = self.generate_hashtags(posts)
        
        # ìµœì¢… ì½˜í…ì¸ 
        content = {
            'id': f'summary_{datetime.now(KST).strftime("%Y%m%d_%H%M%S")}',
            'text': summary_text,
            'hashtags': hashtags,
            'posts_count': len(posts),
            'influencers': list(by_author.keys()),
            'created_at': datetime.now(KST).isoformat(),
            'metadata': {
                'top_post': max(posts, key=lambda x: x['likes'] + x['retweets']) if posts else None
            }
        }
        
        return content
    
    def generate_hashtags(self, posts: List[Dict]) -> List[str]:
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        # ê°„ë‹¨í•œ í•´ì‹œíƒœê·¸ ìƒì„± ë¡œì§
        hashtags = ['#í…Œí¬ë‰´ìŠ¤', '#AI', '#í˜ì‹ ']
        
        # ì¸í”Œë£¨ì–¸ì„œ ê¸°ë°˜ í•´ì‹œíƒœê·¸
        authors = list(set(post['author'] for post in posts))
        for author in authors[:3]:  # ìµœëŒ€ 3ëª…
            if 'elon' in author.lower():
                hashtags.append('#ì¼ë¡ ë¨¸ìŠ¤í¬')
            elif 'sundar' in author.lower():
                hashtags.append('#êµ¬ê¸€')
        
        return hashtags[:5]  # ìµœëŒ€ 5ê°œ
    
    async def post_to_x(self, content: Dict) -> Dict:
        """Xì— ê²Œì‹œ"""
        try:
            if not self.x_client:
                logger.error("âŒ X API í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {
                    'success': False,
                    'error': 'X APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
                }
            
            # í…ìŠ¤íŠ¸ì™€ í•´ì‹œíƒœê·¸ ê²°í•©
            full_text = content.get('text', content.get('summary', ''))
            if content.get('hashtags'):
                hashtags_text = ' '.join(content['hashtags'])
                # 280ì ì œí•œ ê³ ë ¤
                if len(full_text) + len(hashtags_text) + 2 <= 280:
                    full_text = f"{full_text}\n\n{hashtags_text}"
                else:
                    # ë³¸ë¬¸ì„ ì¤„ì´ê³  í•´ì‹œíƒœê·¸ ìœ ì§€
                    max_text_len = 280 - len(hashtags_text) - 5  # "\n\n" + "..."
                    full_text = f"{full_text[:max_text_len]}...\n\n{hashtags_text}"
            
            # íŠ¸ìœ— ê²Œì‹œ (v1.1 API ì‚¬ìš©)
            try:
                tweet = self.x_api_v1.update_status(full_text)
                
                logger.info(f"âœ… X ê²Œì‹œ ì„±ê³µ: {tweet.id_str}")
                
                # ê²Œì‹œ ê¸°ë¡ ì €ì¥
                self.publish_history.append({
                    'timestamp': datetime.now(KST).isoformat(),
                    'tweet_id': tweet.id_str,
                    'url': f'https://twitter.com/user/status/{tweet.id_str}',
                    'content': full_text,
                    'success': True
                })
                
                # API ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                self.api_usage['x_api']['calls'] += 1
                
                return {
                    'success': True,
                    'tweet_id': tweet.id_str,
                    'url': f'https://twitter.com/user/status/{tweet.id_str}',
                    'content': full_text,
                    'created_at': tweet.created_at.isoformat()
                }
                
            except tweepy.errors.Forbidden as e:
                logger.error(f"âŒ ê¶Œí•œ ì˜¤ë¥˜: {str(e)}")
                return {
                    'success': False,
                    'error': 'ê²Œì‹œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. Write ê¶Œí•œì´ ìˆëŠ” API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
                }
            except tweepy.errors.TooManyRequests as e:
                logger.warning(f"âš ï¸ Rate limit ì´ˆê³¼: {str(e)}")
                return {
                    'success': False,
                    'error': 'API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                    'retry_after': 900  # 15ë¶„
                }
            except tweepy.errors.TweepyException as e:
                logger.error(f"âŒ Tweepy ì˜¤ë¥˜: {str(e)}")
                return {
                    'success': False,
                    'error': f'ê²Œì‹œ ì‹¤íŒ¨: {str(e)}'
                }
            
        except Exception as e:
            logger.error(f"âŒ X ê²Œì‹œ ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_stats(self) -> Dict:
        """í†µê³„ ì •ë³´ (API í˜¸ì¶œ ìƒì„¸ í¬í•¨)"""
        # ìµœê·¼ 24ì‹œê°„ í†µê³„
        now = datetime.now(KST)
        last_24h = now - timedelta(hours=24)
        today = now.date()
        
        recent_collections = [
            h for h in self.collection_history 
            if datetime.fromisoformat(h['timestamp']) > last_24h
        ]
        
        recent_publishes = [
            h for h in self.publish_history
            if datetime.fromisoformat(h['timestamp']) > last_24h
        ]
        
        # API í˜¸ì¶œ í†µê³„ ê³„ì‚°
        user_calls_today = 0
        timeline_calls_today = 0
        user_calls_24h = 0
        timeline_calls_24h = 0
        
        # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ í†µê³„
        for call in self.api_usage['x_api'].get('user_calls', []):
            call_time = datetime.fromisoformat(call['timestamp'])
            if call_time.date() == today:
                user_calls_today += 1
            if call_time > last_24h:
                user_calls_24h += 1
        
        # íƒ€ì„ë¼ì¸ ì¡°íšŒ í†µê³„
        for call in self.api_usage['x_api'].get('timeline_calls', []):
            call_time = datetime.fromisoformat(call['timestamp'])
            if call_time.date() == today:
                timeline_calls_today += 1
            if call_time > last_24h:
                timeline_calls_24h += 1
        
        return {
            'overview': {
                'total_collected': len(self.collected_posts),
                'total_published': len(self.publish_history),
                'influencers_count': len(self.influencers),
                'last_collection': self.collection_history[-1]['timestamp'] if self.collection_history else None,
                'last_publish': self.publish_history[-1]['timestamp'] if self.publish_history else None
            },
            'last_24h': {
                'collections': len(recent_collections),
                'publishes': len(recent_publishes),
                'success_rate': self._calculate_success_rate(recent_collections)
            },
            'api_calls': {
                'today': {
                    'user_lookups': user_calls_today,
                    'timeline_fetches': timeline_calls_today,
                    'total': user_calls_today + timeline_calls_today
                },
                'last_24h': {
                    'user_lookups': user_calls_24h,
                    'timeline_fetches': timeline_calls_24h,
                    'total': user_calls_24h + timeline_calls_24h
                },
                'all_time': {
                    'user_lookups': len(self.api_usage['x_api'].get('user_calls', [])),
                    'timeline_fetches': len(self.api_usage['x_api'].get('timeline_calls', [])),
                    'total': self.api_usage['x_api']['calls']
                }
            },
            'api_status': {
                'x_api': {
                    'connected': self.x_client is not None,
                    'calls_made': self.api_usage['x_api']['calls'],
                    'last_reset': self.api_usage['x_api']['last_reset'].isoformat()
                },
                'ai_api': {
                    'connected': self.ai_client is not None,
                    'calls_made': self.api_usage['ai_api']['calls'],
                    'tokens_used': self.api_usage['ai_api']['tokens']
                }
            },
            'recent_activity': {
                'collections': self.collection_history[-5:],  # ìµœê·¼ 5ê°œ
                'publishes': self.publish_history[-5:]  # ìµœê·¼ 5ê°œ
            }
        }
    
    def _calculate_success_rate(self, records):
        """ì„±ê³µë¥  ê³„ì‚°"""
        if not records:
            return 100.0
        successful = sum(1 for r in records if r.get('success', False))
        return round((successful / len(records)) * 100, 1)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
crawler_instance = None

def get_crawler() -> XCrawler:
    """í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global crawler_instance
    if crawler_instance is None:
        crawler_instance = XCrawler()
    return crawler_instance


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    async def test():
        crawler = get_crawler()
        
        # API ì„¤ì • (ì‹¤ì œ í‚¤ í•„ìš”)
        # crawler.setup_x_api({...})
        # crawler.setup_ai_api('openai', 'sk-...')
        
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
        test_posts = [
            {
                'id': '1',
                'author': 'elonmusk',
                'text': 'AI is the future',
                'created_at': datetime.now(KST).isoformat(),
                'likes': 1000,
                'retweets': 500,
                'url': 'https://twitter.com/test'
            }
        ]
        
        # ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸
        content = await crawler.create_summary_content(test_posts)
        print(f"ìƒì„±ëœ ì½˜í…ì¸ : {json.dumps(content, indent=2, ensure_ascii=False)}")
    
    asyncio.run(test())
