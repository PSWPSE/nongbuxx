#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© íŒŒì„œ
ê¸°ì¡´ ì‹œìŠ¤í…œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ íŒŒì„œ
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from typing import List, Dict, Optional, Any
import logging
from fake_useragent import UserAgent
import re
from urllib.parse import urljoin, urlparse

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class NaverNewsExtractor:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ì¶”ì¶œê¸°"""
    
    def __init__(self, base_url: str, search_keywords: Optional[str] = None, max_news: int = 10):
        self.base_url = base_url
        self.search_keywords = search_keywords
        self.max_news = max_news
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì í™” í—¤ë”
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.timeout = 15
        
        # ğŸš¨ ê°•í™”ëœ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
        self.promotional_patterns = {
            # ì œëª© ê¸°ë°˜ í™ë³´ì„± í‚¤ì›Œë“œ
            'title_keywords': [
                # í•œêµ­ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ê´‘ê³ ', 'í”„ë¡œëª¨ì…˜', 'í™ë³´', 'ì„ ì „', 'ì–´í•„', 'ì¶”ì²œ', 'ì†Œê°œ',
                'ë°”ë¡œê°€ê¸°', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°', 'êµ¬ë…', 'íŒ”ë¡œìš°', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…',
                'ëŒ“ê¸€', 'í›„ì›', 'ì œíœ´', 'í˜‘ì°¬', 'ìŠ¤í°ì„œ', 'ì§€ì›', 'ë„ì›€',
                'íŠ¹ê°€', 'í• ì¸', 'ì´ë²¤íŠ¸', 'í–‰ì‚¬', 'ëª¨ì§‘', 'ì±„ìš©', 'ê³µê³ ',
                'ì¶œì‹œ', 'ëŸ°ì¹­', 'ì˜¤í”ˆ', 'ì˜¤í”ˆì‹', 'ê¸°ë…', 'ì¶•í•˜', 'ê°ì‚¬',
                'ë‹¹ì²¨', 'ë‹¹ì²¨ì', 'ìˆ˜ìƒ', 'ìˆ˜ìƒì', 'ì‹œìƒ', 'ì‹œìƒì‹',
                'ë¬´ë£Œ', 'ì²´í—˜', 'ìƒ˜í”Œ', 'ì¦ì •', 'ê¸°í”„íŠ¸', 'ì„ ë¬¼',
                'AD', 'Sponsored', 'í›„ì›', 'ì œíœ´',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± í‚¤ì›Œë“œ ì¶”ê°€
                'etf ì†Œê°œ', 'etf ì¶”ì²œ', 'etf íˆ¬ì', 'etf ë¶„ì„', 'etf ì „ëµ',
                'íˆ¬ì ë ˆì´ë”', 'íˆ¬ì ê¸°íšŒ', 'íˆ¬ì ê°€ì¹˜', 'íˆ¬ì í¬ì¸íŠ¸',
                'íˆ¬ì ê³ ë ¤ì‚¬í•­', 'íˆ¬ì ê²€í† ', 'íˆ¬ì í‰ê°€', 'íˆ¬ì ì „ë§',
                'etf introduction', 'etf recommendation', 'etf investment',
                'investment radar', 'investment opportunity', 'investment value',
                'investment point', 'investment consideration', 'investment review',
                'investment evaluation', 'investment outlook',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ í‚¤ì›Œë“œ ì¶”ê°€
                'ì£¼ì‹ ì–´ë–„', 'ì£¼ì‹ ì¶”ì²œ', 'ì£¼ì‹ íˆ¬ì', 'ì£¼ì‹ ë§¤ìˆ˜', 'ì£¼ì‹ ë§¤ë„',
                'íˆ¬ì ì ê¸°', 'íˆ¬ì íƒ€ì´ë°', 'íˆ¬ì ì œì•ˆ', 'íˆ¬ì ì¶”ì²œ',
                'ë§¤ìˆ˜ ì‹œì ', 'ë§¤ë„ ì‹œì ', 'ë§¤ìˆ˜ íƒ€ì´ë°', 'ë§¤ë„ íƒ€ì´ë°',
                'ì£¼ê°€ ì „ë§', 'ì£¼ê°€ ì˜ˆì¸¡', 'ì£¼ê°€ ë¶„ì„', 'ì£¼ê°€ ì¶”ì²œ',
                'ì¢…ëª© ì¶”ì²œ', 'ì¢…ëª© ë¶„ì„', 'ì¢…ëª© ì „ë§', 'ì¢…ëª© íˆ¬ì',
                'stock recommendation', 'stock pick', 'stock analysis',
                'investment suggestion', 'investment advice', 'buy recommendation',
                'sell recommendation', 'timing', 'opportunity',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ í‚¤ì›Œë“œ (ì½˜í…ì¸  ìƒì„± ì‹œ ì œê±°ìš©)
                'zacks', 'zacks investment research', 'zacks rank', 'zacks industry rank',
                'zacks analyst', 'zacks estimate', 'zacks rating', 'zacks ranking',
                'automated insights', 'ai generated', 'machine learning',
                'ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨', 'ìë£Œ ì‚¬ìš©í•¨', 'ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨',
            ],
            
            # URL íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'url_patterns': [
                r'/ad/', r'/ads/', r'/advertisement/', r'/sponsored/',
                r'/promotion/', r'/promotional/', r'/event/', r'/events/',
                r'/contest/', r'/giveaway/', r'/sweepstakes/', r'/sale/',
                r'/deal/', r'/offer/', r'/special/', r'/limited/',
                r'/free/', r'/trial/', r'/sample/', r'/gift/',
                r'/subscribe/', r'/signup/', r'/register/', r'/join/',
                r'/membership/', r'/premium/', r'/vip', r'/exclusive/',
                r'/press-release/', r'/announcement/', r'/launch/', r'/release/',
                r'/partnership/', r'/collaboration/', r'/sponsor/', r'/sponsored/',
                
                # ğŸš¨ ETF ë° íˆ¬ì ìƒí’ˆ ê´€ë ¨ URL íŒ¨í„´ ì¶”ê°€
                r'/etf/', r'/etfs/', r'/fund/', r'/funds/', r'/investment-product/',
                r'/product/', r'/products/', r'/investment/', r'/investing/',
                r'/portfolio/', r'/strategy/', r'/analysis/', r'/research/',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ ê´€ë ¨ URL íŒ¨í„´ ì¶”ê°€
                r'/recommendation/', r'/recommendations/', r'/pick/', r'/picks/',
                r'/advice/', r'/suggestion/', r'/timing/', r'/opportunity/',
                r'/buy/', r'/sell/', r'/trade/', r'/trading/',
                r'/stock-pick/', r'/stock-recommendation/', r'/investment-advice/',
                r'/market-timing/', r'/investment-timing/',
            ],
            
            # ì œëª© íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'title_patterns': [
                r'\[.*ê´‘ê³ .*\]', r'\[.*sponsored.*\]', r'\[.*ad.*\]',
                r'\(.*ê´‘ê³ .*\)', r'\(.*sponsored.*\)', r'\(.*ad.*\)',
                r'\[.*í”„ë¡œëª¨ì…˜.*\]', r'\[.*promotion.*\]',
                r'\[.*ì´ë²¤íŠ¸.*\]', r'\[.*event.*\]',
                r'\[.*íŠ¹ê°€.*\]', r'\[.*sale.*\]', r'\[.*í• ì¸.*\]',
                r'\[.*ë¬´ë£Œ.*\]', r'\[.*free.*\]',
                r'\[.*ì¶œì‹œ.*\]', r'\[.*launch.*\]',
                r'\[.*ëŸ°ì¹­.*\]', r'\[.*release.*\]',
                r'\[.*ê³µê°œ.*\]', r'\[.*announcement.*\]',
                r'\[.*ë‹¹ì²¨.*\]', r'\[.*winner.*\]',
                r'\[.*ìˆ˜ìƒ.*\]', r'\[.*award.*\]',
                
                # ğŸš¨ ETF ì†Œê°œ ë° íˆ¬ì ìƒí’ˆ í™ë³´ì„± íŒ¨í„´ ì¶”ê°€
                r'.*etf.*íˆ¬ì.*ë ˆì´ë”.*', r'.*etf.*íˆ¬ì.*ê¸°íšŒ.*',
                r'.*etf.*íˆ¬ì.*ê°€ì¹˜.*', r'.*etf.*íˆ¬ì.*í¬ì¸íŠ¸.*',
                r'.*etf.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*', r'.*etf.*íˆ¬ì.*ê²€í† .*',
                r'.*etf.*íˆ¬ì.*í‰ê°€.*', r'.*etf.*íˆ¬ì.*ì „ë§.*',
                r'.*etf.*ì†Œê°œ.*', r'.*etf.*ì¶”ì²œ.*', r'.*etf.*ë¶„ì„.*',
                r'.*etf.*ì „ëµ.*', r'.*íˆ¬ì.*ë ˆì´ë”.*', r'.*íˆ¬ì.*ê¸°íšŒ.*',
                r'.*etf.*íˆ¬ì.*ê°€ì¹˜.*', r'.*etf.*íˆ¬ì.*í¬ì¸íŠ¸.*', r'.*etf.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*',
                r'.*etf.*íˆ¬ì.*ê²€í† .*', r'.*etf.*íˆ¬ì.*í‰ê°€.*', r'.*etf.*íˆ¬ì.*ì „ë§.*',
                
                # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ íŒ¨í„´ ì¶”ê°€
                r'.*ì£¼ì‹\s+ì–´ë–„\?.*', r'.*ì£¼ì‹\s+ì¶”ì²œ.*', r'.*ì£¼ì‹\s+íˆ¬ì.*',
                r'.*ì£¼ì‹\s+ë§¤ìˆ˜.*', r'.*ì£¼ì‹\s+ë§¤ë„.*', r'.*íˆ¬ì\s+ì ê¸°.*',
                r'.*íˆ¬ì\s+íƒ€ì´ë°.*', r'.*íˆ¬ì\s+ì œì•ˆ.*', r'.*íˆ¬ì\s+ì¶”ì²œ.*',
                r'.*ë§¤ìˆ˜\s+ì‹œì .*', r'.*ë§¤ë„\s+ì‹œì .*', r'.*ë§¤ìˆ˜\s+íƒ€ì´ë°.*',
                r'.*ë§¤ë„\s+íƒ€ì´ë°.*', r'.*ì£¼ê°€\s+ì „ë§.*', r'.*ì£¼ê°€\s+ì˜ˆì¸¡.*',
                r'.*ì£¼ê°€\s+ë¶„ì„.*', r'.*ì£¼ê°€\s+ì¶”ì²œ.*', r'.*ì¢…ëª©\s+ì¶”ì²œ.*',
                r'.*ì¢…ëª©\s+ë¶„ì„.*', r'.*ì¢…ëª©\s+ì „ë§.*', r'.*ì¢…ëª©\s+íˆ¬ì.*',
                
                # ğŸš¨ Zacks/Automated Insights ê´€ë ¨ íŒ¨í„´ ì¶”ê°€
                r'.*zacks.*', r'.*automated insights.*', r'.*ai generated.*',
                r'.*machine learning.*', r'.*ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•¨.*',
                r'.*ìë£Œ ì‚¬ìš©í•¨.*', r'.*ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨.*',
            ],
            
            # ì§§ì€ ì œëª© í•„í„° (í™ë³´ì„± ì œëª©ì€ ë³´í†µ ì§§ìŒ)
            'min_title_length': 15,
            
            # ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì í•„í„° (í™ë³´ì„± ì œëª©ì— ìì£¼ ë‚˜íƒ€ë‚¨)
            'excessive_symbols': [
                '!', '!!', '!!!', '?', '??', '???', '~', '~~', '~~~',
                'â˜…', 'â˜†', 'â™¥', 'â™¡', 'â™ ', 'â™£', 'â™¦', 'â—', 'â—‹', 'â—†', 'â—‡',
                'â–¶', 'â—€', 'â–²', 'â–¼', 'â– ', 'â–¡', 'â–£', 'â–¤', 'â–¥', 'â–¦', 'â–§', 'â–¨', 'â–©',
            ]
        }
    
    def extract_news(self) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜"""
        try:
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {self.base_url}")
            
            # HTML ìš”ì²­
            html_content = self._fetch_html()
            if not html_content:
                return []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
            news_links = self._extract_naver_news_links(html_content)
            
            # ğŸš¨ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì ìš©
            filtered_links = self._filter_promotional_content(news_links)
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
            news_items = self._create_news_items(filtered_links)
            
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ {len(news_links)}ê°œ ë‰´ìŠ¤ ì¶”ì¶œ, í™ë³´ì„± í•„í„°ë§ í›„ {len(filtered_links)}ê°œ ìœ ì§€")
            return news_items[:self.max_news]
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _filter_promotional_content(self, links: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """í™ë³´ì„± ì½˜í…ì¸  í•„í„°ë§"""
        filtered_links = []
        
        for link in links:
            title = link['title']
            url = link['url']
            
            # í™ë³´ì„± ì½˜í…ì¸  ì²´í¬
            if self._is_promotional_content(title, url):
                print(f"ğŸš« í™ë³´ì„± ë‰´ìŠ¤ ì œì™¸: {title[:50]}...")
                continue
            
            filtered_links.append(link)
        
        return filtered_links
    
    def _is_promotional_content(self, title: str, url: str) -> bool:
        """í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        url_lower = url.lower()
        
        # 1. ì œëª© í‚¤ì›Œë“œ ì²´í¬ (ë” ì •êµí•œ íŒë‹¨)
        promotional_keywords = 0
        for keyword in self.promotional_patterns['title_keywords']:
            if keyword.lower() in title_lower:
                promotional_keywords += 1
        
        # í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ í™ë³´ì„±ìœ¼ë¡œ íŒë‹¨ (ë‹¨ì¼ í‚¤ì›Œë“œëŠ” í—ˆìš©)
        if promotional_keywords >= 2:
            return True
        
        # 2. URL íŒ¨í„´ ì²´í¬
        for pattern in self.promotional_patterns['url_patterns']:
            if re.search(pattern, url_lower):
                return True
        
        # 3. ì œëª© íŒ¨í„´ ì²´í¬ (ëŒ€ê´„í˜¸, ì†Œê´„í˜¸ ì•ˆì˜ í™ë³´ì„± í‚¤ì›Œë“œ)
        for pattern in self.promotional_patterns['title_patterns']:
            if re.search(pattern, title, re.IGNORECASE):
                return True
        
        # 4. ğŸš¨ ETF í™ë³´ì„± ì½˜í…ì¸  íŠ¹ë³„ ì²´í¬ (ë” ì •í™•í•œ íŒë‹¨)
        if self._is_etf_promotional_content(title):
            return True
        
        # 5. ì œëª© ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ í™ë³´ì„±ì¼ ê°€ëŠ¥ì„±)
        if len(title.strip()) < self.promotional_patterns['min_title_length']:
            return True
        
        # 6. ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì ì²´í¬
        symbol_count = sum(1 for symbol in self.promotional_patterns['excessive_symbols'] if symbol in title)
        if symbol_count >= 3:  # 3ê°œ ì´ìƒì˜ íŠ¹ìˆ˜ë¬¸ìê°€ ìˆìœ¼ë©´ í™ë³´ì„±
            return True
        
        # 7. ë°˜ë³µë˜ëŠ” ë¬¸ì ì²´í¬ (ì˜ˆ: "ëŒ€ë°•!!!", "ìµœê³ !!!")
        if re.search(r'([!?~â˜…â˜†â™¥â™¡])\1{2,}', title):
            return True
        
        # 8. ê³¼ë„í•œ ëŒ€ë¬¸ì ì²´í¬ (í™ë³´ì„± ì œëª©ì€ ëŒ€ë¬¸ìë¥¼ ë§ì´ ì‚¬ìš©)
        uppercase_ratio = sum(1 for char in title if char.isupper()) / len(title) if title else 0
        if uppercase_ratio > 0.7:  # 70% ì´ìƒì´ ëŒ€ë¬¸ìë©´ í™ë³´ì„±
            return True
        
        # 9. ğŸš¨ ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ ì²´í¬ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        if self._is_normal_news_content(title):
            return False
        
        return False
    
    def _is_normal_news_content(self, title: str) -> bool:
        """ì •ìƒì ì¸ ë‰´ìŠ¤ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        
        # ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ë“¤
        normal_news_patterns = [
            # ê¸°ì—… ê´€ë ¨
            r'.*ê¸°ì—….*ì‹¤ì .*',
            r'.*ê¸°ì—….*ì„±ê³¼.*',
            r'.*ê¸°ì—….*ì „ëµ.*',
            r'.*ê¸°ì—….*ë°œí‘œ.*',
            r'.*ê¸°ì—….*ì¶œì‹œ.*',
            r'.*ê¸°ì—….*ì§„ì¶œ.*',
            r'.*ê¸°ì—….*íˆ¬ì.*',
            r'.*ê¸°ì—….*ì¸ìˆ˜.*',
            r'.*ê¸°ì—….*í•©ë³‘.*',
            
            # ì‹œì¥ ê´€ë ¨
            r'.*ì‹œì¥.*ë™í–¥.*',
            r'.*ì‹œì¥.*ë¶„ì„.*',
            r'.*ì‹œì¥.*ì „ë§.*',
            r'.*ì‹œì¥.*ë³€í™”.*',
            r'.*ì‹œì¥.*ì„±ì¥.*',
            r'.*ì‹œì¥.*ê·œëª¨.*',
            
            # ê²½ì œ ê´€ë ¨
            r'.*ê²½ì œ.*ì •ì±….*',
            r'.*ê²½ì œ.*ì§€í‘œ.*',
            r'.*ê²½ì œ.*ì„±ì¥.*',
            r'.*ê²½ì œ.*ì „ë§.*',
            
            # ê¸°ìˆ  ê´€ë ¨
            r'.*ê¸°ìˆ .*ê°œë°œ.*',
            r'.*ê¸°ìˆ .*í˜ì‹ .*',
            r'.*ê¸°ìˆ .*íŠ¸ë Œë“œ.*',
            r'.*ê¸°ìˆ .*ë™í–¥.*',
            
            # ì£¼ì‹/íˆ¬ì ê´€ë ¨ (ì •ìƒì ì¸ ë‰´ìŠ¤)
            r'.*ì£¼ê°€.*ìƒìŠ¹.*',
            r'.*ì£¼ê°€.*í•˜ë½.*',
            r'.*ì£¼ê°€.*ë³€ë™.*',
            r'.*íˆ¬ì.*ë™í–¥.*',
            r'.*íˆ¬ì.*í™˜ê²½.*',
            r'.*íˆ¬ì.*ì‹œì¥.*',
        ]
        
        # ì •ìƒì ì¸ ë‰´ìŠ¤ íŒ¨í„´ í™•ì¸
        for pattern in normal_news_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def _is_etf_promotional_content(self, title: str) -> bool:
        """ETF í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ ë” ì •í™•í•˜ê²Œ íŒë‹¨"""
        title_lower = title.lower()
        
        # ğŸš¨ ëª…í™•í•œ ETF í™ë³´ì„± íŒ¨í„´ë“¤ (ì˜ë¬¸í˜•, ì¶”ì²œí˜•, ì†Œê°œí˜•)
        etf_promotional_patterns = [
            # ì˜ë¬¸í˜• íŒ¨í„´ (íˆ¬ì ê²°ì •ì„ ìš”êµ¬í•˜ëŠ” í˜•íƒœ)
            r'.*etf.*íˆ¬ì.*ë ˆì´ë”.*ì˜¬ë ¤ì•¼.*í• ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ê°€ì¹˜.*ìˆì„ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ê¸°íšŒ.*í• ê¹Œ\?',
            r'.*etf.*íˆ¬ì.*ì¶”ì²œ.*í• ê¹Œ\?',
            
            # ì†Œê°œ/ì¶”ì²œí˜• íŒ¨í„´
            r'.*etf.*ì†Œê°œ.*',
            r'.*etf.*ì¶”ì²œ.*',
            r'.*etf.*íˆ¬ì.*ì „ëµ.*',
            
            # íˆ¬ì ë ˆì´ë” ê´€ë ¨ (ëª…í™•í•œ í™ë³´ì„±)
            r'.*íˆ¬ì.*ë ˆì´ë”.*',
            r'.*íˆ¬ì.*ê¸°íšŒ.*',
            r'.*íˆ¬ì.*ê°€ì¹˜.*',
            r'.*íˆ¬ì.*í¬ì¸íŠ¸.*',
            r'.*íˆ¬ì.*ê³ ë ¤ì‚¬í•­.*',
            r'.*íˆ¬ì.*ê²€í† .*',
            r'.*íˆ¬ì.*í‰ê°€.*',
            r'.*íˆ¬ì.*ì „ë§.*',
        ]
        
        # ğŸš¨ ì£¼ì‹ ì¶”ì²œ ë° íˆ¬ì ì œì•ˆ íŒ¨í„´ë“¤ ì¶”ê°€
        stock_promotional_patterns = [
            # ì˜ë¬¸í˜• íŒ¨í„´ (íˆ¬ì ê²°ì •ì„ ìš”êµ¬í•˜ëŠ” í˜•íƒœ)
            r'.*ì£¼ì‹.*ì–´ë–„\?.*',
            r'.*ì£¼ì‹.*íˆ¬ì.*í• ê¹Œ\?.*',
            r'.*íˆ¬ì.*ì ê¸°.*',
            r'.*íˆ¬ì.*íƒ€ì´ë°.*',
            r'.*ë§¤ìˆ˜.*ì‹œì .*',
            r'.*ë§¤ë„.*ì‹œì .*',
            r'.*ë§¤ìˆ˜.*íƒ€ì´ë°.*',
            r'.*ë§¤ë„.*íƒ€ì´ë°.*',
            
            # ì¶”ì²œ/ì œì•ˆí˜• íŒ¨í„´
            r'.*ì£¼ì‹.*ì¶”ì²œ.*',
            r'.*ì£¼ì‹.*ë§¤ìˆ˜.*',
            r'.*ì£¼ì‹.*ë§¤ë„.*',
            r'.*íˆ¬ì.*ì œì•ˆ.*',
            r'.*íˆ¬ì.*ì¶”ì²œ.*',
            r'.*ì£¼ê°€.*ì „ë§.*',
            r'.*ì£¼ê°€.*ì˜ˆì¸¡.*',
            r'.*ì£¼ê°€.*ë¶„ì„.*',
            r'.*ì£¼ê°€.*ì¶”ì²œ.*',
            r'.*ì¢…ëª©.*ì¶”ì²œ.*',
            r'.*ì¢…ëª©.*ë¶„ì„.*',
            r'.*ì¢…ëª©.*ì „ë§.*',
            r'.*ì¢…ëª©.*íˆ¬ì.*',
        ]
        
        # ğŸš¨ ì •ìƒì ì¸ ETF ë‰´ìŠ¤ íŒ¨í„´ë“¤ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        etf_normal_patterns = [
            r'.*etf.*ì‹œì¥.*ë™í–¥.*',
            r'.*etf.*ì„±ê³¼.*ë¶„ì„.*',
            r'.*etf.*ìˆ˜ìµë¥ .*',
            r'.*etf.*ìì‚°.*ê·œëª¨.*',
            r'.*etf.*ìƒì¥.*',
            r'.*etf.*íì§€.*',
            r'.*etf.*ìš´ìš©ì‚¬.*',
            r'.*etf.*íˆ¬ìì.*',
        ]
        
        # ğŸš¨ ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ íŒ¨í„´ë“¤ (í¬í•¨ë˜ì–´ì•¼ í•¨)
        stock_normal_patterns = [
            r'.*ì£¼ê°€.*ìƒìŠ¹.*',
            r'.*ì£¼ê°€.*í•˜ë½.*',
            r'.*ì£¼ê°€.*ë³€ë™.*',
            r'.*ì£¼ê°€.*ë™í–¥.*',
            r'.*ì£¼ê°€.*ì„±ê³¼.*',
            r'.*ì£¼ê°€.*ì‹¤ì .*',
            r'.*ì£¼ê°€.*ë°œí‘œ.*',
            r'.*ì£¼ê°€.*ì‹œì¥.*',
            r'.*ì¢…ëª©.*ì‹œì¥.*',
            r'.*ì¢…ëª©.*ë™í–¥.*',
            r'.*ì¢…ëª©.*ì„±ê³¼.*',
            r'.*ì¢…ëª©.*ì‹¤ì .*',
        ]
        
        # ì •ìƒì ì¸ ETF ë‰´ìŠ¤ì¸ì§€ ë¨¼ì € í™•ì¸
        for pattern in etf_normal_patterns:
            if re.search(pattern, title_lower):
                return False  # ì •ìƒì ì¸ ETF ë‰´ìŠ¤ëŠ” í™ë³´ì„± ì•„ë‹˜
        
        # ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ì¸ì§€ ë¨¼ì € í™•ì¸
        for pattern in stock_normal_patterns:
            if re.search(pattern, title_lower):
                return False  # ì •ìƒì ì¸ ì£¼ì‹ ë‰´ìŠ¤ëŠ” í™ë³´ì„± ì•„ë‹˜
        
        # ETF í™ë³´ì„± íŒ¨í„´ í™•ì¸
        for pattern in etf_promotional_patterns:
            if re.search(pattern, title_lower):
                return True
        
        # ì£¼ì‹ í™ë³´ì„± íŒ¨í„´ í™•ì¸
        for pattern in stock_promotional_patterns:
            if re.search(pattern, title_lower):
                return True
        
        return False
    
    def _fetch_html(self) -> Optional[str]:
        """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© User-Agent
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': 'https://news.naver.com/',
            }
            
            response = self.session.get(self.base_url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            print(f"[NAVER] ìš”ì²­ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"[NAVER] ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"[NAVER] ì‘ë‹µ ê¸¸ì´: {len(response.text):,} ë¬¸ì")
            
            return response.text
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_naver_news_links(self, html: str) -> List[Dict[str, str]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ë§í¬ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        news_links = []
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´ë³„ ì¶”ì¶œ
        link_patterns = [
            # ë­í‚¹ ë‰´ìŠ¤ í˜ì´ì§€
            'a[href*="/main/ranking/"]',
            'a[href*="/read.naver"]',
            'a[href*="/article/"]',
            # ì„¹ì…˜ ë‰´ìŠ¤ í˜ì´ì§€  
            'a[href*="/section/"]',
            # íŒ©íŠ¸ì²´í¬ í˜ì´ì§€
            'a[href*="/factcheck/"]',
            # ì¼ë°˜ ë‰´ìŠ¤ ë§í¬
            'a.list_title',
            'a.news_tit',
            'a.cluster_text_headline',
            'a.sh_text_headline',
            # ì¶”ê°€ íŒ¨í„´
            '.list_body a',
            '.group_news a',
            '.news_area a',
        ]
        
        for pattern in link_patterns:
            if len(news_links) >= self.max_news * 2:  # ì—¬ìœ ë¶„ í¬í•¨
                break
                
            try:
                links = soup.select(pattern)
                for link in links:
                    if len(news_links) >= self.max_news * 2:
                        break
                        
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    
                    if (href and text and 
                        len(text) > 10 and  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì œëª©
                        self._is_valid_naver_news_link(str(href))):
                        
                        # ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if href.startswith('/'):
                            full_url = f"https://news.naver.com{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            full_url = urljoin(self.base_url, href)
                        
                        news_links.append({
                            'url': full_url,
                            'title': text
                        })
                        
            except Exception as e:
                logger.error(f"íŒ¨í„´ {pattern} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_links = []
        for link in news_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
                
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨
                if len(unique_links) >= self.max_news:
                    break
        
        return unique_links
    
    def _is_valid_naver_news_link(self, url: str) -> bool:
        """ìœ íš¨í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìœ íš¨ íŒ¨í„´
        valid_patterns = [
            r'news\.naver\.com',
            r'/read\.naver',
            r'/article/',
            r'/main/ranking/',
            r'/section/',
            r'/factcheck/',
        ]
        
        # ì œì™¸í•  íŒ¨í„´
        invalid_patterns = [
            r'/sports/',  # ìŠ¤í¬ì¸  ë‰´ìŠ¤ (ì„ íƒì )
            r'/entertainment/',  # ì—°ì˜ˆ ë‰´ìŠ¤ (ì„ íƒì )
            r'/comment/',  # ëŒ“ê¸€ í˜ì´ì§€
            r'/photo/',  # í¬í†  ë‰´ìŠ¤
            r'/video/',  # ë¹„ë””ì˜¤ ë‰´ìŠ¤
            r'javascript:',  # ìë°”ìŠ¤í¬ë¦½íŠ¸ ë§í¬
            r'#',  # ì•µì»¤ ë§í¬
        ]
        
        # ìœ íš¨ íŒ¨í„´ í™•ì¸
        has_valid = any(re.search(pattern, url) for pattern in valid_patterns)
        if not has_valid:
            return False
            
        # ë¬´íš¨ íŒ¨í„´ í™•ì¸
        has_invalid = any(re.search(pattern, url) for pattern in invalid_patterns)
        return not has_invalid
    
    def _create_news_items(self, links: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±"""
        news_items = []
        
        for link in links:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            keywords = self._extract_keywords(link['title'])
            
            news_item = {
                'title': link['title'],
                'url': link['url'],
                'keywords': keywords,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_type': 'naver_news',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹ë³„ì
            }
            
            news_items.append(news_item)
        
        return news_items
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ íŒ¨í„´
        korean_patterns = [
            r'[ê°€-í£]{2,}(?:ê·¸ë£¹|íšŒì‚¬|ê¸°ì—…|ì½”í¼ë ˆì´ì…˜)',  # íšŒì‚¬ëª…
            r'[ê°€-í£]{2,}(?:ì€í–‰|ì¦ê¶Œ|ìƒëª…|í™”ì¬|ì¹´ë“œ)',  # ê¸ˆìœµì‚¬
            r'(?:ì‚¼ì„±|í˜„ëŒ€|LG|SK|ë¡¯ë°|ì‹ ì„¸ê³„|CJ|í•œí™”)',  # ëŒ€ê¸°ì—…
            r'(?:ì •ë¶€|ì²­ì™€ëŒ€|êµ­íšŒ|ë²•ì›|ê²€ì°°|ê²½ì°°)',  # ì •ë¶€ê¸°ê´€
            r'(?:ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ë‚˜ìŠ¤ë‹¥|ë‹¤ìš°|S&P)',  # ì£¼ì‹ì‹œì¥
        ]
        
        for pattern in korean_patterns:
            matches = re.findall(pattern, title)
            keywords.extend(matches[:2])
        
        # ì˜ì–´ í‚¤ì›Œë“œ íŒ¨í„´
        english_patterns = [
            r'\b[A-Z][A-Z0-9]{2,}\b',  # ëŒ€ë¬¸ì ì•½ì–´ (AAPL, TSLA ë“±)
            r'\b(?:AI|IoT|5G|6G|VR|AR|NFT|DeFi|FinTech)\b',  # ê¸°ìˆ  í‚¤ì›Œë“œ
        ]
        
        for pattern in english_patterns:
            matches = re.findall(pattern, title)
            keywords.extend(matches[:2])
        
        return list(set(keywords))[:5]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 5ê°œ


def test_naver_extractor():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸"""
    test_urls = [
        'https://news.naver.com/main/ranking/popularDay.naver',
        'https://news.naver.com/section/101',  # ê²½ì œ
        'https://news.naver.com/factcheck/main',
    ]
    
    for url in test_urls:
        print(f"\n=== í…ŒìŠ¤íŠ¸: {url} ===")
        extractor = NaverNewsExtractor(url, max_news=3)
        news = extractor.extract_news()
        
        for item in news:
            print(f"ì œëª©: {item['title']}")
            print(f"URL: {item['url']}")
            print(f"í‚¤ì›Œë“œ: {item['keywords']}")
            print("---")


if __name__ == "__main__":
    test_naver_extractor() 