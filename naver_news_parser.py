#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© íŒŒì„œ
ê¸°ì¡´ ì‹œìŠ¤í…œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì ì¸ íŒŒì„œ
"""

import requests
from bs4 import BeautifulSoup
import time
import random
from typing import List, Dict, Optional, Any
import logging
from fake_useragent import UserAgent
import re
from urllib.parse import urljoin, urlparse

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class NaverNewsExtractor:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ì¶”ì¶œê¸°"""
    
    def __init__(self, base_url: str, search_keywords: Optional[str] = None, max_news: int = 10):
        self.base_url = base_url
        self.search_keywords = search_keywords
        self.max_news = max_news
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì í™” í—¤ë”
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.timeout = 15
        
        # ğŸš¨ ê°•í™”ëœ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
        self.promotional_patterns = {
            # ì œëª© ê¸°ë°˜ í™ë³´ì„± í‚¤ì›Œë“œ
            'title_keywords': [
                # í•œêµ­ì–´ í™ë³´ì„± í‚¤ì›Œë“œ
                'ê´‘ê³ ', 'í”„ë¡œëª¨ì…˜', 'í™ë³´', 'ì„ ì „', 'ì–´í•„', 'ì¶”ì²œ', 'ì†Œê°œ',
                'ë°”ë¡œê°€ê¸°', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°', 'êµ¬ë…', 'íŒ”ë¡œìš°', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…',
                'ëŒ“ê¸€', 'í›„ì›', 'ì œíœ´', 'í˜‘ì°¬', 'ìŠ¤í°ì„œ', 'ì§€ì›', 'ë„ì›€',
                'íŠ¹ê°€', 'í• ì¸', 'ì´ë²¤íŠ¸', 'í–‰ì‚¬', 'ëª¨ì§‘', 'ì±„ìš©', 'ê³µê³ ',
                'ì¶œì‹œ', 'ëŸ°ì¹­', 'ì˜¤í”ˆ', 'ì˜¤í”ˆì‹', 'ê¸°ë…', 'ì¶•í•˜', 'ê°ì‚¬',
                'ë‹¹ì²¨', 'ë‹¹ì²¨ì', 'ìˆ˜ìƒ', 'ìˆ˜ìƒì', 'ì‹œìƒ', 'ì‹œìƒì‹',
                'ë¬´ë£Œ', 'ì²´í—˜', 'ìƒ˜í”Œ', 'ì¦ì •', 'ê¸°í”„íŠ¸', 'ì„ ë¬¼',
                'AD', 'Sponsored', 'í›„ì›', 'ì œíœ´',
            ],
            
            # URL íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'url_patterns': [
                r'/ad/', r'/ads/', r'/advertisement/', r'/sponsored/',
                r'/promotion/', r'/promotional/', r'/event/', r'/events/',
                r'/contest/', r'/giveaway/', r'/sweepstakes/', r'/sale/',
                r'/deal/', r'/offer/', r'/special/', r'/limited/',
                r'/free/', r'/trial/', r'/sample/', r'/gift/',
                r'/subscribe/', r'/signup/', r'/register/', r'/join/',
                r'/membership/', r'/premium/', r'/vip/', r'/exclusive/',
                r'/press-release/', r'/announcement/', r'/launch/', r'/release/',
                r'/partnership/', r'/collaboration/', r'/sponsor/', r'/sponsored/',
            ],
            
            # ì œëª© íŒ¨í„´ ê¸°ë°˜ í™ë³´ì„± í•„í„°
            'title_patterns': [
                r'\[.*ê´‘ê³ .*\]', r'\[.*sponsored.*\]', r'\[.*ad.*\]',
                r'\(.*ê´‘ê³ .*\)', r'\(.*sponsored.*\)', r'\(.*ad.*\)',
                r'\[.*í”„ë¡œëª¨ì…˜.*\]', r'\[.*promotion.*\]',
                r'\[.*ì´ë²¤íŠ¸.*\]', r'\[.*event.*\]',
                r'\[.*íŠ¹ê°€.*\]', r'\[.*sale.*\]', r'\[.*í• ì¸.*\]',
                r'\[.*ë¬´ë£Œ.*\]', r'\[.*free.*\]',
                r'\[.*ì¶œì‹œ.*\]', r'\[.*launch.*\]',
                r'\[.*ëŸ°ì¹­.*\]', r'\[.*release.*\]',
                r'\[.*ê³µê°œ.*\]', r'\[.*announcement.*\]',
                r'\[.*ë‹¹ì²¨.*\]', r'\[.*winner.*\]',
                r'\[.*ìˆ˜ìƒ.*\]', r'\[.*award.*\]',
            ],
            
            # ì§§ì€ ì œëª© í•„í„° (í™ë³´ì„± ì œëª©ì€ ë³´í†µ ì§§ìŒ)
            'min_title_length': 15,
            
            # ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì í•„í„° (í™ë³´ì„± ì œëª©ì— ìì£¼ ë‚˜íƒ€ë‚¨)
            'excessive_symbols': [
                '!', '!!', '!!!', '?', '??', '???', '~', '~~', '~~~',
                'â˜…', 'â˜†', 'â™¥', 'â™¡', 'â™ ', 'â™£', 'â™¦', 'â—', 'â—‹', 'â—†', 'â—‡',
                'â–¶', 'â—€', 'â–²', 'â–¼', 'â– ', 'â–¡', 'â–£', 'â–¤', 'â–¥', 'â–¦', 'â–§', 'â–¨', 'â–©',
            ]
        }
    
    def extract_news(self) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì¶”ì¶œ ë©”ì¸ í•¨ìˆ˜"""
        try:
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œ ì‹œì‘: {self.base_url}")
            
            # HTML ìš”ì²­
            html_content = self._fetch_html()
            if not html_content:
                return []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
            news_links = self._extract_naver_news_links(html_content)
            
            # ğŸš¨ í™ë³´ì„± ë‰´ìŠ¤ í•„í„°ë§ ì ìš©
            filtered_links = self._filter_promotional_content(news_links)
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
            news_items = self._create_news_items(filtered_links)
            
            print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ {len(news_links)}ê°œ ë‰´ìŠ¤ ì¶”ì¶œ, í™ë³´ì„± í•„í„°ë§ í›„ {len(filtered_links)}ê°œ ìœ ì§€")
            return news_items[:self.max_news]
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _filter_promotional_content(self, links: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """í™ë³´ì„± ì½˜í…ì¸  í•„í„°ë§"""
        filtered_links = []
        
        for link in links:
            title = link['title']
            url = link['url']
            
            # í™ë³´ì„± ì½˜í…ì¸  ì²´í¬
            if self._is_promotional_content(title, url):
                print(f"ğŸš« í™ë³´ì„± ë‰´ìŠ¤ ì œì™¸: {title[:50]}...")
                continue
            
            filtered_links.append(link)
        
        return filtered_links
    
    def _is_promotional_content(self, title: str, url: str) -> bool:
        """í™ë³´ì„± ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        title_lower = title.lower()
        url_lower = url.lower()
        
        # 1. ì œëª© í‚¤ì›Œë“œ ì²´í¬
        for keyword in self.promotional_patterns['title_keywords']:
            if keyword.lower() in title_lower:
                return True
        
        # 2. URL íŒ¨í„´ ì²´í¬
        for pattern in self.promotional_patterns['url_patterns']:
            if re.search(pattern, url_lower):
                return True
        
        # 3. ì œëª© íŒ¨í„´ ì²´í¬ (ëŒ€ê´„í˜¸, ì†Œê´„í˜¸ ì•ˆì˜ í™ë³´ì„± í‚¤ì›Œë“œ)
        for pattern in self.promotional_patterns['title_patterns']:
            if re.search(pattern, title, re.IGNORECASE):
                return True
        
        # 4. ì œëª© ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ í™ë³´ì„±ì¼ ê°€ëŠ¥ì„±)
        if len(title.strip()) < self.promotional_patterns['min_title_length']:
            return True
        
        # 5. ê³¼ë„í•œ íŠ¹ìˆ˜ë¬¸ì ì²´í¬
        symbol_count = sum(1 for symbol in self.promotional_patterns['excessive_symbols'] if symbol in title)
        if symbol_count >= 3:  # 3ê°œ ì´ìƒì˜ íŠ¹ìˆ˜ë¬¸ìê°€ ìˆìœ¼ë©´ í™ë³´ì„±
            return True
        
        # 6. ë°˜ë³µë˜ëŠ” ë¬¸ì ì²´í¬ (ì˜ˆ: "ëŒ€ë°•!!!", "ìµœê³ !!!")
        if re.search(r'([!?~â˜…â˜†â™¥â™¡])\1{2,}', title):
            return True
        
        # 7. ê³¼ë„í•œ ëŒ€ë¬¸ì ì²´í¬ (í™ë³´ì„± ì œëª©ì€ ëŒ€ë¬¸ìë¥¼ ë§ì´ ì‚¬ìš©)
        uppercase_ratio = sum(1 for char in title if char.isupper()) / len(title) if title else 0
        if uppercase_ratio > 0.7:  # 70% ì´ìƒì´ ëŒ€ë¬¸ìë©´ í™ë³´ì„±
            return True
        
        return False
    
    def _fetch_html(self) -> Optional[str]:
        """HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© User-Agent
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': 'https://news.naver.com/',
            }
            
            response = self.session.get(self.base_url, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            
            print(f"[NAVER] ìš”ì²­ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"[NAVER] ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"[NAVER] ì‘ë‹µ ê¸¸ì´: {len(response.text):,} ë¬¸ì")
            
            return response.text
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_naver_news_links(self, html: str) -> List[Dict[str, str]]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ë§í¬ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        news_links = []
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ íŒ¨í„´ë³„ ì¶”ì¶œ
        link_patterns = [
            # ë­í‚¹ ë‰´ìŠ¤ í˜ì´ì§€
            'a[href*="/main/ranking/"]',
            'a[href*="/read.naver"]',
            'a[href*="/article/"]',
            # ì„¹ì…˜ ë‰´ìŠ¤ í˜ì´ì§€  
            'a[href*="/section/"]',
            # íŒ©íŠ¸ì²´í¬ í˜ì´ì§€
            'a[href*="/factcheck/"]',
            # ì¼ë°˜ ë‰´ìŠ¤ ë§í¬
            'a.list_title',
            'a.news_tit',
            'a.cluster_text_headline',
            'a.sh_text_headline',
            # ì¶”ê°€ íŒ¨í„´
            '.list_body a',
            '.group_news a',
            '.news_area a',
        ]
        
        for pattern in link_patterns:
            if len(news_links) >= self.max_news * 2:  # ì—¬ìœ ë¶„ í¬í•¨
                break
                
            try:
                links = soup.select(pattern)
                for link in links:
                    if len(news_links) >= self.max_news * 2:
                        break
                        
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    
                    if (href and text and 
                        len(text) > 10 and  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì œëª©
                        self._is_valid_naver_news_link(str(href))):
                        
                        # ì ˆëŒ€ URLë¡œ ë³€í™˜
                        if href.startswith('/'):
                            full_url = f"https://news.naver.com{href}"
                        elif href.startswith('http'):
                            full_url = href
                        else:
                            full_url = urljoin(self.base_url, href)
                        
                        news_links.append({
                            'url': full_url,
                            'title': text
                        })
                        
            except Exception as e:
                logger.error(f"íŒ¨í„´ {pattern} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_links = []
        for link in news_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
                
                # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ì¶”ì¶œë˜ë©´ ì¤‘ë‹¨
                if len(unique_links) >= self.max_news:
                    break
        
        return unique_links
    
    def _is_valid_naver_news_link(self, url: str) -> bool:
        """ìœ íš¨í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìœ íš¨ íŒ¨í„´
        valid_patterns = [
            r'news\.naver\.com',
            r'/read\.naver',
            r'/article/',
            r'/main/ranking/',
            r'/section/',
            r'/factcheck/',
        ]
        
        # ì œì™¸í•  íŒ¨í„´
        invalid_patterns = [
            r'/sports/',  # ìŠ¤í¬ì¸  ë‰´ìŠ¤ (ì„ íƒì )
            r'/entertainment/',  # ì—°ì˜ˆ ë‰´ìŠ¤ (ì„ íƒì )
            r'/comment/',  # ëŒ“ê¸€ í˜ì´ì§€
            r'/photo/',  # í¬í†  ë‰´ìŠ¤
            r'/video/',  # ë¹„ë””ì˜¤ ë‰´ìŠ¤
            r'javascript:',  # ìë°”ìŠ¤í¬ë¦½íŠ¸ ë§í¬
            r'#',  # ì•µì»¤ ë§í¬
        ]
        
        # ìœ íš¨ íŒ¨í„´ í™•ì¸
        has_valid = any(re.search(pattern, url) for pattern in valid_patterns)
        if not has_valid:
            return False
            
        # ë¬´íš¨ íŒ¨í„´ í™•ì¸
        has_invalid = any(re.search(pattern, url) for pattern in invalid_patterns)
        return not has_invalid
    
    def _create_news_items(self, links: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±"""
        news_items = []
        
        for link in links:
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©ì—ì„œ)
            keywords = self._extract_keywords(link['title'])
            
            news_item = {
                'title': link['title'],
                'url': link['url'],
                'keywords': keywords,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_type': 'naver_news',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹ë³„ì
            }
            
            news_items.append(news_item)
        
        return news_items
    
    def _extract_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ íŒ¨í„´
        korean_patterns = [
            r'[ê°€-í£]{2,}(?:ê·¸ë£¹|íšŒì‚¬|ê¸°ì—…|ì½”í¼ë ˆì´ì…˜)',  # íšŒì‚¬ëª…
            r'[ê°€-í£]{2,}(?:ì€í–‰|ì¦ê¶Œ|ìƒëª…|í™”ì¬|ì¹´ë“œ)',  # ê¸ˆìœµì‚¬
            r'(?:ì‚¼ì„±|í˜„ëŒ€|LG|SK|ë¡¯ë°|ì‹ ì„¸ê³„|CJ|í•œí™”)',  # ëŒ€ê¸°ì—…
            r'(?:ì •ë¶€|ì²­ì™€ëŒ€|êµ­íšŒ|ë²•ì›|ê²€ì°°|ê²½ì°°)',  # ì •ë¶€ê¸°ê´€
            r'(?:ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ë‚˜ìŠ¤ë‹¥|ë‹¤ìš°|S&P)',  # ì£¼ì‹ì‹œì¥
        ]
        
        for pattern in korean_patterns:
            matches = re.findall(pattern, title)
            keywords.extend(matches[:2])
        
        # ì˜ì–´ í‚¤ì›Œë“œ íŒ¨í„´
        english_patterns = [
            r'\b[A-Z][A-Z0-9]{2,}\b',  # ëŒ€ë¬¸ì ì•½ì–´ (AAPL, TSLA ë“±)
            r'\b(?:AI|IoT|5G|6G|VR|AR|NFT|DeFi|FinTech)\b',  # ê¸°ìˆ  í‚¤ì›Œë“œ
        ]
        
        for pattern in english_patterns:
            matches = re.findall(pattern, title)
            keywords.extend(matches[:2])
        
        return list(set(keywords))[:5]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 5ê°œ


def test_naver_extractor():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸"""
    test_urls = [
        'https://news.naver.com/main/ranking/popularDay.naver',
        'https://news.naver.com/section/101',  # ê²½ì œ
        'https://news.naver.com/factcheck/main',
    ]
    
    for url in test_urls:
        print(f"\n=== í…ŒìŠ¤íŠ¸: {url} ===")
        extractor = NaverNewsExtractor(url, max_news=3)
        news = extractor.extract_news()
        
        for item in news:
            print(f"ì œëª©: {item['title']}")
            print(f"URL: {item['url']}")
            print(f"í‚¤ì›Œë“œ: {item['keywords']}")
            print("---")


if __name__ == "__main__":
    test_naver_extractor() 